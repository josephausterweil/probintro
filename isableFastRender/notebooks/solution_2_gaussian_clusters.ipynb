{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Solution: Gaussian 2-Clusters (Problem 2)\n\n## Problem Setup\n\nWe investigate how to make **categorization decisions** for two categories, where each is defined as a Gaussian distribution.\n\n**Generative Process:**\n\nData are generated by:\n1. First picking category $c \\in \\{1, 2\\}$ according to prior probability $\\theta$\n2. Then generating datum $x$ from the corresponding category's likelihood\n\n$$c_n | \\theta \\sim \\text{Bernoulli}(\\theta)$$\n$$x_n | \\mu_{c(n)}, \\sigma_{c(n)}^2 \\overset{iid}{\\sim} \\mathcal{N}(\\mu_{c(n)}, \\sigma_{c(n)}^2)$$\n\nwhere:\n- $P(c_n = 1) = \\theta$ (prior probability of category 1)\n- $P(c_n = 2) = 1 - \\theta$ (prior probability of category 2)\n\nFor all problems, assume $\\mu_1 = -1$ and $\\mu_2 = 1$.\n\n---\n\n## üìö Reviewing Key Concepts\n\nThis problem connects several important concepts from earlier chapters:\n\n**From [Tutorial 1, Chapter 3 - Probability as Counting](../../content/intro/03_prob_count.md)**:\n- **Random variables**: Category c is a discrete random variable\n- **Weighted counting**: When probabilities aren't equal (Œ∏ ‚â† 0.5)\n\n**From [Tutorial 1, Chapter 4 - Conditional Probability](../../content/intro/04_conditional.md)**:\n- **Conditional probability** P(c|x): What category given observed x?\n- **Marginal vs Joint**: We'll compute marginal p(x) by summing over categories\n- **Law of Total Probability**: p(x) = Œ£_c p(x|c)P(c)\n\n**From [Tutorial 1, Chapter 5 - Bayes' Theorem](../../content/intro/05_bayes.md)**:\n- **Bayes' rule**: P(H|E) = P(E|H)P(H) / P(E)\n- Here: P(c|x) = p(x|c)P(c) / p(x)\n- **Critical concept**: Prior belief updated by evidence\n\n**From [Tutorial 2, Chapter 1 - Mystery Bentos](../../content/intro2/01_mystery_bentos.md)**:\n- **Discrete mixtures**: Chibany's 70% tonkatsu, 30% hamburger\n- **Expected value**: E[X] = Œ∏¬∑value‚ÇÅ + (1-Œ∏)¬∑value‚ÇÇ\n- **Now extending with continuous distributions!**\n\n**From [Tutorial 2, Chapter 3 - Gaussian Distribution](../../content/intro2/03_gaussian.md)**:\n- **Gaussian PDF**: N(x; Œº, œÉ¬≤) describes bell curve\n- **Properties**: Mean Œº centers distribution, variance œÉ¬≤ controls spread\n- Each category has its own Gaussian distribution\n\n**What's new in this assignment:**\n- **Latent (hidden) variables**: Category c is unknown, must infer from x\n- **Categorization**: Using Bayes' rule to classify observations\n- **Mixture distributions**: Combining multiple Gaussians with weights\n- **Decision boundaries**: Where does P(c=1|x) = 0.5?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import packages\nimport jax\nimport jax.numpy as jnp\nimport jax.random as random\nimport jax.lax as lax\nfrom genjax import gen, bernoulli, normal\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Configure matplotlib\nplt.style.use('seaborn-v0_8-whitegrid')\n%matplotlib inline\n\n# Set random seed\nnp.random.seed(42)\nkey = random.PRNGKey(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_datum_c1(mu_1, mu_2, sigma_1_squared, sigma_2_squared, theta, x_min, x_max):\n",
    "    \"\"\"\n",
    "    Compute posterior probability of category 1 and marginal distribution.\n",
    "    \n",
    "    Args:\n",
    "        mu_1, mu_2: Means of categories 1 and 2\n",
    "        sigma_1_squared, sigma_2_squared: Variances of categories 1 and 2\n",
    "        theta: Prior probability of category 1\n",
    "        x_min, x_max: Range for plotting\n",
    "    \n",
    "    Returns:\n",
    "        posterior_c1: P(c=1|x) over x_range\n",
    "        marginal: p(x) over x_range\n",
    "    \"\"\"\n",
    "    # Define x range\n",
    "    x_range = np.linspace(x_min, x_max, 1000)\n",
    "    \n",
    "    # Compute likelihoods\n",
    "    likelihood_1 = norm.pdf(x_range, mu_1, np.sqrt(sigma_1_squared))\n",
    "    likelihood_2 = norm.pdf(x_range, mu_2, np.sqrt(sigma_2_squared))\n",
    "    \n",
    "    # Compute posterior probabilities using Bayes' rule\n",
    "    posterior_c1 = (theta * likelihood_1) / (\n",
    "        theta * likelihood_1 + (1 - theta) * likelihood_2\n",
    "    )\n",
    "    \n",
    "    # Compute marginal distribution (mixture)\n",
    "    marginal = theta * likelihood_1 + (1 - theta) * likelihood_2\n",
    "    \n",
    "    return posterior_c1, marginal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 2(a): Derivation - Categorization\n",
    "\n",
    "Using **Bayes' rule**, derive the probability of a single datum being in category 1: $P(c_1=1|x_1)$.\n",
    "\n",
    "### Solution\n",
    "\n",
    "Using Bayes' rule:\n",
    "\n",
    "$$P(c_1 = 1 | x_1) = \\frac{P(x_1 | c_1 = 1) P(c_1 = 1)}{P(x_1)}$$\n",
    "\n",
    "**Step 1: Prior probability**\n",
    "$$P(c_1 = 1) = \\theta$$\n",
    "\n",
    "**Step 2: Likelihood**\n",
    "$$P(x_1 | c_1 = 1) = \\mathcal{N}(x_1; \\mu_1, \\sigma_1^2)$$\n",
    "\n",
    "where:\n",
    "$$\\mathcal{N}(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right)$$\n",
    "\n",
    "**Step 3: Marginal likelihood** (Law of Total Probability)\n",
    "\n",
    "$$P(x_1) = \\sum_{c} P(x_1 | c) P(c)$$\n",
    "$$= P(x_1 | c_1 = 1) P(c_1 = 1) + P(x_1 | c_1 = 2) P(c_1 = 2)$$\n",
    "$$= \\theta \\, \\mathcal{N}(x_1; \\mu_1, \\sigma_1^2) + (1-\\theta) \\, \\mathcal{N}(x_1; \\mu_2, \\sigma_2^2)$$\n",
    "\n",
    "**Step 4: Posterior probability**\n",
    "\n",
    "Substituting into Bayes' rule:\n",
    "\n",
    "$$\\boxed{P(c_1=1|x_1) = \\frac{\\theta \\, \\mathcal{N}(x_1; \\mu_1, \\sigma_1^2)}{\\theta \\, \\mathcal{N}(x_1; \\mu_1, \\sigma_1^2) + (1-\\theta) \\, \\mathcal{N}(x_1; \\mu_2, \\sigma_2^2)}}$$\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "The posterior probability $P(c=1|x)$ is the **proportion of the likelihood weighted by the prior probability**.\n",
    "\n",
    "- Numerator: How likely is $x$ under category 1, weighted by prior belief in category 1\n",
    "- Denominator: Total likelihood of $x$ across both categories (marginal)\n",
    "- This normalizes the weighted likelihood to be a valid probability (sums to 1 over categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 2(b): Categorization\n",
    "\n",
    "Calculate and plot the probability of being in category 1 for:\n",
    "1. $\\theta = 0.5$ and $\\theta = 0.75$ with $\\sigma_1^2 = \\sigma_2^2 = 1$\n",
    "2. $\\theta = 0.5$ when $\\sigma_1^2 = 0.5$ and $\\sigma_2^2 = 2$\n",
    "3. $\\theta = 0.75$ when $\\sigma_1^2 = 0.5$ and $\\sigma_2^2 = 2$\n",
    "\n",
    "**Question**: Describe the effect of changing the prior and the variance on categorization decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed means\n",
    "mu_1 = -1.0\n",
    "mu_2 = 1.0\n",
    "\n",
    "# Configuration sets\n",
    "theta_values = [0.5, 0.75, 0.25]\n",
    "configs = [\n",
    "    (1, 1),      # œÉ‚ÇÅ¬≤ = œÉ‚ÇÇ¬≤ = 1 (equal variances)\n",
    "    (0.5, 2),    # œÉ‚ÇÅ¬≤ = 0.5, œÉ‚ÇÇ¬≤ = 2 (c1 more precise)\n",
    "    (2, 0.5)     # œÉ‚ÇÅ¬≤ = 2, œÉ‚ÇÇ¬≤ = 0.5 (c2 more precise)\n",
    "]\n",
    "\n",
    "# Colors and linestyles\n",
    "colors = {0.5: \"blue\", 0.75: \"orange\", 0.25: \"#980025\"}\n",
    "colors_sigma = {1: \"steelblue\", 0.5: \"red\", 2: \"green\"}\n",
    "linestyles = {1: \"-\", 0.5: \"--\", 2: \":\"}\n",
    "\n",
    "# Plot range\n",
    "x_min = -6\n",
    "x_max = 6\n",
    "x_range = np.linspace(x_min, x_max, 1000)\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Posterior P(c=1|x)\n",
    "for j, (sigma_1_sq, sigma_2_sq) in enumerate(configs):\n",
    "    for theta in theta_values:\n",
    "        posterior_c1, _ = update_datum_c1(mu_1, mu_2, sigma_1_sq, sigma_2_sq, theta, x_min, x_max)\n",
    "        \n",
    "        axes[0].plot(\n",
    "            x_range,\n",
    "            posterior_c1,\n",
    "            label=f\"Œ∏={theta}, œÉ¬≤‚ÇÅ={sigma_1_sq}, œÉ¬≤‚ÇÇ={sigma_2_sq}\",\n",
    "            color=colors[theta],\n",
    "            linestyle=linestyles[sigma_1_sq],\n",
    "            linewidth=2\n",
    "        )\n",
    "\n",
    "# Decision boundary\n",
    "axes[0].axhline(0.5, color='black', linestyle=':', linewidth=1, alpha=0.5, label='Decision boundary (0.5)')\n",
    "axes[0].axvline(0, color='gray', linestyle='--', linewidth=1, alpha=0.3)\n",
    "axes[0].set_title('Posterior Probability $P(c=1|x)$', fontsize=14)\n",
    "axes[0].set_xlabel('$x$', fontsize=12)\n",
    "axes[0].set_ylabel('Probability', fontsize=12)\n",
    "axes[0].set_ylim([-0.05, 1.05])\n",
    "axes[0].legend(fontsize=9, ncol=2)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Likelihoods\n",
    "for sigma_1_sq, sigma_2_sq in configs:\n",
    "    lik_1 = norm.pdf(x_range, mu_1, np.sqrt(sigma_1_sq))\n",
    "    lik_2 = norm.pdf(x_range, mu_2, np.sqrt(sigma_2_sq))\n",
    "    \n",
    "    axes[1].plot(\n",
    "        x_range,\n",
    "        lik_1,\n",
    "        label=f\"L(c=1|x), œÉ¬≤‚ÇÅ={sigma_1_sq}, œÉ¬≤‚ÇÇ={sigma_2_sq}\",\n",
    "        color=colors_sigma[sigma_1_sq],\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2\n",
    "    )\n",
    "    \n",
    "    axes[1].plot(\n",
    "        x_range,\n",
    "        lik_2,\n",
    "        label=f\"L(c=2|x), œÉ¬≤‚ÇÅ={sigma_1_sq}, œÉ¬≤‚ÇÇ={sigma_2_sq}\",\n",
    "        color=colors_sigma[sigma_1_sq],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2\n",
    "    )\n",
    "\n",
    "axes[1].axvline(mu_1, color='blue', linestyle=':', linewidth=1, alpha=0.5)\n",
    "axes[1].axvline(mu_2, color='red', linestyle=':', linewidth=1, alpha=0.5)\n",
    "axes[1].set_title('Likelihood Distributions', fontsize=14)\n",
    "axes[1].set_xlabel('$x$', fontsize=12)\n",
    "axes[1].set_ylabel('Density', fontsize=12)\n",
    "axes[1].legend(fontsize=9, ncol=2)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "**Effect of Prior ($\\theta$) on Categorization:**\n",
    "\n",
    "1. **Equal prior** ($\\theta = 0.5$):\n",
    "   - When variances are equal, decision boundary is at $x = 0$ (midpoint between $\\mu_1 = -1$ and $\\mu_2 = 1$)\n",
    "   - The posterior probability depends purely on the relative likelihood values\n",
    "   - Symmetric behavior around midpoint\n",
    "\n",
    "2. **Unequal prior** ($\\theta = 0.75$ favors c=1):\n",
    "   - Decision boundary **shifts toward category 2** (the less favored category)\n",
    "   - This makes sense: we need stronger evidence (higher likelihood) to overcome prior belief\n",
    "   - The \"75% line\" crosses 0.5 at a positive $x$ value\n",
    "\n",
    "3. **Prior acts as a \"magnifier\"**:\n",
    "   - It doesn't change the shape of likelihood distributions\n",
    "   - It amplifies or diminishes the contribution of each component to posterior\n",
    "   - Higher $\\theta$ ‚Üí category 1 gets more weight in Bayes' rule calculation\n",
    "\n",
    "**Effect of Likelihood Variance on Categorization:**\n",
    "\n",
    "1. **Equal variances** ($\\sigma_1^2 = \\sigma_2^2 = 1$):\n",
    "   - Decision boundary determined by distance from means and prior\n",
    "   - Posterior transitions smoothly between 0 and 1\n",
    "   - Tails determined by which mean is closer\n",
    "\n",
    "2. **Unequal variances** ($\\sigma_1^2 \\neq \\sigma_2^2$):\n",
    "   - **Component with larger variance dominates the tails!**\n",
    "   - Why? Larger variance ‚Üí flatter distribution ‚Üí longer tails\n",
    "   - Creates asymmetric categorization behavior\n",
    "   \n",
    "3. **\"Needle effect\"**:\n",
    "   - Component with smaller variance has higher peak (taller, narrower)\n",
    "   - Near its mean, it can \"punch through\" and dominate categorization\n",
    "   - This creates sharp transitions in posterior probability\n",
    "\n",
    "**Relative degree of dispersion** (variance ratio) determines:\n",
    "- Shape of posterior probability curve\n",
    "- Whether categorization is balanced or dominated by one component\n",
    "- Location and sharpness of decision boundary\n",
    "\n",
    "**Combined Effects:**\n",
    "\n",
    "When **both** prior and variances are unequal:\n",
    "- Prior shifts the baseline favoritism\n",
    "- Variance ratio determines how sharply the posterior transitions\n",
    "- The \"needle\" can be enhanced (same direction as prior) or diminished (opposite direction)\n",
    "\n",
    "**Decision Boundary Analysis:**\n",
    "- Equal prior + equal variance ‚Üí boundary at midpoint (0)\n",
    "- Unequal prior + equal variance ‚Üí boundary shifts away from favored category\n",
    "- Equal prior + unequal variance ‚Üí boundary shifts toward higher-variance category\n",
    "- Unequal prior + unequal variance ‚Üí complex interaction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and print decision boundaries\n",
    "print(\"\\nüìä Decision Boundary Analysis:\\n\")\n",
    "print(f\"{'Configuration':<40} {'Decision Boundary (x where P(c=1|x)=0.5)':<45}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for sigma_1_sq, sigma_2_sq in configs:\n",
    "    for theta in theta_values:\n",
    "        posterior_c1, _ = update_datum_c1(mu_1, mu_2, sigma_1_sq, sigma_2_sq, theta, x_min, x_max)\n",
    "        \n",
    "        # Find where P(c=1|x) ‚âà 0.5\n",
    "        decision_idx = np.argmin(np.abs(posterior_c1 - 0.5))\n",
    "        decision_x = x_range[decision_idx]\n",
    "        \n",
    "        config_str = f\"Œ∏={theta}, œÉ¬≤‚ÇÅ={sigma_1_sq}, œÉ¬≤‚ÇÇ={sigma_2_sq}\"\n",
    "        print(f\"{config_str:<40} {decision_x:<45.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*85)\n",
    "print(\"\\n‚úÖ Key Observations:\")\n",
    "print(\"  ‚Ä¢ Œ∏=0.5, equal variances ‚Üí boundary at 0.0 (midpoint)\")\n",
    "print(\"  ‚Ä¢ Œ∏>0.5 ‚Üí boundary shifts RIGHT (toward less favored category)\")\n",
    "print(\"  ‚Ä¢ Œ∏<0.5 ‚Üí boundary shifts LEFT (toward less favored category)\")\n",
    "print(\"  ‚Ä¢ Larger œÉ¬≤‚ÇÇ ‚Üí boundary shifts toward c=2 (compensates for wider distribution)\")\n",
    "print(\"  ‚Ä¢ Combined effects can reinforce or counteract each other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Problem 2(c): Derivation - Prediction\n\nUsing Bayes' rule and the **Law of Total Probability**, derive the probability of a data point $p(x)$ according to this model (without any given data).\n\n### Solution\n\nUsing the **Law of Total Probability**:\n\n$$p(x_1) = \\sum_{c} p(x_1 | c) \\, P(c)$$\n\n**üìñ Recall from [Tutorial 1, Chapter 4](../../content/intro/04_conditional.md):**\n\nThe **Law of Total Probability** (also called the **sum rule** or **marginalization**):\n- To find P(A), sum over all possibilities of another variable B\n- $P(A) = \\sum_b P(A, B=b) = \\sum_b P(A|B=b) \\cdot P(B=b)$\n- This \"marginalizes out\" the variable B\n\n**Step 1: Expand for two categories**\n\n$$p(x_1) = p(x_1 | c_1 = 1) P(c_1 = 1) + p(x_1 | c_1 = 2) P(c_1 = 2)$$\n\n**Step 2: Substitute terms**\n\nPrior probabilities:\n$$P(c_1 = 1) = \\theta, \\quad P(c_1 = 2) = 1 - \\theta$$\n\nLikelihoods:\n$$p(x_1 | c_1 = 1) = \\mathcal{N}(x_1; \\mu_1, \\sigma_1^2)$$\n$$p(x_1 | c_1 = 2) = \\mathcal{N}(x_1; \\mu_2, \\sigma_2^2)$$\n\n**Step 3: Final result**\n\n$$\\boxed{p(x_1) = \\theta \\, \\mathcal{N}(x_1; \\mu_1, \\sigma_1^2) + (1-\\theta) \\, \\mathcal{N}(x_1; \\mu_2, \\sigma_2^2)}$$\n\n### Interpretation\n\nThe marginal distribution $p(x)$ is a **weighted mixture** of the two component Gaussians:\n- Weights are the prior probabilities ($\\theta$ and $1-\\theta$)\n- Each component contributes according to its weight\n- This is called a **Gaussian Mixture Model** (GMM)\n\n**Connection to [Tutorial 2, Chapter 1](../../content/intro2/01_mystery_bentos.md)**:\n- Remember Chibany's discrete mixture: 70% √ó 500g + 30% √ó 350g = 455g\n- Here we have the **continuous analog**: weighted sum of PDFs, not values!\n- Instead of discrete outcomes, we have continuous distributions\n\nThe marginal can have:\n- **Two peaks** (bimodal) if components are well-separated\n- **One peak** (unimodal) if components overlap heavily\n- **Plateau** if components have similar height and overlap"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 2(d): Prediction\n",
    "\n",
    "Plot $p(x_1)$ for:\n",
    "1. $\\theta = 0.5$ and $\\theta = 0.75$ with $\\sigma_1^2 = \\sigma_2^2 = 1$\n",
    "2. $\\theta = 0.5$ when $\\sigma_1^2 = 0.5$ and $\\sigma_2^2 = 2$\n",
    "3. $\\theta = 0.75$ when $\\sigma_1^2 = 0.5$ and $\\sigma_2^2 = 2$\n",
    "\n",
    "**Question**: How does the prior and variance affect $p(x_1)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration sets (matching part b)\n",
    "theta_values = [0.5, 0.75]\n",
    "configs = [(1, 1), (0.5, 2)]\n",
    "\n",
    "# Colors\n",
    "colors = {0.5: \"blue\", 0.75: \"orange\"}\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Plot range\n",
    "x_min = -6\n",
    "x_max = 6\n",
    "x_range = np.linspace(x_min, x_max, 1000)\n",
    "\n",
    "for j, (sigma_1_sq, sigma_2_sq) in enumerate(configs):\n",
    "    for theta in theta_values:\n",
    "        posterior_c1, marginal = update_datum_c1(mu_1, mu_2, sigma_1_sq, sigma_2_sq, theta, x_min, x_max)\n",
    "        \n",
    "        # Plot posterior (for comparison)\n",
    "        axes[0].plot(\n",
    "            x_range,\n",
    "            posterior_c1,\n",
    "            label=f\"P(c=1|x), Œ∏={theta}, œÉ¬≤‚ÇÅ={sigma_1_sq}, œÉ¬≤‚ÇÇ={sigma_2_sq}\",\n",
    "            color=colors[theta],\n",
    "            linestyle=\"-\" if sigma_1_sq == 1 else \"--\",\n",
    "            linewidth=2\n",
    "        )\n",
    "        \n",
    "        # Plot marginal (predictive)\n",
    "        axes[1].plot(\n",
    "            x_range,\n",
    "            marginal,\n",
    "            label=f\"p(x), Œ∏={theta}, œÉ¬≤‚ÇÅ={sigma_1_sq}, œÉ¬≤‚ÇÇ={sigma_2_sq}\",\n",
    "            color=colors[theta],\n",
    "            linestyle=\"-\" if sigma_1_sq == 1 else \"--\",\n",
    "            linewidth=2\n",
    "        )\n",
    "\n",
    "# Configure posterior plot\n",
    "axes[0].axhline(0.5, color='black', linestyle=':', linewidth=1, alpha=0.3)\n",
    "axes[0].set_title('Posterior Probability $P(c=1|x)$', fontsize=14)\n",
    "axes[0].set_xlabel('$x$', fontsize=12)\n",
    "axes[0].set_ylabel('Probability', fontsize=12)\n",
    "axes[0].set_ylim([-0.05, 1.05])\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Configure marginal plot\n",
    "axes[1].axvline(mu_1, color='blue', linestyle=':', linewidth=1, alpha=0.3, label='Œº‚ÇÅ=-1')\n",
    "axes[1].axvline(mu_2, color='red', linestyle=':', linewidth=1, alpha=0.3, label='Œº‚ÇÇ=1')\n",
    "axes[1].set_title('Marginal Distribution $p(x)$ (Mixture)', fontsize=14)\n",
    "axes[1].set_xlabel('$x$', fontsize=12)\n",
    "axes[1].set_ylabel('Density', fontsize=12)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "**Effect of Prior ($\\theta$) on Marginal Distribution:**\n",
    "\n",
    "The marginal distribution is:\n",
    "$$p(x) = \\theta \\, \\mathcal{N}(x; \\mu_1, \\sigma_1^2) + (1-\\theta) \\, \\mathcal{N}(x; \\mu_2, \\sigma_2^2)$$\n",
    "\n",
    "1. **Prior as a \"magnifier\"**:\n",
    "   - $\\theta$ controls how much each component contributes to the mixture\n",
    "   - Higher $\\theta$ ‚Üí component 1 gets larger weight ‚Üí higher peak near $\\mu_1$\n",
    "   - Lower $\\theta$ ‚Üí component 2 gets larger weight ‚Üí higher peak near $\\mu_2$\n",
    "\n",
    "2. **Envelope constraint**:\n",
    "   - The marginal is always bounded by the individual components\n",
    "   - Extreme case: $\\theta = 1$ ‚Üí $p(x) = \\mathcal{N}(x; \\mu_1, \\sigma_1^2)$ (pure component 1)\n",
    "   - Extreme case: $\\theta = 0$ ‚Üí $p(x) = \\mathcal{N}(x; \\mu_2, \\sigma_2^2)$ (pure component 2)\n",
    "\n",
    "3. **Shape modulation**:\n",
    "   - Prior doesn't change the **shape** of individual components\n",
    "   - It only changes their **relative heights** in the mixture\n",
    "   - The mixture shape depends on overlap and relative weights\n",
    "\n",
    "**Effect of Variance on Marginal Distribution:**\n",
    "\n",
    "1. **Equal variances** ($\\sigma_1^2 = \\sigma_2^2 = 1$):\n",
    "   - With $\\theta = 0.5$: Creates a **plateau** (flat top merged from two diminished peaks)\n",
    "   - With $\\theta \\neq 0.5$: Asymmetric mixture, one peak higher than the other\n",
    "   - Smooth transition between component regions\n",
    "\n",
    "2. **Unequal variances** ($\\sigma_1^2 = 0.5, \\sigma_2^2 = 2$):\n",
    "   - Component with **smaller variance** creates a sharp spike (high, narrow peak)\n",
    "   - Component with **larger variance** creates a wide bump (low, wide peak)\n",
    "   - Bimodal shape more pronounced (two distinct peaks visible)\n",
    "\n",
    "3. **Combined effect**:\n",
    "   - Equal prior + equal variance ‚Üí plateau or symmetric bimodal\n",
    "   - Unequal prior + equal variance ‚Üí asymmetric mixture, one peak larger\n",
    "   - Equal prior + unequal variance ‚Üí asymmetric mixture, narrow peak taller\n",
    "   - Unequal prior + unequal variance ‚Üí complex asymmetry, depends on alignment\n",
    "\n",
    "**Modality Analysis:**\n",
    "\n",
    "The number of peaks (modes) depends on:\n",
    "- **Separation** of means ($|\\mu_1 - \\mu_2|$)\n",
    "- **Variance ratio** ($\\sigma_1^2 / \\sigma_2^2$)\n",
    "- **Mixing weights** ($\\theta$ vs $1-\\theta$)\n",
    "\n",
    "In our examples:\n",
    "- Blue solid (Œ∏=0.5, equal var): **Unimodal plateau** (components merge)\n",
    "- Blue dashed (Œ∏=0.5, unequal var): **Bimodal** (distinct peaks)\n",
    "- Orange solid (Œ∏=0.75, equal var): **Asymmetric unimodal** (one side heavier)\n",
    "- Orange dashed (Œ∏=0.75, unequal var): **Asymmetric bimodal** (different peak heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze modality\n",
    "print(\"\\nüìä Marginal Distribution Analysis:\\n\")\n",
    "print(f\"{'Configuration':<40} {'Modality':<20} {'Peak Location(s)':<30}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for sigma_1_sq, sigma_2_sq in configs:\n",
    "    for theta in theta_values:\n",
    "        _, marginal = update_datum_c1(mu_1, mu_2, sigma_1_sq, sigma_2_sq, theta, x_min, x_max)\n",
    "        \n",
    "        # Find peaks (local maxima)\n",
    "        from scipy.signal import find_peaks\n",
    "        peaks, _ = find_peaks(marginal, height=0.01, distance=50)\n",
    "        peak_locations = x_range[peaks]\n",
    "        \n",
    "        # Determine modality\n",
    "        if len(peaks) == 0:\n",
    "            modality = \"No clear peak\"\n",
    "        elif len(peaks) == 1:\n",
    "            modality = \"Unimodal\"\n",
    "        elif len(peaks) == 2:\n",
    "            modality = \"Bimodal\"\n",
    "        else:\n",
    "            modality = f\"{len(peaks)} peaks\"\n",
    "        \n",
    "        peak_str = \", \".join([f\"{p:.2f}\" for p in peak_locations]) if len(peaks) > 0 else \"N/A\"\n",
    "        \n",
    "        config_str = f\"Œ∏={theta}, œÉ¬≤‚ÇÅ={sigma_1_sq}, œÉ¬≤‚ÇÇ={sigma_2_sq}\"\n",
    "        print(f\"{config_str:<40} {modality:<20} {peak_str:<30}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"\\n‚úÖ Conclusions:\")\n",
    "print(\"  ‚Ä¢ Prior Œ∏ acts as weight/magnifier for each component\")\n",
    "print(\"  ‚Ä¢ Equal variances + equal prior ‚Üí plateau (merged peaks)\")\n",
    "print(\"  ‚Ä¢ Unequal variances ‚Üí distinct peaks (bimodal)\")\n",
    "print(\"  ‚Ä¢ Smaller variance component has taller, sharper peak\")\n",
    "print(\"  ‚Ä¢ Prior doesn't change component shapes, only their relative heights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GenJAX Implementation: Mixture Model\n",
    "\n",
    "Let's implement and simulate from the Gaussian mixture model using GenJAX!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@gen\ndef gaussian_mixture_model(theta, mu_1, mu_2, sigma_1, sigma_2):\n    \"\"\"\n    GenJAX generative model for Gaussian mixture.\n    \n    Args:\n        theta: Prior probability of category 1\n        mu_1, mu_2: Means of categories 1 and 2\n        sigma_1, sigma_2: Standard deviations of categories 1 and 2\n    \"\"\"\n    # Sample category (Bernoulli with parameter theta)\n    c = bernoulli(theta) @ \"category\"\n    \n    # Sample from both distributions (GenJAX needs @ operator at top level)\n    x1 = normal(mu_1, sigma_1) @ \"observation_1\"\n    x2 = normal(mu_2, sigma_2) @ \"observation_2\"\n    \n    # Select which observation to use based on category\n    x = lax.cond(c == 1, lambda _: x1, lambda _: x2, None)\n    \n    return x, c\n\n# Simulate from mixture model\nprint(\"üî¨ GenJAX Simulation: Gaussian Mixture Model\\n\")\n\n# Parameters\ntheta = 0.7\nsigma_1 = 1.0\nsigma_2 = 1.0\nn_samples = 2000\n\nprint(f\"Parameters: Œ∏={theta}, Œº‚ÇÅ={mu_1}, Œº‚ÇÇ={mu_2}, œÉ‚ÇÅ={sigma_1}, œÉ‚ÇÇ={sigma_2}\")\nprint(f\"Generating {n_samples} samples...\\n\")\n\n# Generate samples\nkey = random.PRNGKey(42)\nobservations = []\ncategories = []\n\nfor _ in range(n_samples):\n    key, subkey = random.split(key)\n    trace = gaussian_mixture_model.simulate(subkey, (theta, mu_1, mu_2, sigma_1, sigma_2))\n    x, c = trace.get_retval()\n    observations.append(float(x))\n    categories.append(int(c))\n\nobservations = np.array(observations)\ncategories = np.array(categories)\n\n# Separate by category\nobs_c1 = observations[categories == 1]\nobs_c2 = observations[categories == 0]\n\nprint(f\"Results:\")\nprint(f\"  Category 1: {len(obs_c1)} samples ({len(obs_c1)/n_samples*100:.1f}%) [expected: {theta*100:.1f}%]\")\nprint(f\"  Category 2: {len(obs_c2)} samples ({len(obs_c2)/n_samples*100:.1f}%) [expected: {(1-theta)*100:.1f}%]\")\nprint(f\"  Overall mean: {np.mean(observations):.2f} [expected: {theta*mu_1 + (1-theta)*mu_2:.2f}]\")\nprint(f\"  Category 1 mean: {np.mean(obs_c1):.2f} [expected: {mu_1:.2f}]\")\nprint(f\"  Category 2 mean: {np.mean(obs_c2):.2f} [expected: {mu_2:.2f}]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GenJAX simulation results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: All observations (marginal distribution)\n",
    "axes[0].hist(observations, bins=60, density=True, alpha=0.7, color='purple', \n",
    "            edgecolor='black', label='Simulated data')\n",
    "\n",
    "# Overlay theoretical marginal\n",
    "x_plot = np.linspace(-5, 5, 1000)\n",
    "theoretical_marginal = theta * norm.pdf(x_plot, mu_1, sigma_1) + \\\n",
    "                      (1-theta) * norm.pdf(x_plot, mu_2, sigma_2)\n",
    "axes[0].plot(x_plot, theoretical_marginal, 'r-', linewidth=2, \n",
    "            label='Theoretical p(x)')\n",
    "\n",
    "axes[0].set_xlabel('x', fontsize=12)\n",
    "axes[0].set_ylabel('Density', fontsize=12)\n",
    "axes[0].set_title('Marginal Distribution: Simulated vs Theoretical', fontsize=13)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Observations by true category\n",
    "axes[1].hist(obs_c1, bins=30, density=True, alpha=0.6, color='blue', \n",
    "            edgecolor='black', label=f'Category 1 (n={len(obs_c1)})')\n",
    "axes[1].hist(obs_c2, bins=30, density=True, alpha=0.6, color='red', \n",
    "            edgecolor='black', label=f'Category 2 (n={len(obs_c2)})')\n",
    "\n",
    "# Overlay theoretical likelihoods\n",
    "axes[1].plot(x_plot, norm.pdf(x_plot, mu_1, sigma_1), 'b-', linewidth=2, \n",
    "            alpha=0.7, label='Theoretical L(c=1|x)')\n",
    "axes[1].plot(x_plot, norm.pdf(x_plot, mu_2, sigma_2), 'r-', linewidth=2, \n",
    "            alpha=0.7, label='Theoretical L(c=2|x)')\n",
    "\n",
    "axes[1].axvline(mu_1, color='blue', linestyle='--', linewidth=1, alpha=0.5)\n",
    "axes[1].axvline(mu_2, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "axes[1].set_xlabel('x', fontsize=12)\n",
    "axes[1].set_ylabel('Density', fontsize=12)\n",
    "axes[1].set_title('Observations by True Category', fontsize=13)\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ GenJAX simulation matches theoretical predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Insights from Problem 2:\n",
    "\n",
    "1. **Categorization (Posterior Probability)**:\n",
    "   - Computed using Bayes' rule: $P(c=1|x) = \\frac{\\theta \\mathcal{N}(x;\\mu_1,\\sigma_1^2)}{\\theta \\mathcal{N}(x;\\mu_1,\\sigma_1^2) + (1-\\theta)\\mathcal{N}(x;\\mu_2,\\sigma_2^2)}$\n",
    "   - Prior $\\theta$ acts as a weight/magnifier for each component\n",
    "   - Decision boundary shifts based on prior and variance ratios\n",
    "   - Component with larger variance dominates tails\n",
    "\n",
    "2. **Effect of Prior ($\\theta$)**:\n",
    "   - Shifts decision boundary toward less favored category\n",
    "   - Amplifies contribution of favored component in mixture\n",
    "   - Doesn't change likelihood shapes, only their relative importance\n",
    "\n",
    "3. **Effect of Variance Ratio**:\n",
    "   - Equal variances ‚Üí symmetric behavior around midpoint\n",
    "   - Unequal variances ‚Üí asymmetric categorization\n",
    "   - Smaller variance ‚Üí taller, sharper peak (\"needle effect\")\n",
    "   - Larger variance ‚Üí wider, flatter distribution (dominates tails)\n",
    "\n",
    "4. **Marginal Distribution (Mixture)**:\n",
    "   - Weighted sum: $p(x) = \\theta \\mathcal{N}(x;\\mu_1,\\sigma_1^2) + (1-\\theta)\\mathcal{N}(x;\\mu_2,\\sigma_2^2)$\n",
    "   - Can be unimodal, bimodal, or plateau depending on parameters\n",
    "   - Prior controls relative heights of component peaks\n",
    "   - Variance ratio determines peak sharpness\n",
    "\n",
    "5. **Mixture Model Behavior**:\n",
    "   - Components bounded by envelope of individual Gaussians\n",
    "   - Separation of means, variance ratio, and mixing weights determine modality\n",
    "   - GenJAX simulations verify theoretical predictions\n",
    "\n",
    "### Connection to Bayesian Learning:\n",
    "\n",
    "This problem introduces **latent variable models**:\n",
    "- Observed: $x$ (continuous)\n",
    "- Latent (hidden): $c$ (discrete category)\n",
    "- Inference: Given $x$, infer $c$ (categorization)\n",
    "- Generation: Given $c$, generate $x$ (sampling)\n",
    "\n",
    "This framework extends to:\n",
    "- Gaussian Mixture Models (GMM) with unknown $\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2$\n",
    "- Expectation-Maximization (EM) algorithm for learning parameters\n",
    "- Clustering and unsupervised learning\n",
    "\n",
    "**Next steps**: Extend to unknown parameters and learn from data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}