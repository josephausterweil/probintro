{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Exploration: Gaussian Bayesian Updates and Mixture Models\n",
    "\n",
    "This notebook provides interactive visualizations to explore:\n",
    "1. **Gaussian-Gaussian Bayesian Updates**: How variance and number of observations affect learning\n",
    "2. **Gaussian Mixture Models**: How priors and variance affect categorization\n",
    "\n",
    "All implementations use **GenJAX** for probabilistic programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "from genjax import gen, simulate, choice_map\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm as scipy_norm\n",
    "from ipywidgets import interact, FloatSlider, IntSlider, fixed\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "key = random.PRNGKey(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Gaussian-Gaussian Bayesian Update\n",
    "\n",
    "### The Model\n",
    "\n",
    "**Prior**: $\\mu \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$\n",
    "\n",
    "**Likelihood**: $x_1, \\ldots, x_N | \\mu, \\sigma_x^2 \\overset{iid}{\\sim} \\mathcal{N}(\\mu, \\sigma_x^2)$\n",
    "\n",
    "**Posterior**: $\\mu | x_1, \\ldots, x_N \\sim \\mathcal{N}(\\mu_N, \\sigma_N^2)$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\mu_N = \\frac{\\mu_0 \\sigma_0^{-2} + \\sigma_x^{-2} \\sum_{n=1}^N x_n}{\\sigma_0^{-2} + N \\sigma_x^{-2}}$$\n",
    "\n",
    "$$\\sigma_N^2 = \\left[ \\sigma_0^{-2} + N \\sigma_x^{-2} \\right]^{-1}$$\n",
    "\n",
    "**Predictive**: $x_{N+1} | x_1, \\ldots, x_N \\sim \\mathcal{N}(\\mu_N, \\sigma_N^2 + \\sigma_x^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical Bayesian update function\n",
    "def gaussian_gaussian_update(mu_0, sigma_0_sq, observations, sigma_x_sq):\n",
    "    \"\"\"\n",
    "    Analytical Bayesian update for Gaussian-Gaussian conjugate prior.\n",
    "    \n",
    "    Args:\n",
    "        mu_0: Prior mean\n",
    "        sigma_0_sq: Prior variance\n",
    "        observations: Array of observations\n",
    "        sigma_x_sq: Likelihood variance (known)\n",
    "    \n",
    "    Returns:\n",
    "        posterior_mu: Posterior mean\n",
    "        posterior_sigma_sq: Posterior variance\n",
    "    \"\"\"\n",
    "    N = len(observations)\n",
    "    sum_x = jnp.sum(jnp.array(observations))\n",
    "    \n",
    "    # Precision-weighted update\n",
    "    prior_precision = 1.0 / sigma_0_sq\n",
    "    data_precision = N / sigma_x_sq\n",
    "    \n",
    "    posterior_precision = prior_precision + data_precision\n",
    "    posterior_sigma_sq = 1.0 / posterior_precision\n",
    "    \n",
    "    posterior_mu = posterior_sigma_sq * (prior_precision * mu_0 + (sum_x / sigma_x_sq))\n",
    "    \n",
    "    return posterior_mu, posterior_sigma_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GenJAX model for Gaussian-Gaussian learning\n",
    "@gen\n",
    "def gaussian_learning_model(observations, sigma_x_sq):\n",
    "    \"\"\"\n",
    "    Generative model for Gaussian learning with unknown mean.\n",
    "    \n",
    "    Args:\n",
    "        observations: Observed data points\n",
    "        sigma_x_sq: Known likelihood variance\n",
    "    \"\"\"\n",
    "    # Prior on unknown mean\n",
    "    mu = jnp.normal(0.0, 1.0) @ \"mu\"\n",
    "    \n",
    "    # Generate observations\n",
    "    sigma_x = jnp.sqrt(sigma_x_sq)\n",
    "    for i in range(len(observations)):\n",
    "        x = jnp.normal(mu, sigma_x) @ f\"obs_{i}\"\n",
    "    \n",
    "    return mu\n",
    "\n",
    "@gen\n",
    "def posterior_predictive_model(posterior_mu, posterior_sigma_sq, sigma_x_sq):\n",
    "    \"\"\"\n",
    "    Sample from posterior predictive distribution.\n",
    "    \n",
    "    Args:\n",
    "        posterior_mu: Posterior mean for mu\n",
    "        posterior_sigma_sq: Posterior variance for mu\n",
    "        sigma_x_sq: Likelihood variance\n",
    "    \"\"\"\n",
    "    # Sample mu from posterior\n",
    "    mu = jnp.normal(posterior_mu, jnp.sqrt(posterior_sigma_sq)) @ \"mu\"\n",
    "    \n",
    "    # Sample new observation given mu\n",
    "    x_new = jnp.normal(mu, jnp.sqrt(sigma_x_sq)) @ \"x_new\"\n",
    "    \n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Visualization: Effect of Likelihood Variance\n",
    "\n",
    "Explore how changing the **likelihood variance** ($\\sigma_x^2$) affects the posterior and predictive distributions.\n",
    "\n",
    "**Question**: What happens when the likelihood has:\n",
    "- **Small variance** (precise measurements)?\n",
    "- **Large variance** (noisy measurements)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variance_effect(sigma_x_sq, n_obs=1, x_obs=2.0):\n",
    "    \"\"\"\n",
    "    Interactive plot showing effect of likelihood variance.\n",
    "    \"\"\"\n",
    "    # Prior parameters (fixed)\n",
    "    mu_0 = 0.0\n",
    "    sigma_0_sq = 1.0\n",
    "    \n",
    "    # Generate observations (all at x_obs for simplicity)\n",
    "    observations = [x_obs] * n_obs\n",
    "    \n",
    "    # Analytical update\n",
    "    post_mu, post_sigma_sq = gaussian_gaussian_update(mu_0, sigma_0_sq, observations, sigma_x_sq)\n",
    "    \n",
    "    # Predictive parameters\n",
    "    pred_mu = post_mu\n",
    "    pred_sigma_sq = post_sigma_sq + sigma_x_sq\n",
    "    \n",
    "    # Plot range\n",
    "    x_range = np.linspace(-5, 8, 1000)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Prior PDF\n",
    "    prior_pdf = scipy_norm.pdf(x_range, mu_0, np.sqrt(sigma_0_sq))\n",
    "    \n",
    "    # Posterior PDF\n",
    "    post_pdf = scipy_norm.pdf(x_range, post_mu, np.sqrt(post_sigma_sq))\n",
    "    \n",
    "    # Predictive PDF\n",
    "    pred_pdf = scipy_norm.pdf(x_range, pred_mu, np.sqrt(pred_sigma_sq))\n",
    "    \n",
    "    # Plot 1: Prior vs Posterior\n",
    "    axes[0].plot(x_range, prior_pdf, 'k--', linewidth=2, label=f'Prior: N({mu_0}, {sigma_0_sq})', alpha=0.7)\n",
    "    axes[0].plot(x_range, post_pdf, 'b-', linewidth=2, label=f'Posterior: N({post_mu:.2f}, {post_sigma_sq:.2f})')\n",
    "    axes[0].axvline(x_obs, color='red', linestyle=':', linewidth=2, label=f'Observation(s): {x_obs}')\n",
    "    axes[0].axvline(post_mu, color='blue', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[0].set_xlabel('Î¼', fontsize=12)\n",
    "    axes[0].set_ylabel('Density', fontsize=12)\n",
    "    axes[0].set_title(f'Posterior Distribution (ÏƒÂ²_x = {sigma_x_sq})', fontsize=13)\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Posterior vs Predictive\n",
    "    axes[1].plot(x_range, post_pdf, 'b-', linewidth=2, label=f'Posterior: N({post_mu:.2f}, {post_sigma_sq:.2f})', alpha=0.7)\n",
    "    axes[1].plot(x_range, pred_pdf, 'orange', linewidth=2, label=f'Predictive: N({pred_mu:.2f}, {pred_sigma_sq:.2f})')\n",
    "    axes[1].axvline(post_mu, color='blue', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[1].set_xlabel('x', fontsize=12)\n",
    "    axes[1].set_ylabel('Density', fontsize=12)\n",
    "    axes[1].set_title(f'Predictive Distribution (ÏƒÂ²_x = {sigma_x_sq})', fontsize=13)\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print interpretation\n",
    "    print(f\"\\nðŸ“Š Interpretation:\")\n",
    "    print(f\"  Prior mean: {mu_0:.2f}, Prior std: {np.sqrt(sigma_0_sq):.2f}\")\n",
    "    print(f\"  Observation(s): {x_obs:.2f} (n={n_obs})\")\n",
    "    print(f\"  Likelihood std: {np.sqrt(sigma_x_sq):.2f}\")\n",
    "    print(f\"  Posterior mean: {post_mu:.2f} (shifted toward data)\")\n",
    "    print(f\"  Posterior std: {np.sqrt(post_sigma_sq):.2f} (uncertainty decreased)\")\n",
    "    print(f\"  Predictive std: {np.sqrt(pred_sigma_sq):.2f} (larger due to data variance)\")\n",
    "\n",
    "# Create interactive widget\n",
    "interact(plot_variance_effect,\n",
    "         sigma_x_sq=FloatSlider(min=0.1, max=5.0, step=0.1, value=0.25, description='ÏƒÂ²_x (likelihood var)'),\n",
    "         n_obs=IntSlider(min=1, max=10, step=1, value=1, description='Num observations'),\n",
    "         x_obs=FloatSlider(min=-2.0, max=4.0, step=0.5, value=2.0, description='Observation value'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Visualization: Sequential Learning\n",
    "\n",
    "Watch how the posterior evolves as we add more observations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sequential_learning(n_observations, sigma_x_sq=0.25):\n",
    "    \"\"\"\n",
    "    Show sequential Bayesian updates.\n",
    "    \"\"\"\n",
    "    # Fixed parameters\n",
    "    mu_0 = 0.0\n",
    "    sigma_0_sq = 1.0\n",
    "    \n",
    "    # Generate observations around x=2\n",
    "    np.random.seed(42)\n",
    "    observations = [2.1, 2.5, 1.4, 2.2, 1.8, 2.0, 1.9, 2.3, 1.7, 2.1]\n",
    "    observations = observations[:n_observations]\n",
    "    \n",
    "    # Plot range\n",
    "    x_range = np.linspace(-3, 5, 1000)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    \n",
    "    # Plot prior\n",
    "    prior_pdf = scipy_norm.pdf(x_range, mu_0, np.sqrt(sigma_0_sq))\n",
    "    ax.plot(x_range, prior_pdf, 'k--', linewidth=2, label='Prior: N(0, 1)', alpha=0.7)\n",
    "    \n",
    "    # Sequential updates\n",
    "    colors = plt.cm.Blues(np.linspace(0.3, 0.9, min(n_observations, 5)))\n",
    "    \n",
    "    mu_current = mu_0\n",
    "    sigma_sq_current = sigma_0_sq\n",
    "    \n",
    "    # Show specific steps\n",
    "    steps_to_show = [1, 3, 5, n_observations] if n_observations > 5 else list(range(1, n_observations + 1))\n",
    "    steps_to_show = sorted(set(steps_to_show))\n",
    "    \n",
    "    for i in range(n_observations):\n",
    "        mu_current, sigma_sq_current = gaussian_gaussian_update(\n",
    "            mu_current, sigma_sq_current, [observations[i]], sigma_x_sq\n",
    "        )\n",
    "        \n",
    "        if (i + 1) in steps_to_show:\n",
    "            post_pdf = scipy_norm.pdf(x_range, mu_current, np.sqrt(sigma_sq_current))\n",
    "            color_idx = min(steps_to_show.index(i + 1), len(colors) - 1)\n",
    "            ax.plot(x_range, post_pdf, linewidth=2, \n",
    "                   label=f'After {i+1} obs: N({mu_current:.2f}, {sigma_sq_current:.2f})',\n",
    "                   color=colors[color_idx] if n_observations > 1 else 'blue')\n",
    "    \n",
    "    # Mark sample mean\n",
    "    sample_mean = np.mean(observations)\n",
    "    ax.axvline(sample_mean, color='red', linestyle=':', linewidth=2, \n",
    "              label=f'Sample mean: {sample_mean:.2f}')\n",
    "    \n",
    "    ax.set_xlabel('Î¼', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title(f'Sequential Bayesian Learning (ÏƒÂ²_x = {sigma_x_sq})', fontsize=13)\n",
    "    ax.legend(fontsize=10, loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nðŸ“Š Summary after {n_observations} observations:\")\n",
    "    print(f\"  Observations: {observations}\")\n",
    "    print(f\"  Sample mean: {sample_mean:.2f}\")\n",
    "    print(f\"  Final posterior: N({mu_current:.2f}, {sigma_sq_current:.2f})\")\n",
    "    print(f\"  Posterior std: {np.sqrt(sigma_sq_current):.2f} (uncertainty: {np.sqrt(sigma_0_sq):.2f} â†’ {np.sqrt(sigma_sq_current):.2f})\")\n",
    "\n",
    "# Create interactive widget\n",
    "interact(plot_sequential_learning,\n",
    "         n_observations=IntSlider(min=1, max=10, step=1, value=1, description='Num observations'),\n",
    "         sigma_x_sq=FloatSlider(min=0.1, max=2.0, step=0.1, value=0.25, description='ÏƒÂ²_x'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gaussian Mixture - Two Clusters\n",
    "\n",
    "### The Model\n",
    "\n",
    "**Generative Process**:\n",
    "\n",
    "$$c_n | \\theta \\sim \\text{Bernoulli}(\\theta)$$\n",
    "$$x_n | \\mu_{c(n)}, \\sigma_{c(n)}^2 \\sim \\mathcal{N}(\\mu_{c(n)}, \\sigma_{c(n)}^2)$$\n",
    "\n",
    "**Posterior Probability of Category 1**:\n",
    "\n",
    "$$P(c=1|x) = \\frac{\\theta \\, \\mathcal{N}(x; \\mu_1, \\sigma_1^2)}{\\theta \\, \\mathcal{N}(x; \\mu_1, \\sigma_1^2) + (1-\\theta) \\, \\mathcal{N}(x; \\mu_2, \\sigma_2^2)}$$\n",
    "\n",
    "**Marginal Distribution** (Predictive):\n",
    "\n",
    "$$p(x) = \\theta \\, \\mathcal{N}(x; \\mu_1, \\sigma_1^2) + (1-\\theta) \\, \\mathcal{N}(x; \\mu_2, \\sigma_2^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_categorization(x_range, mu_1, mu_2, sigma_1_sq, sigma_2_sq, theta):\n",
    "    \"\"\"\n",
    "    Compute posterior probability of category 1 and marginal distribution.\n",
    "    \n",
    "    Args:\n",
    "        x_range: Range of x values\n",
    "        mu_1, mu_2: Means of categories 1 and 2\n",
    "        sigma_1_sq, sigma_2_sq: Variances of categories 1 and 2\n",
    "        theta: Prior probability of category 1\n",
    "    \n",
    "    Returns:\n",
    "        posterior_c1: P(c=1|x)\n",
    "        marginal: p(x)\n",
    "        likelihood_1: p(x|c=1)\n",
    "        likelihood_2: p(x|c=2)\n",
    "    \"\"\"\n",
    "    # Likelihoods\n",
    "    likelihood_1 = scipy_norm.pdf(x_range, mu_1, np.sqrt(sigma_1_sq))\n",
    "    likelihood_2 = scipy_norm.pdf(x_range, mu_2, np.sqrt(sigma_2_sq))\n",
    "    \n",
    "    # Marginal distribution\n",
    "    marginal = theta * likelihood_1 + (1 - theta) * likelihood_2\n",
    "    \n",
    "    # Posterior probability of category 1\n",
    "    posterior_c1 = (theta * likelihood_1) / marginal\n",
    "    \n",
    "    return posterior_c1, marginal, likelihood_1, likelihood_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Visualization: Effect of Prior (Î¸) on Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorization(theta, sigma_1_sq=1.0, sigma_2_sq=1.0):\n",
    "    \"\"\"\n",
    "    Interactive plot of categorization decisions.\n",
    "    \"\"\"\n",
    "    # Fixed means\n",
    "    mu_1 = -1.0\n",
    "    mu_2 = 1.0\n",
    "    \n",
    "    # x range\n",
    "    x_range = np.linspace(-6, 6, 1000)\n",
    "    \n",
    "    # Compute probabilities\n",
    "    post_c1, marginal, lik_1, lik_2 = compute_categorization(\n",
    "        x_range, mu_1, mu_2, sigma_1_sq, sigma_2_sq, theta\n",
    "    )\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Plot 1: Likelihoods\n",
    "    axes[0].plot(x_range, lik_1, 'b-', linewidth=2, \n",
    "                label=f'Likelihood c=1: N({mu_1}, {sigma_1_sq})', alpha=0.7)\n",
    "    axes[0].plot(x_range, lik_2, 'r-', linewidth=2, \n",
    "                label=f'Likelihood c=2: N({mu_2}, {sigma_2_sq})', alpha=0.7)\n",
    "    axes[0].axvline(mu_1, color='blue', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[0].axvline(mu_2, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[0].set_xlabel('x', fontsize=12)\n",
    "    axes[0].set_ylabel('Density', fontsize=12)\n",
    "    axes[0].set_title('Likelihood Distributions', fontsize=13)\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Posterior probability of c=1\n",
    "    axes[1].plot(x_range, post_c1, 'purple', linewidth=2, \n",
    "                label=f'P(c=1|x), Î¸={theta}')\n",
    "    axes[1].axhline(0.5, color='black', linestyle=':', linewidth=1, alpha=0.5, label='Decision boundary')\n",
    "    axes[1].fill_between(x_range, 0, post_c1, alpha=0.3, color='blue', label='Classify as c=1')\n",
    "    axes[1].fill_between(x_range, post_c1, 1, alpha=0.3, color='red', label='Classify as c=2')\n",
    "    \n",
    "    # Find decision boundary (where P(c=1|x) = 0.5)\n",
    "    decision_idx = np.argmin(np.abs(post_c1 - 0.5))\n",
    "    decision_x = x_range[decision_idx]\n",
    "    axes[1].axvline(decision_x, color='green', linestyle='--', linewidth=2, \n",
    "                   label=f'Decision boundary: x={decision_x:.2f}')\n",
    "    \n",
    "    axes[1].set_xlabel('x', fontsize=12)\n",
    "    axes[1].set_ylabel('P(c=1|x)', fontsize=12)\n",
    "    axes[1].set_title(f'Posterior Probability of Category 1 (Î¸={theta})', fontsize=13)\n",
    "    axes[1].set_ylim([-0.05, 1.05])\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print interpretation\n",
    "    print(f\"\\nðŸ“Š Interpretation:\")\n",
    "    print(f\"  Prior P(c=1) = {theta:.2f}\")\n",
    "    print(f\"  Decision boundary at x = {decision_x:.2f}\")\n",
    "    print(f\"  Variance ratio ÏƒÂ²_1/ÏƒÂ²_2 = {sigma_1_sq/sigma_2_sq:.2f}\")\n",
    "    if theta > 0.5:\n",
    "        print(f\"  â†’ Prior favors c=1, so decision boundary shifts toward c=2\")\n",
    "    elif theta < 0.5:\n",
    "        print(f\"  â†’ Prior favors c=2, so decision boundary shifts toward c=1\")\n",
    "    else:\n",
    "        print(f\"  â†’ Equal prior, decision boundary at midpoint (if equal variances)\")\n",
    "\n",
    "# Create interactive widget\n",
    "interact(plot_categorization,\n",
    "         theta=FloatSlider(min=0.1, max=0.9, step=0.05, value=0.5, description='Î¸ (prior P(c=1))'),\n",
    "         sigma_1_sq=FloatSlider(min=0.25, max=3.0, step=0.25, value=1.0, description='ÏƒÂ²_1'),\n",
    "         sigma_2_sq=FloatSlider(min=0.25, max=3.0, step=0.25, value=1.0, description='ÏƒÂ²_2'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Visualization: Marginal Distribution (Mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_marginal_distribution(theta, sigma_1_sq=1.0, sigma_2_sq=1.0):\n",
    "    \"\"\"\n",
    "    Interactive plot of marginal distribution (mixture).\n",
    "    \"\"\"\n",
    "    # Fixed means\n",
    "    mu_1 = -1.0\n",
    "    mu_2 = 1.0\n",
    "    \n",
    "    # x range\n",
    "    x_range = np.linspace(-6, 6, 1000)\n",
    "    \n",
    "    # Compute probabilities\n",
    "    post_c1, marginal, lik_1, lik_2 = compute_categorization(\n",
    "        x_range, mu_1, mu_2, sigma_1_sq, sigma_2_sq, theta\n",
    "    )\n",
    "    \n",
    "    # Weighted components\n",
    "    weighted_1 = theta * lik_1\n",
    "    weighted_2 = (1 - theta) * lik_2\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    \n",
    "    # Plot components\n",
    "    ax.plot(x_range, weighted_1, 'b--', linewidth=2, alpha=0.6,\n",
    "           label=f'Î¸ Ã— N(x; {mu_1}, {sigma_1_sq}) [weight={theta:.2f}]')\n",
    "    ax.plot(x_range, weighted_2, 'r--', linewidth=2, alpha=0.6,\n",
    "           label=f'(1-Î¸) Ã— N(x; {mu_2}, {sigma_2_sq}) [weight={1-theta:.2f}]')\n",
    "    \n",
    "    # Plot marginal (mixture)\n",
    "    ax.plot(x_range, marginal, 'purple', linewidth=3,\n",
    "           label='Marginal p(x) = Mixture')\n",
    "    \n",
    "    ax.fill_between(x_range, 0, marginal, alpha=0.2, color='purple')\n",
    "    \n",
    "    ax.set_xlabel('x', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title(f'Marginal Distribution: p(x) = Î¸Â·N(x;Î¼â‚,ÏƒÂ²â‚) + (1-Î¸)Â·N(x;Î¼â‚‚,ÏƒÂ²â‚‚)', fontsize=13)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print interpretation\n",
    "    print(f\"\\nðŸ“Š Interpretation:\")\n",
    "    print(f\"  Mixture weights: Î¸={theta:.2f} for c=1, (1-Î¸)={1-theta:.2f} for c=2\")\n",
    "    \n",
    "    # Find peaks\n",
    "    peak_indices = [i for i in range(1, len(marginal)-1) \n",
    "                   if marginal[i] > marginal[i-1] and marginal[i] > marginal[i+1]]\n",
    "    n_peaks = len(peak_indices)\n",
    "    \n",
    "    if n_peaks == 2:\n",
    "        print(f\"  â†’ Bimodal: two clear peaks (one from each component)\")\n",
    "    elif n_peaks == 1:\n",
    "        print(f\"  â†’ Unimodal: components merged into single peak\")\n",
    "    else:\n",
    "        # Check if plateau\n",
    "        max_density = np.max(marginal)\n",
    "        plateau_range = x_range[marginal > 0.95 * max_density]\n",
    "        if len(plateau_range) > 100:  # arbitrary threshold\n",
    "            print(f\"  â†’ Plateau: components form flat top\")\n",
    "        else:\n",
    "            print(f\"  â†’ Complex shape with {n_peaks} peaks\")\n",
    "\n",
    "# Create interactive widget\n",
    "interact(plot_marginal_distribution,\n",
    "         theta=FloatSlider(min=0.1, max=0.9, step=0.05, value=0.5, description='Î¸ (mixing weight)'),\n",
    "         sigma_1_sq=FloatSlider(min=0.25, max=3.0, step=0.25, value=1.0, description='ÏƒÂ²_1'),\n",
    "         sigma_2_sq=FloatSlider(min=0.25, max=3.0, step=0.25, value=1.0, description='ÏƒÂ²_2'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: GenJAX Simulation - Mixture Model\n",
    "\n",
    "Let's implement and simulate from the mixture model using GenJAX!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def gaussian_mixture_model(theta, mu_1, mu_2, sigma_1, sigma_2):\n",
    "    \"\"\"\n",
    "    Generative model for Gaussian mixture.\n",
    "    \n",
    "    Args:\n",
    "        theta: Prior probability of category 1\n",
    "        mu_1, mu_2: Means of categories 1 and 2\n",
    "        sigma_1, sigma_2: Standard deviations of categories 1 and 2\n",
    "    \"\"\"\n",
    "    # Sample category\n",
    "    c = jnp.bernoulli(theta) @ \"category\"\n",
    "    \n",
    "    # Sample observation based on category\n",
    "    if c == 1:\n",
    "        x = jnp.normal(mu_1, sigma_1) @ \"observation\"\n",
    "    else:\n",
    "        x = jnp.normal(mu_2, sigma_2) @ \"observation\"\n",
    "    \n",
    "    return x, c\n",
    "\n",
    "# Simulate from mixture model\n",
    "def simulate_mixture(theta=0.5, mu_1=-1.0, mu_2=1.0, sigma_1=1.0, sigma_2=1.0, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Simulate from Gaussian mixture model.\n",
    "    \"\"\"\n",
    "    key = random.PRNGKey(42)\n",
    "    \n",
    "    observations = []\n",
    "    categories = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        key, subkey = random.split(key)\n",
    "        trace = simulate(gaussian_mixture_model)(subkey, theta, mu_1, mu_2, sigma_1, sigma_2)\n",
    "        x, c = trace.get_retval()\n",
    "        observations.append(float(x))\n",
    "        categories.append(int(c))\n",
    "    \n",
    "    return np.array(observations), np.array(categories)\n",
    "\n",
    "# Run simulation\n",
    "print(\"Simulating from mixture model...\")\n",
    "obs, cats = simulate_mixture(theta=0.7, mu_1=-1.0, mu_2=1.0, sigma_1=1.0, sigma_2=1.0, n_samples=2000)\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of all observations\n",
    "axes[0].hist(obs, bins=50, density=True, alpha=0.7, edgecolor='black', color='purple')\n",
    "axes[0].set_xlabel('x', fontsize=12)\n",
    "axes[0].set_ylabel('Density', fontsize=12)\n",
    "axes[0].set_title('Simulated Observations from Mixture', fontsize=13)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram by category\n",
    "obs_c1 = obs[cats == 1]\n",
    "obs_c2 = obs[cats == 0]\n",
    "axes[1].hist(obs_c1, bins=30, density=True, alpha=0.6, edgecolor='black', color='blue', label=f'Category 1 (n={len(obs_c1)})')\n",
    "axes[1].hist(obs_c2, bins=30, density=True, alpha=0.6, edgecolor='black', color='red', label=f'Category 2 (n={len(obs_c2)})')\n",
    "axes[1].set_xlabel('x', fontsize=12)\n",
    "axes[1].set_ylabel('Density', fontsize=12)\n",
    "axes[1].set_title('Observations by True Category', fontsize=13)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Simulation Summary:\")\n",
    "print(f\"  Total samples: {len(obs)}\")\n",
    "print(f\"  Category 1: {len(obs_c1)} ({len(obs_c1)/len(obs)*100:.1f}%)\")\n",
    "print(f\"  Category 2: {len(obs_c2)} ({len(obs_c2)/len(obs)*100:.1f}%)\")\n",
    "print(f\"  Overall mean: {np.mean(obs):.2f}\")\n",
    "print(f\"  Category 1 mean: {np.mean(obs_c1):.2f}\")\n",
    "print(f\"  Category 2 mean: {np.mean(obs_c2):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook explored:\n",
    "\n",
    "### Part 1: Gaussian-Gaussian Bayesian Update\n",
    "- **Likelihood variance** effect: Smaller ÏƒÂ²_x â†’ sharper posterior (data more influential)\n",
    "- **Number of observations** effect: More data â†’ narrower posterior (less uncertainty)\n",
    "- **Sequential learning**: Posterior converges to sample mean as n â†’ âˆž\n",
    "- **Predictive distribution**: Has larger variance than posterior (adds data uncertainty)\n",
    "\n",
    "### Part 2: Gaussian Mixture Models\n",
    "- **Prior Î¸** effect: Shifts decision boundary, acts as \"magnifier\" for one component\n",
    "- **Variance ratio** effect: Component with larger variance dominates tails\n",
    "- **Marginal distribution**: Weighted sum of components, can be bimodal or unimodal\n",
    "- **Decision boundary**: Where P(c=1|x) = 0.5, depends on both Î¸ and variance ratios\n",
    "\n",
    "### GenJAX Implementation\n",
    "- Analytical updates for conjugate priors\n",
    "- Simulation-based approaches for verification\n",
    "- Full generative models for mixture distributions\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**: Apply these concepts to the assignment problems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
