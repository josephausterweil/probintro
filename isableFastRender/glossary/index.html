<!DOCTYPE html>
<html lang="en-us" dir="ltr" itemscope itemtype="http://schema.org/Article" data-r-output-format="html">
  <head><script src="/probintro/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=probintro/livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.148.2">
    <meta name="generator" content="Relearn 8.0.0+9803d5122ebb3276acea823f476e9eb44f607862">
    <meta name="description" content="How to Use This Glossary This glossary covers all three tutorials in the Probability with GenJAX series. Terms are tagged to show which tutorial introduces them:
ðŸ“˜ Tutorial 1 (Discrete Probability) - Sets and counting approach ðŸ’» Tutorial 2 (GenJAX Programming) - Probabilistic programming basics ðŸ“Š Tutorial 3 (Continuous Probability) - Advanced topics and Bayesian learning Click on any term to expand its definition with examples and code.
Core Concepts (Tutorial 1) Bayes Theorem ðŸ“˜ Bayes Theorem Bayes Theorem (or Bayesâ€™ rule) is a formula for reversing the order that variables are conditioned â€” how to go from $P(A \mid B)$ to $P(B \mid A)$.">
    <meta name="author" content="Joseph Austerweil">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Glossary - All Tutorials :: Probability & Probabilistic Computing Tutorial">
    <meta name="twitter:description" content="How to Use This Glossary This glossary covers all three tutorials in the Probability with GenJAX series. Terms are tagged to show which tutorial introduces them:
ðŸ“˜ Tutorial 1 (Discrete Probability) - Sets and counting approach ðŸ’» Tutorial 2 (GenJAX Programming) - Probabilistic programming basics ðŸ“Š Tutorial 3 (Continuous Probability) - Advanced topics and Bayesian learning Click on any term to expand its definition with examples and code.
Core Concepts (Tutorial 1) Bayes Theorem ðŸ“˜ Bayes Theorem Bayes Theorem (or Bayesâ€™ rule) is a formula for reversing the order that variables are conditioned â€” how to go from $P(A \mid B)$ to $P(B \mid A)$.">
    <meta property="og:url" content="http://localhost:1313/probintro/glossary/index.html">
    <meta property="og:site_name" content="Probability & Probabilistic Computing Tutorial">
    <meta property="og:title" content="Glossary - All Tutorials :: Probability & Probabilistic Computing Tutorial">
    <meta property="og:description" content="How to Use This Glossary This glossary covers all three tutorials in the Probability with GenJAX series. Terms are tagged to show which tutorial introduces them:
ðŸ“˜ Tutorial 1 (Discrete Probability) - Sets and counting approach ðŸ’» Tutorial 2 (GenJAX Programming) - Probabilistic programming basics ðŸ“Š Tutorial 3 (Continuous Probability) - Advanced topics and Bayesian learning Click on any term to expand its definition with examples and code.
Core Concepts (Tutorial 1) Bayes Theorem ðŸ“˜ Bayes Theorem Bayes Theorem (or Bayesâ€™ rule) is a formula for reversing the order that variables are conditioned â€” how to go from $P(A \mid B)$ to $P(B \mid A)$.">
    <meta property="og:locale" content="en_us">
    <meta property="og:type" content="article">
    <meta property="article:section" content="Probability & Probabilistic Computing Tutorial">
    <meta itemprop="name" content="Glossary - All Tutorials :: Probability & Probabilistic Computing Tutorial">
    <meta itemprop="description" content="How to Use This Glossary This glossary covers all three tutorials in the Probability with GenJAX series. Terms are tagged to show which tutorial introduces them:
ðŸ“˜ Tutorial 1 (Discrete Probability) - Sets and counting approach ðŸ’» Tutorial 2 (GenJAX Programming) - Probabilistic programming basics ðŸ“Š Tutorial 3 (Continuous Probability) - Advanced topics and Bayesian learning Click on any term to expand its definition with examples and code.
Core Concepts (Tutorial 1) Bayes Theorem ðŸ“˜ Bayes Theorem Bayes Theorem (or Bayesâ€™ rule) is a formula for reversing the order that variables are conditioned â€” how to go from $P(A \mid B)$ to $P(B \mid A)$.">
    <meta itemprop="wordCount" content="5367">
    <title>Glossary - All Tutorials :: Probability &amp; Probabilistic Computing Tutorial</title>
    <link href="/probintro/images/favicon.png?1764989345" rel="icon" type="image/png">
    <link href="/probintro/css/auto-complete/auto-complete.min.css?1764989345" rel="stylesheet">
    <script src="/probintro/js/auto-complete/auto-complete.min.js?1764989345" defer></script>
    <script src="/probintro/js/search-lunr.js?1764989345" defer></script>
    <script src="/probintro/js/search.js?1764989345" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.index_js_url="/probintro/searchindex.en.js?1764989345";
    </script>
    <script src="/probintro/js/lunr/lunr.min.js?1764989345" defer></script>
    <script src="/probintro/js/lunr/lunr.stemmer.support.min.js?1764989345" defer></script>
    <script src="/probintro/js/lunr/lunr.multi.min.js?1764989345" defer></script>
    <script src="/probintro/js/lunr/lunr.en.min.js?1764989345" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.contentLangs=['en'];
    </script>
    <link href="/probintro/fonts/fontawesome/css/fontawesome-all.min.css?1764989345" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/probintro/fonts/fontawesome/css/fontawesome-all.min.css?1764989345" rel="stylesheet"></noscript>
    <link href="/probintro/css/perfect-scrollbar/perfect-scrollbar.min.css?1764989345" rel="stylesheet">
    <link href="/probintro/css/theme.css?1764989345" rel="stylesheet">
    <link href="/probintro/css/format-html.css?1764989345" rel="stylesheet" id="R-format-style">
    <script>
      window.relearn = window.relearn || {};
      // configuration
      window.relearn.min = ``;
      window.relearn.path='\/glossary\/index.html';
      window.relearn.relBasePath='..';
      window.relearn.relBaseUri='..\/..';
      window.relearn.absBaseUri='http:\/\/localhost:1313\/probintro';
      window.relearn.disableAnchorCopy=false;
      window.relearn.disableAnchorScrolling=false;
      window.relearn.disableInlineCopyToClipboard=false;
      window.relearn.enableBlockCodeWrap=true;
      // legal
      window.relearn.getItem = (s,n) => {return s.getItem(n)};
      window.relearn.setItem = (s,n,v) => {return s.setItem(n,v)};
      window.relearn.removeItem = (s,n) => {return s.removeItem(n)};
      // translations
      window.T_Copy_to_clipboard = `Copy to clipboard`;
      window.T_Copied_to_clipboard = `Copied to clipboard!`;
      window.T_Copy_link_to_clipboard = `Copy link to clipboard`;
      window.T_Link_copied_to_clipboard = `Copied link to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
      // variant stuff
      window.relearn.themevariants = [ 'relearn-dark' ];
      window.relearn.customvariantname = "my-custom-variant";
      window.relearn.changeVariant = function(variant) {
        var oldVariant = document.documentElement.dataset.rThemeVariant;
        window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        document.documentElement.dataset.rThemeVariant = variant;
        if (oldVariant != variant) {
          document.dispatchEvent( new CustomEvent('themeVariantLoaded', { detail: { variant, oldVariant } }) );
          window.relearn.markVariant();
        }
      }
      window.relearn.markVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant");
        document.querySelectorAll(".R-variantswitcher select").forEach((select) => {select.value = variant;});
      }
      window.relearn.initVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant") ?? "";
        if( variant == window.relearn.customvariantname ){
        }else if( !variant || !window.relearn.themevariants.includes(variant) ){
          variant = window.relearn.themevariants[0];
          window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        }
        document.documentElement.dataset.rThemeVariant = variant;
      }
      window.relearn.initVariant();
      window.relearn.markVariant();
    </script>

<link rel="stylesheet" href="/probintro/css/custom.css">

  </head>
  <body class="mobile-support html" data-url="/probintro/glossary/index.html">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide"><button class="topbar-control" onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button>
            </div>
            <div class="topbar-button topbar-button-toc" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="Table of Contents (CTRL&#43;ALT&#43;t)"><i class="fa-fw fas fa-list-alt"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
<nav class="TableOfContents">
  <ul>
    <li><a href="#how-to-use-this-glossary">How to Use This Glossary</a></li>
    <li><a href="#core-concepts-tutorial-1">Core Concepts (Tutorial 1)</a>
      <ul>
        <li><a href="#bayes-theorem-">Bayes Theorem ðŸ“˜</a></li>
        <li><a href="#cardinality-">Cardinality ðŸ“˜</a></li>
        <li><a href="#conditional-probability-">Conditional Probability ðŸ“˜</a></li>
        <li><a href="#dependence-">Dependence ðŸ“˜</a></li>
        <li><a href="#event-">Event ðŸ“˜</a></li>
        <li><a href="#generative-process-">Generative Process ðŸ“˜ðŸ’»</a></li>
        <li><a href="#joint-probability-">Joint Probability ðŸ“˜</a></li>
        <li><a href="#marginal-probability-">Marginal Probability ðŸ“˜</a></li>
        <li><a href="#outcome-space-">Outcome Space ðŸ“˜</a></li>
        <li><a href="#probability-">Probability ðŸ“˜</a></li>
        <li><a href="#random-variable-">Random Variable ðŸ“˜</a></li>
        <li><a href="#set-">Set ðŸ“˜</a></li>
      </ul>
    </li>
    <li><a href="#genjax-programming-tutorial-2">GenJAX Programming (Tutorial 2)</a>
      <ul>
        <li><a href="#gen-decorator-">@gen Decorator ðŸ’»</a></li>
        <li><a href="#bernoulli-distribution-">Bernoulli Distribution ðŸ’»</a></li>
        <li><a href="#flip-">flip() ðŸ’»</a></li>
        <li><a href="#categorical-distribution-">Categorical Distribution ðŸ’»ðŸ“Š</a></li>
        <li><a href="#choicemap-">ChoiceMap ðŸ’»</a></li>
        <li><a href="#generative-function-">Generative Function ðŸ’»</a></li>
        <li><a href="#importance-sampling-">Importance Sampling ðŸ’»ðŸ“Š</a></li>
        <li><a href="#jax-key-">JAX Key ðŸ’»</a></li>
        <li><a href="#monte-carlo-simulation-">Monte Carlo Simulation ðŸ“˜ðŸ’»</a></li>
        <li><a href="#normal-distribution-">Normal Distribution ðŸ’»ðŸ“Š</a></li>
        <li><a href="#simulate-">simulate() ðŸ’»</a></li>
        <li><a href="#target-">Target ðŸ’»</a></li>
        <li><a href="#trace-">Trace ðŸ’»</a></li>
        <li><a href="#vmap-">vmap ðŸ’»</a></li>
      </ul>
    </li>
    <li><a href="#continuous-probability-tutorial-3">Continuous Probability (Tutorial 3)</a>
      <ul>
        <li><a href="#beta-distribution-">Beta Distribution ðŸ“Š</a></li>
        <li><a href="#chinese-restaurant-process-crp-">Chinese Restaurant Process (CRP) ðŸ“Š</a></li>
        <li><a href="#concentration-parameter-Î±-">Concentration Parameter (Î±) ðŸ“Š</a></li>
        <li><a href="#conjugate-prior-">Conjugate Prior ðŸ“Š</a></li>
        <li><a href="#cumulative-distribution-function-cdf-">Cumulative Distribution Function (CDF) ðŸ“Š</a></li>
        <li><a href="#dirichlet-distribution-">Dirichlet Distribution ðŸ“Š</a></li>
        <li><a href="#dirichlet-process-dp-">Dirichlet Process (DP) ðŸ“Š</a></li>
        <li><a href="#dirichlet-process-mixture-model-dpmm-">Dirichlet Process Mixture Model (DPMM) ðŸ“Š</a></li>
        <li><a href="#expected-value-">Expected Value ðŸ“Š</a></li>
        <li><a href="#gaussian-distribution-">Gaussian Distribution ðŸ“Š</a></li>
        <li><a href="#gaussian-mixture-model-gmm-">Gaussian Mixture Model (GMM) ðŸ“Š</a></li>
        <li><a href="#likelihood-">Likelihood ðŸ“Š</a></li>
        <li><a href="#mixture-model-">Mixture Model ðŸ“Š</a></li>
        <li><a href="#posterior-distribution-">Posterior Distribution ðŸ“Š</a></li>
        <li><a href="#predictive-distribution-">Predictive Distribution ðŸ“Š</a></li>
        <li><a href="#prior-distribution-">Prior Distribution ðŸ“Š</a></li>
        <li><a href="#probability-density-function-pdf-">Probability Density Function (PDF) ðŸ“Š</a></li>
        <li><a href="#standard-normal-">Standard Normal ðŸ“Š</a></li>
        <li><a href="#stick-breaking-construction-">Stick-Breaking Construction ðŸ“Š</a></li>
        <li><a href="#truncation-in-dpmm-">Truncation (in DPMM) ðŸ“Š</a></li>
        <li><a href="#uniform-distribution-">Uniform Distribution ðŸ“Š</a></li>
        <li><a href="#variance-">Variance ðŸ“Š</a></li>
        <li><a href="#weight-degeneracy-">Weight Degeneracy ðŸ“Š</a></li>
      </ul>
    </li>
    <li><a href="#navigation">Navigation</a></li>
  </ul>
</nav>
                </div>
              </div>
            </div>
          </div>
          <ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype="http://schema.org/BreadcrumbList"><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/probintro/index.html"><span itemprop="name">Probability &amp; Probabilistic Computing Tutorial</span></a><meta itemprop="position" content="1">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><span itemprop="name">Glossary - All Tutorials</span><meta itemprop="position" content="2"></li>
          </ol>
          <div class="topbar-area topbar-area-end" data-area="end">
            <div class="topbar-button topbar-button-prev" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/probintro/notebook_guide/index.html" title="Interactive Notebooks - All Tutorials (ðŸ¡)"><i class="fa-fw fas fa-chevron-left"></i></a>
            </div>
            <div class="topbar-button topbar-button-next" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><span class="topbar-control"><i class="fa-fw fas fa-chevron-right"></i></span>
            </div>
            <div class="topbar-button topbar-button-more" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="More"><i class="fa-fw fas fa-ellipsis-v"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
                  <div class="topbar-area topbar-area-more" data-area="more">
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable page" tabindex="-1">
        <div class="flex-block-wrapper">
<article class="default">
  <header class="headline">
  </header>

<h1 id="glossary---all-tutorials">Glossary - All Tutorials</h1>

<h2 id="how-to-use-this-glossary">How to Use This Glossary</h2>
<p>This glossary covers all three tutorials in the Probability with GenJAX series. Terms are tagged to show which tutorial introduces them:</p>
<ul>
<li>ðŸ“˜ <strong>Tutorial 1</strong> (Discrete Probability) - Sets and counting approach</li>
<li>ðŸ’» <strong>Tutorial 2</strong> (GenJAX Programming) - Probabilistic programming basics</li>
<li>ðŸ“Š <strong>Tutorial 3</strong> (Continuous Probability) - Advanced topics and Bayesian learning</li>
</ul>
<p>Click on any term to expand its definition with examples and code.</p>
<hr>
<h2 id="core-concepts-tutorial-1">Core Concepts (Tutorial 1)</h2>
<h3 id="bayes-theorem-">Bayes Theorem ðŸ“˜</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Bayes Theorem
  </summary>
  <div class="box-content">
<p><em>Bayes Theorem</em> (or Bayes&rsquo; rule) is a formula for reversing the order that variables are conditioned â€” how to go from $P(A \mid B)$ to $P(B \mid A)$.</p>
<p><strong>Formula:</strong> $P(H \mid D) = \frac{P(D \mid H) P(H)}{P(D)}$</p>
<p><strong>Components:</strong></p>
<ul>
<li>$P(H \mid D)$ = posterior (updated belief after seeing data)</li>
<li>$P(D \mid H)$ = likelihood (how well data fits hypothesis)</li>
<li>$P(H)$ = prior (belief before seeing data)</li>
<li>$P(D)$ = evidence (total probability of data)</li>
</ul>
<p><strong>Application:</strong> Updating beliefs with new information</p>
<p><strong>See also</strong>: Prior, Posterior, Likelihood</p>
  </div>
</details>
<h3 id="cardinality-">Cardinality ðŸ“˜</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Cardinality
  </summary>
  <div class="box-content">
<p>The <em>cardinality</em> or <em>size</em> of a set is the number of elements it contains. If $A = \{H, T\}$, then the cardinality of $A$ is $|A|=2$.</p>
<p><strong>Notation:</strong> $|A|$ means &ldquo;the size of set $A$&rdquo;</p>
<p><strong>In programming</strong>: This is like <code>len(A)</code> in Python or counting array elements</p>
  </div>
</details>
<h3 id="conditional-probability-">Conditional Probability ðŸ“˜</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Conditional Probability
  </summary>
  <div class="box-content">
<p>The <em>conditional probability</em> is the probability of an event conditioned on knowledge of another event. Conditioning on an event means that the possible outcomes in that event form the set of possibilities or outcome space. We then calculate probabilities as normal within that <em>restricted</em> outcome space.</p>
<p><strong>Formally:</strong> $P(A \mid B) = \frac{|A \cap B|}{|B|}$, where everything to the left of the $\mid$ is what we&rsquo;re interested in knowing the probability of and everything to the right of the $\mid$ is what we know to be true.</p>
<p><strong>Alternative formula:</strong> $P(A \mid B) = \frac{P(A,B)}{P(B)}$ (assuming $P(B) &gt; 0$)</p>
<p><strong>In GenJAX ðŸ’»</strong>: We condition using <code>ChoiceMap</code> to specify observed values</p>
  </div>
</details>
<h3 id="dependence-">Dependence ðŸ“˜</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Dependence
  </summary>
  <div class="box-content">
<p>When knowing the outcome of one random variable or event influences the probability of another, those variables or events are called <em>dependent</em>. This is denoted as $A \not\perp B$.</p>
<p>When they do not influence each other, they are called <em>independent</em>. This is denoted as $A \perp B$.</p>
<p><strong>Formal definition of independence:</strong> $P(A \mid B) = P(A)$, or equivalently, $P(A, B) = P(A) \times P(B)$</p>
<p><strong>Example</strong>: Coin flips are independent (one doesn&rsquo;t affect the next). Drawing cards without replacement is dependent (first draw affects second).</p>
  </div>
</details>
<h3 id="event-">Event ðŸ“˜</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Event
  </summary>
  <div class="box-content">
<p>An <em>event</em> is a set that contains none, some, or all of the possible outcomes. In other words, an event is any subset of the outcome space $\Omega$.</p>
<p><strong>Example:</strong> &ldquo;At least one tonkatsu&rdquo; is the event $\{HT, TH, TT\} \subseteq \Omega$.</p>
<p><strong>In programming</strong>: Events correspond to filtering/counting samples that satisfy a condition</p>
  </div>
</details>
<h3 id="generative-process-">Generative Process ðŸ“˜ðŸ’»</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Generative Process
  </summary>
  <div class="box-content">
<p>A <em>generative process</em> defines the probabilities for possible outcomes according to an algorithm with random choices. Think of it as a recipe for producing outcomes.</p>
<p><strong>Example:</strong> &ldquo;Flip two coins: first for lunch (H or T), second for dinner (H or T). Record the pair.&rdquo;</p>
<p><strong>In GenJAX ðŸ’»</strong>: We write generative processes as <code>@gen</code> decorated functions</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@gen</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">chibany_day</span>():
</span></span><span style="display:flex;"><span>    lunch <span style="color:#f92672">=</span> flip(<span style="color:#ae81ff">0.5</span>) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;lunch&#34;</span>
</span></span><span style="display:flex;"><span>    dinner <span style="color:#f92672">=</span> flip(<span style="color:#ae81ff">0.5</span>) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;dinner&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> (lunch, dinner)</span></span></code></pre></td></tr></table>
</div>
</div>
<p>This connects probabilistic thinking to actual executable code!</p>
  </div>
</details>
<h3 id="joint-probability-">Joint Probability ðŸ“˜</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Joint Probability
  </summary>
  <div class="box-content">
<p>The <em>joint probability</em> is the probability that multiple events all occur. This corresponds to the intersection of the events (outcomes that are in all the events).</p>
<p><strong>Notation:</strong> $P(A, B)$ or $P(A \cap B)$</p>
<p><strong>Intuition:</strong> &ldquo;What&rsquo;s the probability that both $A$ and $B$ happen?&rdquo;</p>
<p><strong>Example</strong>: $P(\text{lunch}=T, \text{dinner}=T) = P(TT)$</p>
  </div>
</details>
<h3 id="marginal-probability-">Marginal Probability ðŸ“˜</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Marginal Probability
  </summary>
  <div class="box-content">
<p>A <em>marginal probability</em> is the probability of a random variable that has been calculated by summing over the possible values of one or more other random variables.</p>
<p><strong>Formula:</strong> $P(A) = \sum_{b} P(A, B=b)$</p>
<p><strong>Intuition:</strong> &ldquo;What&rsquo;s the probability of $A$ regardless of what $B$ is?&rdquo;</p>
<p><strong>Example</strong>: $P(\text{lunch}=T) = P(TH) + P(TT)$ (tonkatsu for lunch, regardless of dinner)</p>
  </div>
</details>
<h3 id="outcome-space-">Outcome Space ðŸ“˜</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Outcome Space
  </summary>
  <div class="box-content">
<p>The <em>outcome space</em> (denoted $\Omega$, the Greek letter omega) is the set of all possible outcomes for a random process. It forms the foundation for calculating probabilities.</p>
<p><strong>Example:</strong> For Chibany&rsquo;s two daily meals, $\Omega = \{HH, HT, TH, TT\}$.</p>
<p><strong>In GenJAX ðŸ’»</strong>: We generate outcomes from the outcome space by running <code>simulate()</code> many times</p>
  </div>
</details>
<h3 id="probability-">Probability ðŸ“˜</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Probability
  </summary>
  <div class="box-content">
<p>The <em>probability</em> of an event $A$ relative to an outcome space $\Omega$ is the ratio of their sizes: $P(A) = \frac{|A|}{|\Omega|}$.</p>
<p>When outcomes are weighted (not equally likely), we sum the weights instead of counting.</p>
<p><strong>Interpretation:</strong> &ldquo;What fraction of possible outcomes are in event $A$?&rdquo;</p>
<p><strong>In code</strong>: We approximate this by simulation: run the process many times and compute the fraction of runs where the event occurs.</p>
  </div>
</details>
<h3 id="random-variable-">Random Variable ðŸ“˜</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Random Variable
  </summary>
  <div class="box-content">
<p>A <em>random variable</em> is a function that maps from the set of possible outcomes to some set or space. The output or range of the function could be the set of outcomes again, a whole number based on the outcome (e.g., counting the number of Tonkatsu), or something more complex.</p>
<p>Technically the output must be <em>measurable</em>. You shouldn&rsquo;t worry about that distinction unless your random variable&rsquo;s output gets really, really big (like continuous). We&rsquo;ll talk more about probabilities over continuous random variables in Tutorial 3 ðŸ“Š.</p>
<p><strong>Key insight:</strong> It&rsquo;s called &ldquo;random&rdquo; because its value depends on which outcome occurs, but it&rsquo;s really just a function!</p>
<p><strong>Example</strong>: $X(\omega)$ = number of tonkatsu meals in outcome $\omega$</p>
  </div>
</details>
<h3 id="set-">Set ðŸ“˜</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Set
  </summary>
  <div class="box-content">
<p>A <em>set</em> is a collection of elements or members. Sets are defined by the elements they do or do not contain. The elements are listed with commas between them and &ldquo;$\{$&rdquo; denotes the start of a set and &ldquo;$\}$&rdquo; the end of a set. Note that the elements of a set are unique.</p>
<p><strong>Example:</strong> $\{H, T\}$ is a set containing two elements: H and T.</p>
<p><strong>In programming</strong>: Like a Python set <code>{0, 1}</code> or a list of unique elements</p>
  </div>
</details>
<hr>
<h2 id="genjax-programming-tutorial-2">GenJAX Programming (Tutorial 2)</h2>
<h3 id="gen-decorator-">@gen Decorator ðŸ’»</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    @gen Decorator
  </summary>
  <div class="box-content">
<p>The <code>@gen</code> decorator in GenJAX marks a Python function as a <em>generative function</em> that can make addressed random choices and be used for probabilistic inference.</p>
<p><strong>Usage</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@gen</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">my_model</span>():
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> bernoulli(<span style="color:#ae81ff">0.5</span>) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;x&#34;</span>  <span style="color:#75715e"># Random choice at address &#34;x&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> x</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>What it does</strong>:</p>
<ul>
<li>Tracks all random choices made</li>
<li>Allows conditioning on observations</li>
<li>Enables inference (importance sampling, MCMC, etc.)</li>
</ul>
<p><strong>See also</strong>: Generative Function, Trace, ChoiceMap</p>
  </div>
</details>
<h3 id="bernoulli-distribution-">Bernoulli Distribution ðŸ’»</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Bernoulli Distribution
  </summary>
  <div class="box-content">
<p>A probability distribution representing a single binary trial (success/failure, 1/0, true/false). Named after mathematician Jacob Bernoulli.</p>
<p><strong>Parameter</strong>: $p$ = probability of success (returning 1)</p>
<p><strong>What it represents</strong>: A single yes/no outcome. Think of it as a biased coin flip where the coin comes up heads with probability $p$ and tails with probability $1-p$.</p>
<p><strong>In GenJAX</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@gen</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">coin_flip</span>():
</span></span><span style="display:flex;"><span>    is_heads <span style="color:#f92672">=</span> flip(<span style="color:#ae81ff">0.5</span>) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;coin&#34;</span>  <span style="color:#75715e"># 50% chance of 1 (heads)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> is_heads</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Note</strong>: In GenJAX, we use <code>flip(p)</code> instead of <code>bernoulli(p)</code> â€” the name reflects the coin flip metaphor!</p>
<p><strong>Returns</strong>: <code>True</code>/<code>1</code> (success) or <code>False</code>/<code>0</code> (failure)</p>
<p><strong>Example uses</strong>: Coin flips, yes/no questions, on/off states, binary decisions</p>
<p><strong>See also</strong>: flip(), Categorical distribution (generalization to multiple outcomes)</p>
  </div>
</details>
<h3 id="flip-">flip() ðŸ’»</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    flip()
  </summary>
  <div class="box-content">
<p>GenJAX&rsquo;s function for sampling from a Bernoulli distribution. The name reflects the coin flip metaphor.</p>
<p><strong>Signature</strong>: <code>flip(p)</code></p>
<p><strong>Parameter</strong>:</p>
<ul>
<li><code>p</code> - probability of returning <code>True</code>/<code>1</code> (like getting heads)</li>
</ul>
<p><strong>Returns</strong>: <code>True</code> or <code>False</code> (represented as <code>1</code> or <code>0</code> in JAX arrays)</p>
<p><strong>In GenJAX</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@gen</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">coin_flip</span>():
</span></span><span style="display:flex;"><span>    result <span style="color:#f92672">=</span> flip(<span style="color:#ae81ff">0.7</span>) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;coin&#34;</span>  <span style="color:#75715e"># 70% chance of True (heads)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> result</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Common values</strong>:</p>
<ul>
<li><code>flip(0.5)</code> - Fair coin flip (50/50)</li>
<li><code>flip(0.8)</code> - Biased toward True (80% chance)</li>
<li><code>flip(0.2)</code> - Biased toward False (80% chance of False)</li>
</ul>
<p><strong>Why &ldquo;flip&rdquo; instead of &ldquo;bernoulli&rdquo;?</strong> GenJAX has both functions, but they take different arguments:</p>
<ul>
<li><code>flip(p)</code> - takes a <strong>probability</strong> (0 to 1) - more intuitive for most users</li>
<li><code>bernoulli(logit)</code> - takes a <strong>logit</strong> (log-odds, -âˆž to +âˆž) - inherited from TensorFlow conventions</li>
</ul>
<p>Most users should use <code>flip()</code> as it works the way you&rsquo;d expect from probability theory (pass in 0.7 for 70% chance of true).</p>
<p><strong>See also</strong>: Bernoulli Distribution</p>
  </div>
</details>
<h3 id="categorical-distribution-">Categorical Distribution ðŸ’»ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Categorical Distribution
  </summary>
  <div class="box-content">
<p>Probability distribution over discrete outcomes with specified probabilities.</p>
<p><strong>Parameters</strong>: Array of probabilities that sum to 1.0</p>
<p><strong>In GenJAX</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@gen</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">roll_die</span>(probs):
</span></span><span style="display:flex;"><span>    outcome <span style="color:#f92672">=</span> categorical(probs) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;roll&#34;</span>  <span style="color:#75715e"># Returns 0,1,2,3,4, or 5</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> outcome</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Example</strong>: <code>categorical([0.25, 0.25, 0.25, 0.25])</code> for fair 4-sided die</p>
<p><strong>Returns</strong>: Integer index (0, 1, 2, &hellip;, k-1)</p>
<p><strong>Connection to Tutorial 1 ðŸ“˜</strong>: Generalizes the discrete outcome spaces you learned with sets</p>
<p><strong>Used in ðŸ“Š</strong>: Cluster assignment in mixture models, DPMM</p>
  </div>
</details>
<h3 id="choicemap-">ChoiceMap ðŸ’»</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    ChoiceMap
  </summary>
  <div class="box-content">
<p>GenJAX&rsquo;s way of specifying observed values for random choices. A dictionary-like structure that maps addresses (names) to values.</p>
<p><strong>Used for</strong>:</p>
<ul>
<li>Recording what random choices were made (from traces)</li>
<li>Specifying observations for inference</li>
<li>Constraining random choices</li>
</ul>
<p><strong>In code</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">9
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> genjax <span style="color:#f92672">import</span> ChoiceMap
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Observe x=2.5</span>
</span></span><span style="display:flex;"><span>observations <span style="color:#f92672">=</span> ChoiceMap<span style="color:#f92672">.</span>d({<span style="color:#e6db74">&#34;x&#34;</span>: <span style="color:#ae81ff">2.5</span>})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Or use builder pattern</span>
</span></span><span style="display:flex;"><span>cm <span style="color:#f92672">=</span> ChoiceMap<span style="color:#f92672">.</span>empty()
</span></span><span style="display:flex;"><span>cm <span style="color:#f92672">=</span> cm<span style="color:#f92672">.</span>set(<span style="color:#e6db74">&#34;x&#34;</span>, <span style="color:#ae81ff">2.5</span>)
</span></span><span style="display:flex;"><span>cm <span style="color:#f92672">=</span> cm<span style="color:#f92672">.</span>set(<span style="color:#e6db74">&#34;y&#34;</span>, <span style="color:#ae81ff">1.0</span>)</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Think of it as</strong>: A way to name and track all the random decisions</p>
<p><strong>See also</strong>: Trace, Target</p>
  </div>
</details>
<h3 id="generative-function-">Generative Function ðŸ’»</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Generative Function
  </summary>
  <div class="box-content">
<p>In GenJAX, a generative function is a Python function decorated with <code>@gen</code> that can make addressed random choices. It represents a probability distribution over its return values.</p>
<p><strong>Structure</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@gen</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">model</span>(params):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Random choices with addresses</span>
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> distribution(params) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;address&#34;</span>
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> another_distribution(x) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;another_address&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> result</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Key features</strong>:</p>
<ul>
<li>Makes random choices at named addresses</li>
<li>Can condition on observations</li>
<li>Supports inference operations</li>
</ul>
<p><strong>See also</strong>: @gen decorator, Trace, ChoiceMap</p>
  </div>
</details>
<h3 id="importance-sampling-">Importance Sampling ðŸ’»ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Importance Sampling
  </summary>
  <div class="box-content">
<p>An inference method that approximates the posterior distribution by:</p>
<ol>
<li>Generating samples from a proposal distribution</li>
<li>Weighting each sample by how well it matches observations</li>
<li>Using weighted samples to approximate the posterior</li>
</ol>
<p><strong>In GenJAX</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>trace, log_weight <span style="color:#f92672">=</span> target<span style="color:#f92672">.</span>importance(key, choicemap)</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Key concept</strong>: Effective sample size (ESS) measures how well the weights are distributed. ESS close to the number of samples is good; ESS of 1 means only one sample has meaningful weight (bad).</p>
<p><strong>Formula</strong>: $\text{ESS} = \frac{(\sum w_i)^2}{\sum w_i^2}$</p>
<p><strong>Used in ðŸ“Š</strong>: Posterior inference for Bayesian models, DPMM</p>
<p><strong>See also</strong>: Target, Weight degeneracy</p>
  </div>
</details>
<h3 id="jax-key-">JAX Key ðŸ’»</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    JAX Key
  </summary>
  <div class="box-content">
<p>JAX uses explicit random keys to control randomness (unlike NumPy&rsquo;s global random state). Think of it like a seed that you explicitly pass around.</p>
<p><strong>Why</strong>: Enables reproducibility and functional programming patterns</p>
<p><strong>Usage</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> jax
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create a key</span>
</span></span><span style="display:flex;"><span>key <span style="color:#f92672">=</span> jax<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>key(<span style="color:#ae81ff">42</span>)  <span style="color:#75715e"># 42 is the seed</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Split into multiple keys</span>
</span></span><span style="display:flex;"><span>keys <span style="color:#f92672">=</span> jax<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>split(key, num<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)  <span style="color:#75715e"># Get 100 independent keys</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Use a key</span>
</span></span><span style="display:flex;"><span>trace <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>simulate(keys[<span style="color:#ae81ff">0</span>], ())</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Best practice</strong>: Always split keys, never reuse the same key twice</p>
<p><strong>See also</strong>: vmap (often used together)</p>
  </div>
</details>
<h3 id="monte-carlo-simulation-">Monte Carlo Simulation ðŸ“˜ðŸ’»</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Monte Carlo Simulation
  </summary>
  <div class="box-content">
<p>A computational method for approximating probabilities by generating many random samples and counting outcomes. Named after the Monte Carlo casino.</p>
<p><strong>Process:</strong></p>
<ol>
<li>Generate many random outcomes (e.g., 10,000 simulated days)</li>
<li>Count how many satisfy your event</li>
<li>Calculate the ratio</li>
</ol>
<p><strong>In GenJAX</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Generate 10,000 samples</span>
</span></span><span style="display:flex;"><span>keys <span style="color:#f92672">=</span> jax<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>split(key, <span style="color:#ae81ff">10000</span>)
</span></span><span style="display:flex;"><span>samples <span style="color:#f92672">=</span> jax<span style="color:#f92672">.</span>vmap(<span style="color:#66d9ef">lambda</span> k: model<span style="color:#f92672">.</span>simulate(k, ())<span style="color:#f92672">.</span>get_retval())(keys)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Count event occurrences</span>
</span></span><span style="display:flex;"><span>event_count <span style="color:#f92672">=</span> jnp<span style="color:#f92672">.</span>sum(samples <span style="color:#f92672">&gt;=</span> threshold)
</span></span><span style="display:flex;"><span>probability <span style="color:#f92672">=</span> event_count <span style="color:#f92672">/</span> <span style="color:#ae81ff">10000</span></span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>When useful:</strong> When outcome spaces are too large to enumerate by hand</p>
<p><strong>See also</strong>: vmap, Trace</p>
  </div>
</details>
<h3 id="normal-distribution-">Normal Distribution ðŸ’»ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Normal Distribution
  </summary>
  <div class="box-content">
<p>See <strong>Gaussian Distribution</strong> (same thing)</p>
  </div>
</details>
<h3 id="simulate-">simulate() ðŸ’»</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    simulate()
  </summary>
  <div class="box-content">
<p>The <code>simulate()</code> method generates one random execution of a generative function.</p>
<p><strong>Signature</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>trace <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>simulate(key, args)</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Parameters</strong>:</p>
<ul>
<li><code>key</code>: JAX random key</li>
<li><code>args</code>: Tuple of arguments to the generative function</li>
<li>Optional: <code>observations</code> (ChoiceMap) to condition on</li>
</ul>
<p><strong>Returns</strong>: A trace containing all random choices and the return value</p>
<p><strong>Example</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@gen</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">coin_flip</span>():
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> bernoulli(<span style="color:#ae81ff">0.5</span>) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;flip&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>trace <span style="color:#f92672">=</span> coin_flip<span style="color:#f92672">.</span>simulate(key, ())
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> trace<span style="color:#f92672">.</span>get_retval()  <span style="color:#75715e"># 0 or 1</span></span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>See also</strong>: Trace, importance(), JAX Key</p>
  </div>
</details>
<h3 id="target-">Target ðŸ’»</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Target
  </summary>
  <div class="box-content">
<p>In GenJAX, a <code>Target</code> is created by conditioning a generative function on observations. It represents the posterior distribution.</p>
<p><strong>Creating a target</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> genjax <span style="color:#f92672">import</span> Target
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Observe some data</span>
</span></span><span style="display:flex;"><span>observations <span style="color:#f92672">=</span> ChoiceMap<span style="color:#f92672">.</span>d({<span style="color:#e6db74">&#34;x_0&#34;</span>: <span style="color:#ae81ff">2.5</span>, <span style="color:#e6db74">&#34;x_1&#34;</span>: <span style="color:#ae81ff">3.0</span>})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create target (posterior)</span>
</span></span><span style="display:flex;"><span>target <span style="color:#f92672">=</span> Target(model, (params,), observations)</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Using for inference</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Importance sampling</span>
</span></span><span style="display:flex;"><span>trace, log_weight <span style="color:#f92672">=</span> target<span style="color:#f92672">.</span>importance(key, ChoiceMap<span style="color:#f92672">.</span>empty())</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Key concept</strong>: The target represents $P(\text{latent variables} \mid \text{observations})$</p>
<p><strong>See also</strong>: ChoiceMap, Importance Sampling, Posterior</p>
  </div>
</details>
<h3 id="trace-">Trace ðŸ’»</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Trace
  </summary>
  <div class="box-content">
<p>In probabilistic programming, a <em>trace</em> records all random choices made during one execution of a generative function, along with their addresses (names) and the return value.</p>
<p><strong>Think of it as:</strong> A complete record of &ldquo;what happened&rdquo; during one run of a probabilistic program</p>
<p><strong>Structure</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>trace <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>simulate(key, args)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Access components</span>
</span></span><span style="display:flex;"><span>retval <span style="color:#f92672">=</span> trace<span style="color:#f92672">.</span>get_retval()         <span style="color:#75715e"># Return value</span>
</span></span><span style="display:flex;"><span>choices <span style="color:#f92672">=</span> trace<span style="color:#f92672">.</span>get_choices()        <span style="color:#75715e"># ChoiceMap with all random choices</span>
</span></span><span style="display:flex;"><span>log_prob <span style="color:#f92672">=</span> trace<span style="color:#f92672">.</span>get_score()         <span style="color:#75715e"># Log probability of this trace</span></span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Example</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@gen</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">example</span>():
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> flip(<span style="color:#ae81ff">0.5</span>) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;x&#34;</span>
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> normal(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;y&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> x <span style="color:#f92672">+</span> y
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>trace <span style="color:#f92672">=</span> example<span style="color:#f92672">.</span>simulate(key, ())
</span></span><span style="display:flex;"><span>print(trace<span style="color:#f92672">.</span>get_choices()[<span style="color:#e6db74">&#34;x&#34;</span>])  <span style="color:#75715e"># e.g., True or False</span>
</span></span><span style="display:flex;"><span>print(trace<span style="color:#f92672">.</span>get_choices()[<span style="color:#e6db74">&#34;y&#34;</span>])  <span style="color:#75715e"># e.g., 0.234</span>
</span></span><span style="display:flex;"><span>print(trace<span style="color:#f92672">.</span>get_retval())        <span style="color:#75715e"># e.g., 1.234</span></span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Used in:</strong> GenJAX and other probabilistic programming systems</p>
<p><strong>See also</strong>: ChoiceMap, Generative Function</p>
  </div>
</details>
<h3 id="vmap-">vmap ðŸ’»</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    vmap
  </summary>
  <div class="box-content">
<p>JAX&rsquo;s &ldquo;vectorized map&rdquo; - applies a function to many inputs in parallel (very fast!).</p>
<p><strong>Concept</strong>: Instead of a for-loop running sequentially, vmap runs operations in parallel on the GPU/CPU.</p>
<p><strong>Usage</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> jax
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Regular loop (slow)</span>
</span></span><span style="display:flex;"><span>results <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> key <span style="color:#f92672">in</span> keys:
</span></span><span style="display:flex;"><span>    results<span style="color:#f92672">.</span>append(model<span style="color:#f92672">.</span>simulate(key, ())<span style="color:#f92672">.</span>get_retval())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># vmap (fast!)</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run_once</span>(key):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> model<span style="color:#f92672">.</span>simulate(key, ())<span style="color:#f92672">.</span>get_retval()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>results <span style="color:#f92672">=</span> jax<span style="color:#f92672">.</span>vmap(run_once)(keys)</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Think of it as</strong>: &ldquo;Do this function 10,000 times, but do them all at once&rdquo;</p>
<p><strong>Why it&rsquo;s fast</strong>: Leverages parallel hardware (GPU, vectorized CPU operations)</p>
<p><strong>See also</strong>: JAX Key, Monte Carlo Simulation</p>
  </div>
</details>
<hr>
<h2 id="continuous-probability-tutorial-3">Continuous Probability (Tutorial 3)</h2>
<h3 id="beta-distribution-">Beta Distribution ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Beta Distribution
  </summary>
  <div class="box-content">
<p>A continuous probability distribution on the interval [0,1], parameterized by two shape parameters $\alpha$ and $\beta$.</p>
<p><strong>Parameters</strong>:</p>
<ul>
<li>$\alpha$ (alpha) - shape parameter</li>
<li>$\beta$ (beta) - shape parameter</li>
</ul>
<p><strong>PDF</strong>: $p(x \mid \alpha, \beta) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha, \beta)}$</p>
<p><strong>In GenJAX</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@gen</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">stick_breaking</span>(alpha):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Beta(1, alpha) for stick-breaking</span>
</span></span><span style="display:flex;"><span>    beta_k <span style="color:#f92672">=</span> beta(<span style="color:#ae81ff">1.0</span>, alpha) <span style="color:#f92672">@</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;beta_</span><span style="color:#e6db74">{</span>k<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> beta_k</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Special cases</strong>:</p>
<ul>
<li>Beta(1,1) = Uniform(0,1)</li>
<li>Beta(Î±,Î±) is symmetric around 0.5</li>
</ul>
<p><strong>Used in ðŸ“Š</strong>:</p>
<ul>
<li>Stick-breaking construction for Dirichlet Process</li>
<li>Modeling probabilities and proportions</li>
<li>Conjugate prior for Bernoulli/Binomial</li>
</ul>
<p><strong>See also</strong>: Dirichlet distribution, Stick-breaking</p>
  </div>
</details>
<h3 id="chinese-restaurant-process-crp-">Chinese Restaurant Process (CRP) ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Chinese Restaurant Process
  </summary>
  <div class="box-content">
<p>A metaphor and algorithm for understanding the Dirichlet Process. Imagine customers entering a restaurant with infinite tables:</p>
<ul>
<li>First customer sits at table 1</li>
<li>Next customer: sit at an occupied table with probability proportional to its occupancy, OR sit at a new table with probability proportional to Î±</li>
</ul>
<p><strong>Parameters</strong>: Î± (concentration parameter)</p>
<p><strong>Properties</strong>:</p>
<ul>
<li>&ldquo;Rich get richer&rdquo; - popular tables attract more customers</li>
<li>But always a chance to start new tables</li>
<li>Î± controls tendency to create new clusters</li>
</ul>
<p><strong>Connection to DPMM</strong>: Each table = a cluster. CRP determines cluster assignments, then each cluster has its own Gaussian distribution.</p>
<p><strong>Not used directly in code</strong>: Stick-breaking construction is mathematically equivalent but more practical for implementation</p>
<p><strong>See also</strong>: Dirichlet Process, DPMM, Stick-breaking</p>
  </div>
</details>
<h3 id="concentration-parameter-Î±-">Concentration Parameter (Î±) ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Concentration Parameter (Î±)
  </summary>
  <div class="box-content">
<p>The parameter Î± in the Dirichlet Process and related models controls the tendency to create new clusters vs. reusing existing ones.</p>
<p><strong>Effect</strong>:</p>
<ul>
<li><strong>Small Î±</strong> (e.g., 0.1): Few clusters, strong preference for existing clusters</li>
<li><strong>Medium Î±</strong> (e.g., 1-5): Balanced exploration/exploitation</li>
<li><strong>Large Î±</strong> (e.g., 10+): Many clusters, high probability of creating new ones</li>
</ul>
<p><strong>In stick-breaking</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>beta_k <span style="color:#f92672">=</span> beta(<span style="color:#ae81ff">1.0</span>, alpha) <span style="color:#f92672">@</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;beta_</span><span style="color:#e6db74">{</span>k<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span></span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Intuition</strong>: Î± is like a &ldquo;prior strength&rdquo; for new clusters. Higher Î± = more willing to explain data with new clusters rather than fitting to existing ones.</p>
<p><strong>Typical range</strong>: 0.1 to 10 for most applications</p>
<p><strong>See also</strong>: Dirichlet Process, DPMM, Stick-breaking</p>
  </div>
</details>
<h3 id="conjugate-prior-">Conjugate Prior ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Conjugate Prior
  </summary>
  <div class="box-content">
<p>A prior distribution is <em>conjugate</em> to a likelihood when the posterior distribution is in the same family as the prior.</p>
<p><strong>Why useful</strong>: Enables closed-form posterior calculation (no need for sampling)</p>
<p><strong>Classic examples</strong>:</p>
<ul>
<li><strong>Beta-Binomial</strong>: Beta prior Ã— Binomial likelihood = Beta posterior</li>
<li><strong>Gamma-Poisson</strong>: Gamma prior Ã— Poisson likelihood = Gamma posterior</li>
<li><strong>Gaussian-Gaussian</strong>: Normal prior Ã— Normal likelihood = Normal posterior</li>
</ul>
<p><strong>Example (Gaussian-Gaussian)</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Prior: Î¼ ~ Normal(Î¼â‚€, Ïƒâ‚€Â²)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Likelihood: x | Î¼ ~ Normal(Î¼, ÏƒÂ²)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Posterior: Î¼ | x ~ Normal(Î¼_post, Ïƒ_postÂ²)  # Still Gaussian!</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Posterior parameters:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Î¼_post = (ÏƒÂ²Â·Î¼â‚€ + Ïƒâ‚€Â²Â·x) / (ÏƒÂ² + Ïƒâ‚€Â²)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Ïƒ_postÂ² = (ÏƒÂ²Â·Ïƒâ‚€Â²) / (ÏƒÂ² + Ïƒâ‚€Â²)</span></span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Trade-off</strong>: Mathematical convenience vs. modeling flexibility</p>
<p><strong>Tutorial 3, Chapter 4</strong> covers Gaussian-Gaussian conjugacy in detail</p>
<p><strong>See also</strong>: Prior, Posterior, Bayesian Learning</p>
  </div>
</details>
<h3 id="cumulative-distribution-function-cdf-">Cumulative Distribution Function (CDF) ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Cumulative Distribution Function (CDF)
  </summary>
  <div class="box-content">
<p>For a continuous random variable, the CDF gives the probability that the variable is less than or equal to a value:</p>
<p>$$F(x) = P(X \leq x) = \int_{-\infty}^x p(t) , dt$$</p>
<p><strong>Key properties</strong>:</p>
<ul>
<li>Always increasing (or flat)</li>
<li>Ranges from 0 to 1</li>
<li>$F(-\infty) = 0$ and $F(\infty) = 1$</li>
<li>Derivative of CDF = PDF: $\frac{dF}{dx} = p(x)$</li>
</ul>
<p><strong>Interpretation</strong>: &ldquo;What&rsquo;s the probability of getting a value this small or smaller?&rdquo;</p>
<p><strong>Example (Standard Normal)</strong>:</p>
<ul>
<li>CDF(0) â‰ˆ 0.5 (50% chance of being â‰¤ 0)</li>
<li>CDF(1.96) â‰ˆ 0.975 (97.5% chance of being â‰¤ 1.96)</li>
</ul>
<p><strong>In code</strong>: Usually not needed directly in GenJAX (we sample instead), but useful for understanding quantiles and probabilities</p>
<p><strong>See also</strong>: PDF, Quantile</p>
  </div>
</details>
<h3 id="dirichlet-distribution-">Dirichlet Distribution ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Dirichlet Distribution
  </summary>
  <div class="box-content">
<p>The multivariate generalization of the Beta distribution. Produces probability vectors that sum to 1.</p>
<p><strong>Parameters</strong>: Î± = (Î±â‚, Î±â‚‚, &hellip;, Î±â‚–) - concentration parameters</p>
<p><strong>Output</strong>: Vector (pâ‚, pâ‚‚, &hellip;, pâ‚–) where all páµ¢ &gt; 0 and Î£páµ¢ = 1</p>
<p><strong>In GenJAX</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@gen</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">mixture_weights</span>(alpha_vector):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Returns a probability distribution over K categories</span>
</span></span><span style="display:flex;"><span>    probs <span style="color:#f92672">=</span> dirichlet(alpha_vector) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;probs&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> probs</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Special case</strong>: Dirichlet(1,1,1,&hellip;,1) = Uniform over probability simplex</p>
<p><strong>Intuition</strong>: Like rolling a weighted die where the weights themselves are random</p>
<p><strong>Used in</strong>:</p>
<ul>
<li>Prior for mixture weights in GMM</li>
<li><del>DPMM</del> (not directly - stick-breaking is used instead)</li>
<li>Topic modeling (LDA)</li>
</ul>
<p><strong>See also</strong>: Beta distribution, Categorical distribution</p>
  </div>
</details>
<h3 id="dirichlet-process-dp-">Dirichlet Process (DP) ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Dirichlet Process
  </summary>
  <div class="box-content">
<p>A distribution over distributions. It&rsquo;s a <em>prior</em> for mixture models when you don&rsquo;t know how many clusters/components you need.</p>
<p><strong>Parameters</strong>:</p>
<ul>
<li>Î± (concentration parameter) - controls cluster formation</li>
<li>Gâ‚€ (base distribution) - the &ldquo;prototype&rdquo; distribution for clusters</li>
</ul>
<p><strong>Key properties</strong>:</p>
<ul>
<li><strong>Infinite mixture</strong>: Can have arbitrarily many clusters</li>
<li><strong>Automatic model selection</strong>: Data determines effective number of clusters</li>
<li><strong>Clustering property</strong>: Enforces that some samples share the same parameter values (clusters)</li>
</ul>
<p><strong>Two representations</strong>:</p>
<ol>
<li><strong>Chinese Restaurant Process</strong> (CRP) - metaphorical, sequential</li>
<li><strong>Stick-breaking</strong> - constructive, practical for implementation</li>
</ol>
<p><strong>Why &ldquo;Dirichlet Process&rdquo;</strong>: It&rsquo;s a generalization of the Dirichlet distribution to infinite dimensions</p>
<p><strong>In practice</strong>: Used via DPMM for clustering without specifying K</p>
<p><strong>Tutorial 3, Chapter 6</strong> covers DP in detail</p>
<p><strong>See also</strong>: DPMM, Stick-breaking, Chinese Restaurant Process</p>
  </div>
</details>
<h3 id="dirichlet-process-mixture-model-dpmm-">Dirichlet Process Mixture Model (DPMM) ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Dirichlet Process Mixture Model (DPMM)
  </summary>
  <div class="box-content">
<p>An infinite mixture model that automatically determines the number of clusters from data.</p>
<p><strong>Structure</strong>:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>1. Generate cluster parameters using stick-breaking:
   - Î²â‚, Î²â‚‚, ... ~ Beta(1, Î±)
   - Ï€â‚ = Î²â‚, Ï€â‚‚ = Î²â‚‚(1-Î²â‚), Ï€â‚ƒ = Î²â‚ƒ(1-Î²â‚)(1-Î²â‚‚), ...

2. For each data point:
   - z ~ Categorical(Ï€)  # Assign to cluster
   - x | z ~ Normal(Î¼_z, ÏƒÂ²)  # Generate from that cluster&#39;s Gaussian</code></pre></div>
<p><strong>Parameters</strong>:</p>
<ul>
<li>Î± - controls number of clusters</li>
<li>Î¼â‚€, Ïƒâ‚€ - prior for cluster means</li>
<li>Ïƒ - observation noise</li>
</ul>
<p><strong>In GenJAX</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@gen</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">dpmm</span>(alpha, mu0, sig0, sigx):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Stick-breaking for mixture weights</span>
</span></span><span style="display:flex;"><span>    pis <span style="color:#f92672">=</span> stick_breaking_construction(alpha, K)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Cluster means</span>
</span></span><span style="display:flex;"><span>    mus <span style="color:#f92672">=</span> [normal(mu0, sig0) <span style="color:#f92672">@</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;mu_</span><span style="color:#e6db74">{</span>k<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> range(K)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Assign data points and generate observations</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(N):
</span></span><span style="display:flex;"><span>        z_i <span style="color:#f92672">=</span> categorical(pis) <span style="color:#f92672">@</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;z_</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>        x_i <span style="color:#f92672">=</span> normal(mus[z_i], sigx) <span style="color:#f92672">@</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;x_</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span></span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Advantages</strong>:</p>
<ul>
<li>No need to specify K in advance</li>
<li>Principled Bayesian uncertainty</li>
<li>Automatic model complexity control</li>
</ul>
<p><strong>Challenges</strong>:</p>
<ul>
<li>Requires truncation (approximate with K clusters)</li>
<li>Inference can be slow for large datasets</li>
<li>Sensitive to Î± choice</li>
</ul>
<p><strong>Tutorial 3, Chapter 6</strong> has full implementation and interactive notebook</p>
<p><strong>See also</strong>: GMM, Dirichlet Process, Stick-breaking</p>
  </div>
</details>
<h3 id="expected-value-">Expected Value ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Expected Value
  </summary>
  <div class="box-content">
<p>The average value of a random variable, weighted by probabilities. Also called the <em>mean</em> or <em>expectation</em>.</p>
<p><strong>For discrete</strong>: $E[X] = \sum_{x} x \cdot P(X=x)$</p>
<p><strong>For continuous</strong>: $E[X] = \int_{-\infty}^{\infty} x \cdot p(x) , dx$</p>
<p><strong>In GenJAX</strong> (approximation by sampling):</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Generate many samples</span>
</span></span><span style="display:flex;"><span>samples <span style="color:#f92672">=</span> [model<span style="color:#f92672">.</span>simulate(key_i, ())<span style="color:#f92672">.</span>get_retval() <span style="color:#66d9ef">for</span> key_i <span style="color:#f92672">in</span> keys]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Expected value â‰ˆ average of samples</span>
</span></span><span style="display:flex;"><span>expected_value <span style="color:#f92672">=</span> jnp<span style="color:#f92672">.</span>mean(samples)</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Properties</strong>:</p>
<ul>
<li>Linearity: $E[aX + bY] = aE[X] + bE[Y]$</li>
<li>For independent variables: $E[XY] = E[X]E[Y]$</li>
</ul>
<p><strong>Interpretation</strong>: &ldquo;If I repeated this experiment many times, what would the average outcome be?&rdquo;</p>
<p><strong>Tutorial 3, Chapter 1</strong> covers expected value with the &ldquo;mystery bento&rdquo; paradox</p>
<p><strong>See also</strong>: Variance, Law of Iterated Expectation</p>
  </div>
</details>
<h3 id="gaussian-distribution-">Gaussian Distribution ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Gaussian Distribution
  </summary>
  <div class="box-content">
<p>Also called the <em>Normal distribution</em>. The famous bell curve, ubiquitous in statistics and machine learning.</p>
<p><strong>Parameters</strong>:</p>
<ul>
<li>Î¼ (mu) - mean (center of the bell)</li>
<li>ÏƒÂ² (sigma squared) - variance (width of the bell)</li>
</ul>
<p><strong>PDF</strong>:
$$p(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$</p>
<p><strong>In GenJAX</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@gen</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gaussian_model</span>():
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> normal(mu, sigma) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;x&#34;</span>  <span style="color:#75715e"># Note: sigma, not sigmaÂ²</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> x</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>The 68-95-99.7 Rule</strong>:</p>
<ul>
<li>68% of data within Î¼ Â± Ïƒ</li>
<li>95% of data within Î¼ Â± 2Ïƒ</li>
<li>99.7% of data within Î¼ Â± 3Ïƒ</li>
</ul>
<p><strong>Why so common</strong>:</p>
<ul>
<li>Central Limit Theorem (sums converge to Gaussian)</li>
<li>Maximum entropy distribution for given mean and variance</li>
<li>Mathematically tractable (conjugate priors!)</li>
</ul>
<p><strong>Tutorial 3, Chapter 3</strong> covers Gaussians in detail</p>
<p><strong>See also</strong>: Normal distribution (same thing), Standard Normal</p>
  </div>
</details>
<h3 id="gaussian-mixture-model-gmm-">Gaussian Mixture Model (GMM) ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Gaussian Mixture Model (GMM)
  </summary>
  <div class="box-content">
<p>A mixture of multiple Gaussian distributions, each with its own mean, variance, and mixing weight.</p>
<p><strong>Structure</strong>:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>1. Choose cluster k with probability Ï€â‚–
2. Sample from Normal(Î¼â‚–, Ïƒâ‚–Â²)</code></pre></div>
<p><strong>Parameters</strong>:</p>
<ul>
<li>K - number of components (must be specified)</li>
<li>Ï€â‚, &hellip;, Ï€â‚– - mixing weights (sum to 1)</li>
<li>Î¼â‚, &hellip;, Î¼â‚– - component means</li>
<li>Ïƒâ‚Â², &hellip;, Ïƒâ‚–Â² - component variances</li>
</ul>
<p><strong>In GenJAX</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@gen</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gmm</span>(pis, mus, sigmas):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Choose component</span>
</span></span><span style="display:flex;"><span>    z <span style="color:#f92672">=</span> categorical(pis) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;z&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Sample from chosen component</span>
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> normal(mus[z], sigmas[z]) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;x&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> x</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Use cases</strong>:</p>
<ul>
<li>Clustering data with multiple groups</li>
<li>Modeling multimodal distributions</li>
<li>Density estimation</li>
</ul>
<p><strong>Limitation</strong>: Must specify K in advance (DPMM fixes this!)</p>
<p><strong>Tutorial 3, Chapter 5</strong> covers GMM</p>
<p><strong>See also</strong>: DPMM, Mixture Model</p>
  </div>
</details>
<h3 id="likelihood-">Likelihood ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Likelihood
  </summary>
  <div class="box-content">
<p>The probability of observing the data given specific parameter values: $P(D \mid \theta)$</p>
<p><strong>Key distinction</strong>:</p>
<ul>
<li>As a function of data (Î¸ fixed): <strong>Probability</strong></li>
<li>As a function of parameters (data fixed): <strong>Likelihood</strong></li>
</ul>
<p><strong>In Bayes&rsquo; Theorem</strong>:
$$P(\theta \mid D) = \frac{P(D \mid \theta) \cdot P(\theta)}{P(D)}$$</p>
<ul>
<li>$P(D \mid \theta)$ is the <strong>likelihood</strong></li>
<li>$P(\theta)$ is the <strong>prior</strong></li>
<li>$P(\theta \mid D)$ is the <strong>posterior</strong></li>
</ul>
<p><strong>Example</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">9
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Observed data: x = [2.5, 3.0, 2.8]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Model: x[i] ~ Normal(Î¼, 1.0)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Likelihood of Î¼ = 3.0:</span>
</span></span><span style="display:flex;"><span>likelihood <span style="color:#f92672">=</span> product([
</span></span><span style="display:flex;"><span>    normal_pdf(<span style="color:#ae81ff">2.5</span>, mu<span style="color:#f92672">=</span><span style="color:#ae81ff">3.0</span>, sigma<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>),
</span></span><span style="display:flex;"><span>    normal_pdf(<span style="color:#ae81ff">3.0</span>, mu<span style="color:#f92672">=</span><span style="color:#ae81ff">3.0</span>, sigma<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>),
</span></span><span style="display:flex;"><span>    normal_pdf(<span style="color:#ae81ff">2.8</span>, mu<span style="color:#f92672">=</span><span style="color:#ae81ff">3.0</span>, sigma<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>)
</span></span><span style="display:flex;"><span>])</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>In GenJAX</strong>: The trace log probability includes the likelihood</p>
<p><strong>See also</strong>: Posterior, Prior, Bayes&rsquo; Theorem</p>
  </div>
</details>
<h3 id="mixture-model-">Mixture Model ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Mixture Model
  </summary>
  <div class="box-content">
<p>A probability model that combines multiple component distributions, each active with some probability.</p>
<p><strong>General form</strong>:
$$p(x) = \sum_{k=1}^K \pi_k \cdot p_k(x)$$</p>
<p>where:</p>
<ul>
<li>Ï€â‚– = mixing weights (probabilities, sum to 1)</li>
<li>pâ‚–(x) = component distributions</li>
</ul>
<p><strong>Generative process</strong>:</p>
<ol>
<li>Choose component k with probability Ï€â‚–</li>
<li>Sample from component pâ‚–</li>
</ol>
<p><strong>Common types</strong>:</p>
<ul>
<li><strong>Gaussian Mixture Model (GMM)</strong>: Components are Gaussians</li>
<li><strong>DPMM</strong>: Infinite mixture (K â†’ âˆž)</li>
</ul>
<p><strong>Why useful</strong>:</p>
<ul>
<li>Model complex, multimodal distributions</li>
<li>Perform soft clustering</li>
<li>Represent heterogeneous populations</li>
</ul>
<p><strong>Tutorial 3, Chapter 5</strong> covers finite mixtures (GMM)
<strong>Tutorial 3, Chapter 6</strong> covers infinite mixtures (DPMM)</p>
<p><strong>See also</strong>: GMM, DPMM, Categorical distribution</p>
  </div>
</details>
<h3 id="posterior-distribution-">Posterior Distribution ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Posterior Distribution
  </summary>
  <div class="box-content">
<p>The updated probability distribution over parameters after observing data: $P(\theta \mid D)$</p>
<p><strong>Via Bayes&rsquo; Theorem</strong>:
$$P(\theta \mid D) = \frac{P(D \mid \theta) \cdot P(\theta)}{P(D)}$$</p>
<ul>
<li>$P(\theta)$ = <strong>prior</strong> (before seeing data)</li>
<li>$P(D \mid \theta)$ = <strong>likelihood</strong> (how well Î¸ explains data)</li>
<li>$P(D)$ = <strong>evidence</strong> (normalizing constant)</li>
<li>$P(\theta \mid D)$ = <strong>posterior</strong> (after seeing data)</li>
</ul>
<p><strong>In GenJAX</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Specify observations</span>
</span></span><span style="display:flex;"><span>observations <span style="color:#f92672">=</span> ChoiceMap<span style="color:#f92672">.</span>d({<span style="color:#e6db74">&#34;x_0&#34;</span>: <span style="color:#ae81ff">2.5</span>, <span style="color:#e6db74">&#34;x_1&#34;</span>: <span style="color:#ae81ff">3.0</span>})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create posterior target</span>
</span></span><span style="display:flex;"><span>target <span style="color:#f92672">=</span> Target(model, (params,), observations)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Sample from posterior</span>
</span></span><span style="display:flex;"><span>trace, log_weight <span style="color:#f92672">=</span> target<span style="color:#f92672">.</span>importance(key, ChoiceMap<span style="color:#f92672">.</span>empty())</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Interpretation</strong>: &ldquo;Given what I observed, what parameter values are most plausible?&rdquo;</p>
<p><strong>Tutorial 3, Chapter 4</strong> covers Bayesian learning and posteriors</p>
<p><strong>See also</strong>: Prior, Likelihood, Bayes&rsquo; Theorem</p>
  </div>
</details>
<h3 id="predictive-distribution-">Predictive Distribution ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Predictive Distribution
  </summary>
  <div class="box-content">
<p>The distribution over new, unobserved data given the data we&rsquo;ve already seen.</p>
<p><strong>Posterior Predictive</strong>: $P(x_{\text{new}} \mid D) = \int P(x_{\text{new}} \mid \theta) \cdot P(\theta \mid D) , d\theta$</p>
<p><strong>In words</strong>:</p>
<ol>
<li>Consider all possible parameter values Î¸</li>
<li>Weight each by posterior probability P(Î¸ | D)</li>
<li>Average their predictions for new data</li>
</ol>
<p><strong>In GenJAX</strong> (via sampling):</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 1. Get posterior samples for Î¸</span>
</span></span><span style="display:flex;"><span>posterior_samples <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> key <span style="color:#f92672">in</span> keys:
</span></span><span style="display:flex;"><span>    trace, _ <span style="color:#f92672">=</span> target<span style="color:#f92672">.</span>importance(key, ChoiceMap<span style="color:#f92672">.</span>empty())
</span></span><span style="display:flex;"><span>    theta <span style="color:#f92672">=</span> trace<span style="color:#f92672">.</span>get_choices()[<span style="color:#e6db74">&#34;theta&#34;</span>]
</span></span><span style="display:flex;"><span>    posterior_samples<span style="color:#f92672">.</span>append(theta)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. For each Î¸, generate predictions</span>
</span></span><span style="display:flex;"><span>predictions <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> theta <span style="color:#f92672">in</span> posterior_samples:
</span></span><span style="display:flex;"><span>    x_new <span style="color:#f92672">=</span> generate_new_data(theta)
</span></span><span style="display:flex;"><span>    predictions<span style="color:#f92672">.</span>append(x_new)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># predictions is now a sample from the predictive distribution!</span></span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Why important</strong>: Captures uncertainty in both parameters AND new data</p>
<p><strong>Tutorial 3, Chapter 4</strong> shows predictive distributions for Bayesian learning</p>
<p><strong>See also</strong>: Posterior, Prior</p>
  </div>
</details>
<h3 id="prior-distribution-">Prior Distribution ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Prior Distribution
  </summary>
  <div class="box-content">
<p>The probability distribution over parameters <em>before</em> seeing any data: $P(\theta)$</p>
<p><strong>In Bayes&rsquo; Theorem</strong>:
$$P(\theta \mid D) = \frac{P(D \mid \theta) \cdot P(\theta)}{P(D)}$$</p>
<ul>
<li>$P(\theta)$ = <strong>prior</strong> (our initial belief)</li>
<li>$P(\theta \mid D)$ = <strong>posterior</strong> (updated belief after seeing data D)</li>
</ul>
<p><strong>Types of priors</strong>:</p>
<ul>
<li><strong>Informative</strong>: Strong beliefs (e.g., Normal(0, 0.1Â²) says Î¼ is near 0)</li>
<li><strong>Weakly informative</strong>: Gentle guidance (e.g., Normal(0, 10Â²))</li>
<li><strong>Uninformative/Flat</strong>: No preference (e.g., Uniform(-âˆž, âˆž))</li>
</ul>
<p><strong>In GenJAX</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@gen</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">bayesian_model</span>(mu0, sigma0):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Prior: Î¼ ~ Normal(mu0, sigma0)</span>
</span></span><span style="display:flex;"><span>    mu <span style="color:#f92672">=</span> normal(mu0, sigma0) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;mu&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Likelihood: x | Î¼ ~ Normal(Î¼, 1.0)</span>
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> normal(mu, <span style="color:#ae81ff">1.0</span>) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;x&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> x</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Controversy</strong>: Subjectivity of priors is both a feature (encode knowledge) and criticism (bias results) of Bayesian methods</p>
<p><strong>Tutorial 3, Chapter 4</strong> discusses priors in Bayesian learning</p>
<p><strong>See also</strong>: Posterior, Likelihood, Conjugate Prior</p>
  </div>
</details>
<h3 id="probability-density-function-pdf-">Probability Density Function (PDF) ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Probability Density Function (PDF)
  </summary>
  <div class="box-content">
<p>For continuous random variables, the PDF describes the <em>density</em> of probability at each value.</p>
<p><strong>Key insight</strong>: $p(x)$ is NOT a probability! It&rsquo;s a <strong>density</strong>.</p>
<p><strong>Why</strong>:</p>
<ul>
<li>Probability of any exact value is 0 (infinitely many possible values)</li>
<li>Probability is the <strong>area under the PDF curve</strong> over an interval:
$$P(a \leq X \leq b) = \int_a^b p(x) , dx$$</li>
</ul>
<p><strong>Properties</strong>:</p>
<ul>
<li>$p(x) \geq 0$ (non-negative)</li>
<li>$\int_{-\infty}^{\infty} p(x) , dx = 1$ (total area = 1)</li>
<li>$p(x)$ can be &gt; 1! (it&rsquo;s density, not probability)</li>
</ul>
<p><strong>Example (Gaussian)</strong>:
$$p(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$</p>
<p><strong>In GenJAX</strong>: We usually sample from PDFs rather than compute them directly</p>
<p><strong>Connection to discrete ðŸ“˜</strong>: PDF is the continuous analog of probability mass function (PMF)</p>
<p><strong>Tutorial 3, Chapter 2</strong> introduces PDFs</p>
<p><strong>See also</strong>: CDF, Continuous Random Variable</p>
  </div>
</details>
<h3 id="standard-normal-">Standard Normal ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Standard Normal
  </summary>
  <div class="box-content">
<p>The Gaussian distribution with Î¼=0 and ÏƒÂ²=1.</p>
<p><strong>PDF</strong>:
$$p(x) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x^2}{2}\right)$$</p>
<p><strong>Notation</strong>: $X \sim \mathcal{N}(0,1)$</p>
<p><strong>Why special</strong>:</p>
<ul>
<li>Reference distribution (z-scores)</li>
<li>Any Normal(Î¼, ÏƒÂ²) can be standardized: $Z = \frac{X - \mu}{\sigma} \sim \mathcal{N}(0,1)$</li>
<li>Tables and functions often use standard normal</li>
</ul>
<p><strong>In GenJAX</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>z <span style="color:#f92672">=</span> normal(<span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">1.0</span>) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;z&#34;</span>  <span style="color:#75715e"># Standard normal</span></span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>See also</strong>: Gaussian Distribution, Z-score</p>
  </div>
</details>
<h3 id="stick-breaking-construction-">Stick-Breaking Construction ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Stick-Breaking Construction
  </summary>
  <div class="box-content">
<p>A way to construct the infinite mixture weights in a Dirichlet Process by &ldquo;breaking sticks.&rdquo;</p>
<p><strong>Metaphor</strong>: Start with a stick of length 1. Repeatedly:</p>
<ol>
<li>Break off a fraction (Î²) of the remaining stick</li>
<li>That piece becomes the weight for the next cluster</li>
<li>Continue with the remaining stick</li>
</ol>
<p><strong>Mathematical process</strong>:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>Î²â‚, Î²â‚‚, Î²â‚ƒ, ... ~ Beta(1, Î±)

Ï€â‚ = Î²â‚
Ï€â‚‚ = Î²â‚‚ Â· (1 - Î²â‚)
Ï€â‚ƒ = Î²â‚ƒ Â· (1 - Î²â‚) Â· (1 - Î²â‚‚)
...
Ï€â‚– = Î²â‚– Â· âˆ(1 - Î²â±¼) for j &lt; k</code></pre></div>
<p><strong>Properties</strong>:</p>
<ul>
<li>All Ï€â‚– &gt; 0</li>
<li>Î£ Ï€â‚– = 1 (sum to 1)</li>
<li>Ï€â‚– decreases (on average) as k increases</li>
</ul>
<p><strong>In GenJAX</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@gen</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">stick_breaking</span>(alpha, K):
</span></span><span style="display:flex;"><span>    betas <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    pis <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> range(K):
</span></span><span style="display:flex;"><span>        beta_k <span style="color:#f92672">=</span> beta(<span style="color:#ae81ff">1.0</span>, alpha) <span style="color:#f92672">@</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;beta_</span><span style="color:#e6db74">{</span>k<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>        betas<span style="color:#f92672">.</span>append(beta_k)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Convert betas to pis</span>
</span></span><span style="display:flex;"><span>    remaining <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> range(K):
</span></span><span style="display:flex;"><span>        pis<span style="color:#f92672">.</span>append(betas[k] <span style="color:#f92672">*</span> remaining)
</span></span><span style="display:flex;"><span>        remaining <span style="color:#f92672">*=</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> betas[k])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> jnp<span style="color:#f92672">.</span>array(pis)</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Used in</strong>: DPMM implementation</p>
<p><strong>Tutorial 3, Chapter 6</strong> explains stick-breaking in detail</p>
<p><strong>See also</strong>: Dirichlet Process, DPMM, Beta Distribution</p>
  </div>
</details>
<h3 id="truncation-in-dpmm-">Truncation (in DPMM) ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Truncation
  </summary>
  <div class="box-content">
<p>The Dirichlet Process is theoretically infinite, but in practice we approximate it by limiting to K components.</p>
<p><strong>Why necessary</strong>:</p>
<ul>
<li>Can&rsquo;t actually implement infinite dimensions in code</li>
<li>After K components, remaining weights are negligibly small</li>
</ul>
<p><strong>How it works</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Truncated stick-breaking</span>
</span></span><span style="display:flex;"><span>K_max <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>  <span style="color:#75715e"># Truncation level</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># First K-1 components use stick-breaking</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> range(K_max <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>    beta_k <span style="color:#f92672">=</span> beta(<span style="color:#ae81ff">1.0</span>, alpha) <span style="color:#f92672">@</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;beta_</span><span style="color:#e6db74">{</span>k<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>    pis[k] <span style="color:#f92672">=</span> beta_k <span style="color:#f92672">*</span> remaining
</span></span><span style="display:flex;"><span>    remaining <span style="color:#f92672">*=</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> beta_k)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Last component gets all remaining weight</span>
</span></span><span style="display:flex;"><span>pis[K_max <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> remaining</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Choosing K</strong>:</p>
<ul>
<li>Too small: Can&rsquo;t capture true number of clusters</li>
<li>Too large: Slower inference, but mathematically fine</li>
<li>Rule of thumb: K = 2-3Ã— expected clusters</li>
</ul>
<p><strong>Quality check</strong>: If highest cluster indices have significant weight, increase K</p>
<p><strong>Tutorial 3, Chapter 6</strong> discusses truncation in DPMM</p>
<p><strong>See also</strong>: DPMM, Stick-breaking</p>
  </div>
</details>
<h3 id="uniform-distribution-">Uniform Distribution ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Uniform Distribution
  </summary>
  <div class="box-content">
<p>A continuous distribution where all values in a range [a, b] are equally likely.</p>
<p><strong>Parameters</strong>:</p>
<ul>
<li>a - minimum value</li>
<li>b - maximum value</li>
</ul>
<p><strong>PDF</strong>:
$$p(x) = \begin{cases}
\frac{1}{b-a} &amp; \text{if } a \leq x \leq b \\
0 &amp; \text{otherwise}
\end{cases}$$</p>
<p><strong>In GenJAX</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@gen</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">uniform_example</span>():
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> uniform(a, b) <span style="color:#f92672">@</span> <span style="color:#e6db74">&#34;x&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> x</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Properties</strong>:</p>
<ul>
<li>Mean: (a + b) / 2</li>
<li>Variance: (b - a)Â² / 12</li>
</ul>
<p><strong>Example uses</strong>:</p>
<ul>
<li>Random initialization</li>
<li>Uninformative prior on bounded parameters</li>
<li>Modeling &ldquo;complete ignorance&rdquo; in a range</li>
</ul>
<p><strong>Connection to discrete ðŸ“˜</strong>: Continuous analog of &ldquo;all outcomes equally likely&rdquo;</p>
<p><strong>Tutorial 3, Chapter 2</strong> introduces uniform distribution</p>
<p><strong>See also</strong>: PDF, Continuous Random Variable</p>
  </div>
</details>
<h3 id="variance-">Variance ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Variance
  </summary>
  <div class="box-content">
<p>A measure of spread/variability in a distribution. The expected squared deviation from the mean.</p>
<p><strong>Formula</strong>: $\text{Var}(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2$</p>
<p><strong>Notation</strong>:</p>
<ul>
<li>Var(X) or ÏƒÂ²</li>
<li>Standard deviation: Ïƒ = âˆš(Var(X))</li>
</ul>
<p><strong>In GenJAX</strong> (approximation by sampling):</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Generate samples</span>
</span></span><span style="display:flex;"><span>samples <span style="color:#f92672">=</span> jnp<span style="color:#f92672">.</span>array([model<span style="color:#f92672">.</span>simulate(key_i, ())<span style="color:#f92672">.</span>get_retval() <span style="color:#66d9ef">for</span> key_i <span style="color:#f92672">in</span> keys])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Variance â‰ˆ sample variance</span>
</span></span><span style="display:flex;"><span>variance <span style="color:#f92672">=</span> jnp<span style="color:#f92672">.</span>var(samples)
</span></span><span style="display:flex;"><span>std_dev <span style="color:#f92672">=</span> jnp<span style="color:#f92672">.</span>sqrt(variance)</span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Properties</strong>:</p>
<ul>
<li>Always non-negative</li>
<li>Var(aX + b) = aÂ² Â· Var(X)</li>
<li>For independent X, Y: Var(X + Y) = Var(X) + Var(Y)</li>
</ul>
<p><strong>Interpretation</strong>: &ldquo;How spread out is the data?&rdquo;</p>
<p><strong>See also</strong>: Expected Value, Standard Deviation, Gaussian</p>
  </div>
</details>
<h3 id="weight-degeneracy-">Weight Degeneracy ðŸ“Š</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Weight Degeneracy
  </summary>
  <div class="box-content">
<p>A problem in importance sampling where most samples have negligible weight, so only one or a few samples contribute meaningfully.</p>
<p><strong>Symptom</strong>: Effective sample size (ESS) &laquo; number of samples</p>
<p><strong>Example</strong>:</p>
<div class="highlight wrap-code" dir="auto"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">9
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Generate 100 samples with importance weights</span>
</span></span><span style="display:flex;"><span>samples <span style="color:#f92672">=</span> [<span style="color:#f92672">..</span><span style="color:#ae81ff">.100</span> samples<span style="color:#f92672">...</span>]
</span></span><span style="display:flex;"><span>weights <span style="color:#f92672">=</span> [<span style="color:#f92672">..</span><span style="color:#ae81ff">.100</span> weights<span style="color:#f92672">...</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Compute ESS</span>
</span></span><span style="display:flex;"><span>normalized_weights <span style="color:#f92672">=</span> weights <span style="color:#f92672">/</span> sum(weights)
</span></span><span style="display:flex;"><span>ESS <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> sum(normalized_weights<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ESS â‰ˆ 1.0 out of 100 = severe weight degeneracy!</span></span></span></code></pre></td></tr></table>
</div>
</div>
<p><strong>Causes</strong>:</p>
<ul>
<li>Prior and posterior very different</li>
<li>Proposal distribution poor match for posterior</li>
<li>Model misspecification</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Use more samples</li>
<li>Better proposal distribution</li>
<li>Different inference method (MCMC)</li>
<li>Fix model (e.g., remove extra randomization)</li>
</ul>
<p><strong>Tutorial 3, Chapter 6</strong>: The DPMM notebook had weight degeneracy (ESS=1/10) due to double randomization bug, which was fixed</p>
<p><strong>See also</strong>: Importance Sampling, Effective Sample Size</p>
  </div>
</details>
<hr>
<h2 id="navigation">Navigation</h2>
<p><strong>By Tutorial</strong>:</p>
<ul>
<li><a href="/probintro/intro/index.html">Tutorial 1: Discrete Probability</a> - ðŸ“˜ Tagged terms</li>
<li><a href="/probintro/genjax/index.html">Tutorial 2: GenJAX Programming</a> - ðŸ’» Tagged terms</li>
<li><a href="/probintro/intro2/index.html">Tutorial 3: Continuous Probability</a> - ðŸ“Š Tagged terms</li>
</ul>
<p><strong>By Topic</strong>:</p>
<ul>
<li><strong>Probability Basics</strong>: Set, Outcome Space, Event, Probability, Conditional Probability</li>
<li><strong>Programming</strong>: @gen, Trace, ChoiceMap, simulate(), importance(), vmap</li>
<li><strong>Distributions</strong>: Bernoulli, Categorical, Normal/Gaussian, Beta, Uniform</li>
<li><strong>Bayesian Learning</strong>: Prior, Likelihood, Posterior, Predictive Distribution</li>
<li><strong>Advanced Models</strong>: GMM, DPMM, Dirichlet Process, Stick-breaking</li>
</ul>
<hr>
<p><em>This glossary is designed to grow with the tutorials. If a term is missing, please let us know!</em></p>

  <footer class="footline">
  </footer>
</article>
        </div>
      </main>
    </div>
    <aside id="R-sidebar" class="default-animation">
      <div id="R-header-topbar" class="default-animation"></div>
      <div id="R-header-wrapper" class="default-animation">
        <div id="R-header" class="default-animation">
          <a id="R-logo" class="R-default" href="/probintro/index.html">
            <div class="logo-title">Probability &amp; Probabilistic Computing Tutorial</div>
          </a>
        </div>
        <search><form action="/probintro/search/index.html" method="get">
          <div class="searchbox default-animation">
            <button class="search-detail" type="submit" title="Search (CTRL+ALT+f)"><i class="fas fa-search"></i></button>
            <label class="a11y-only" for="R-search-by">Search</label>
            <input data-search-input id="R-search-by" name="search-by" class="search-by" type="search" placeholder="Search...">
            <button class="search-clear" type="button" data-search-clear="" title="Clear search"><i class="fas fa-times" title="Clear search"></i></button>
          </div>
        </form></search>
      </div>
      <div id="R-homelinks" class="default-animation homelinks">
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-homelinks">
          <ul class="space collapsible-menu">
            <li class="" data-nav-id="/probintro/index.html"><a class="padding" href="/probintro/index.html"><i class="fa-fw fas fa-home"></i> Home</a></li>
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-headercontrols">
          <ul class="">
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
      </div>
      <div id="R-content-wrapper" class="highlightable">
        <div class="R-sidebarmenu R-shortcutmenu-main">
          <ul class="enlarge morespace collapsible-menu">
            <li class="alwaysopen " data-nav-id="/probintro/intro/index.html"><a class="padding" href="/probintro/intro/index.html">A Narrative Introduction to Probability</a><ul id="R-subsections-58f2a84c91d5deefe8bdd3a21213404a" class="collapsible-menu"></ul></li>
            <li class="alwaysopen " data-nav-id="/probintro/genjax/index.html"><a class="padding" href="/probintro/genjax/index.html">Probabilistic Programming with GenJAX</a><ul id="R-subsections-40a5f3b764be01351a755ea2f58fc4ae" class="collapsible-menu"></ul></li>
            <li class="alwaysopen " data-nav-id="/probintro/intro2/index.html"><a class="padding" href="/probintro/intro2/index.html">Continuous Probability and Bayesian Learning</a><ul id="R-subsections-6e37e5026001eb89123afab26613c1bf" class="collapsible-menu"></ul></li>
            <li class="" data-nav-id="/probintro/notebook_guide/index.html"><a class="padding" href="/probintro/notebook_guide/index.html">Interactive Notebooks - All Tutorials</a></li>
            <li class="active " data-nav-id="/probintro/glossary/index.html"><a class="padding" href="/probintro/glossary/index.html">Glossary - All Tutorials</a></li>
          </ul>
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-shortcuts">
          <ul class="space collapsible-menu">
          </ul>
        </div>
        <div id="R-footer-margin"></div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-footercontrols">
          <ul class="">
          </ul>
        </div>
<div id="R-footer"><p>Built with <a href="https://github.com/McShelby/hugo-theme-relearn" title="love"><i class="fas fa-heart"></i></a> by <a href="https://gohugo.io/">Hugo</a></p></div>
      </div>
    </aside>
    <script>
      window.MathJax = Object.assign( window.MathJax || {}, {
        tex: {
          inlineMath:  [['\\(', '\\)'], ['$',  '$']],  
          displayMath: [['\\[', '\\]'], ['$$', '$$']], 
        },
        options: {
          enableMenu: false 
        }
      }, JSON.parse("{}") );
    </script>
    <script id="MathJax-script" async src="/probintro/js/mathjax/tex-mml-chtml.js?1764989345"></script>
    <script src="/probintro/js/clipboard/clipboard.min.js?1764989345" defer></script>
    <script src="/probintro/js/perfect-scrollbar/perfect-scrollbar.min.js?1764989345" defer></script>
    <script src="/probintro/js/theme.js?1764989345" defer></script>
  </body>
</html>
