{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Solution: Gaussian Bayesian Update (Problem 1)\n\n## Problem Setup\n\nIn class, we derived the posterior and predictive distributions for a Gaussian-Gaussian model:\n\n**Generative Process:**\n$$\\mu \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$$\n$$x_1, \\ldots, x_N | \\mu, \\sigma_x^2 \\overset{iid}{\\sim} \\mathcal{N}(\\mu, \\sigma_x^2)$$\n\n**Posterior Distribution:**\n$$\\mu | x_1, \\ldots, x_N \\sim \\mathcal{N}\\left( \\frac{\\mu_0 \\sigma_0^{-2} + \\sigma_x^{-2} \\sum_{n=1}^N x_n}{\\sigma_0^{-2} + N \\sigma_x^{-2}}, \\left[ \\sigma_0^{-2} + N \\sigma_x^{-2} \\right]^{-1} \\right)$$\n\n**Predictive Distribution:**\n$$x_{N+1} | x_1, \\ldots, x_N \\sim \\mathcal{N}\\left( \\frac{\\mu_0 \\sigma_0^{-2} + \\sigma_x^{-2} \\sum_{n=1}^N x_n}{\\sigma_0^{-2} + N \\sigma_x^{-2}}, \\left[ \\sigma_0^{-2} + N \\sigma_x^{-2} \\right]^{-1} + \\sigma_x^2 \\right)$$\n\nFor this problem, use $\\mu_0 = 0$ and $\\sigma_0^2 = 1$.\n\nWe will explore how the number of data points and variance of the likelihood affect the posterior and predictive distributions.\n\n---\n\n## üìö Reviewing Key Concepts\n\nThis problem builds on concepts from earlier chapters:\n\n**From [Tutorial 1, Chapter 5 - Bayes' Theorem](../../content/intro/05_bayes.md)**:\n- Remember Bayes' rule: **Posterior ‚àù Likelihood √ó Prior**\n- We're applying it to continuous distributions here!\n- $p(\\mu|data) = \\frac{p(data|\\mu) \\cdot p(\\mu)}{p(data)}$\n\n**From [Tutorial 2, Chapter 3 - Gaussian Distribution](../../content/intro2/03_gaussian.md)**:\n- The Gaussian (Normal) distribution N(Œº, œÉ¬≤) with bell curve shape\n- The 68-95-99.7 rule for standard deviations\n- Why Gaussians appear everywhere (Central Limit Theorem)\n\n**From [Tutorial 2, Chapter 4 - Bayesian Learning](../../content/intro2/04_bayesian_learning.md)**:\n- **Conjugate priors**: Gaussian prior + Gaussian likelihood = Gaussian posterior\n- **Precision-weighted averaging**: Posterior mean balances prior and data\n- **Sequential learning**: Update one observation at a time\n- **Predictive distribution**: Combines posterior uncertainty + data variance\n\n**What's new in this assignment:**\n- **Systematic exploration**: How do œÉ¬≤_x and N affect learning?\n- **Visual intuition**: See the precision-weighting in action\n- **Verification**: Compare analytical formulas with GenJAX simulations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import packages\nimport jax\nimport jax.numpy as jnp\nimport jax.random as random\nfrom genjax import gen, normal\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Configure matplotlib\nplt.style.use('seaborn-v0_8-whitegrid')\n%matplotlib inline\n\n# Set random seed\nnp.random.seed(42)\nkey = random.PRNGKey(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_posterior(mu_0, sigma_0_squared, x_s, sigma_x_squared, x_min, x_max):\n",
    "    \"\"\"\n",
    "    Analytical Bayesian update for Gaussian-Gaussian conjugate prior.\n",
    "    \n",
    "    Args:\n",
    "        mu_0: Prior mean\n",
    "        sigma_0_squared: Prior variance\n",
    "        x_s: List or array of observations\n",
    "        sigma_x_squared: Likelihood variance (known)\n",
    "        x_min: Minimum x value for plotting\n",
    "        x_max: Maximum x value for plotting\n",
    "    \n",
    "    Returns:\n",
    "        posterior_mu: Posterior mean\n",
    "        posterior_pdf: Posterior PDF values\n",
    "        predictive_mu: Predictive mean\n",
    "        predictive_pdf: Predictive PDF values\n",
    "    \"\"\"\n",
    "    n = len(x_s)\n",
    "    \n",
    "    # Posterior parameters\n",
    "    posterior_mu = (mu_0 / sigma_0_squared + sum(x_s) / sigma_x_squared) / \\\n",
    "                   (1 / sigma_0_squared + n / sigma_x_squared)\n",
    "    posterior_sigma_squared = 1 / (1 / sigma_0_squared + n / sigma_x_squared)\n",
    "    posterior_sigma = np.sqrt(posterior_sigma_squared)\n",
    "    \n",
    "    # Predictive parameters\n",
    "    predictive_mu = posterior_mu\n",
    "    predictive_sigma_squared = posterior_sigma_squared + sigma_x_squared\n",
    "    predictive_sigma = np.sqrt(predictive_sigma_squared)\n",
    "    \n",
    "    # Compute PDFs for plotting\n",
    "    x_range = np.linspace(x_min, x_max, 1000)\n",
    "    posterior_pdf = norm.pdf(x_range, posterior_mu, posterior_sigma)\n",
    "    predictive_pdf = norm.pdf(x_range, predictive_mu, predictive_sigma)\n",
    "    \n",
    "    return posterior_mu, posterior_pdf, predictive_mu, predictive_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GenJAX Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@gen\ndef gaussian_learning_model(observations, mu_0, sigma_0, sigma_x):\n    \"\"\"\n    GenJAX generative model for Gaussian learning.\n    \n    Args:\n        observations: Observed data points\n        mu_0: Prior mean\n        sigma_0: Prior standard deviation\n        sigma_x: Likelihood standard deviation (known)\n    \"\"\"\n    # Prior on unknown mean\n    mu = normal(mu_0, sigma_0) @ \"mu\"\n    \n    # Generate observations\n    for i in range(len(observations)):\n        x = normal(mu, sigma_x) @ f\"obs_{i}\"\n    \n    return mu\n\n@gen\ndef posterior_predictive(posterior_mu, posterior_sigma, sigma_x):\n    \"\"\"\n    Sample from posterior predictive distribution.\n    \n    Args:\n        posterior_mu: Posterior mean for mu\n        posterior_sigma: Posterior std dev for mu\n        sigma_x: Likelihood standard deviation\n    \"\"\"\n    # Sample mu from posterior\n    mu = normal(posterior_mu, posterior_sigma) @ \"mu\"\n    \n    # Sample new observation\n    x_new = normal(mu, sigma_x) @ \"x_new\"\n    \n    return x_new"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 1(a): Prior Distribution\n",
    "\n",
    "Plot the prior distribution to provide a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prior parameters\nmu_0 = 0\nsigma_0 = 1\n\n# Axis range\nx = np.linspace(-8, 8, 1000)\n\n# Density\ny = norm.pdf(x, mu_0, sigma_0)\n\n# Plot prior\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, label=r'$\\mathcal{N}(\\mu_0=0, \\sigma_0^2=1)$', color='red', linewidth=2)\nplt.axvline(mu_0, color='red', linestyle='--', linewidth=1, alpha=0.5, label=f'Prior mean: {mu_0}')\n\n# Mark 68-95-99.7 regions\nplt.axvline(mu_0 - sigma_0, color='gray', linestyle=':', linewidth=1, alpha=0.5)\nplt.axvline(mu_0 + sigma_0, color='gray', linestyle=':', linewidth=1, alpha=0.5, label='¬±1œÉ (68%)')\nplt.axvline(mu_0 - 2*sigma_0, color='gray', linestyle=':', linewidth=1, alpha=0.3)\nplt.axvline(mu_0 + 2*sigma_0, color='gray', linestyle=':', linewidth=1, alpha=0.3, label='¬±2œÉ (95%)')\n\nplt.xlabel(r'$\\mu$', fontsize=14)\nplt.ylabel('Density', fontsize=14)\nplt.title('Prior Distribution', fontsize=16)\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüìä Interpretation:\")\nprint(f\"  The prior distribution is centered at Œº = {mu_0}, with standard deviation œÉ = {sigma_0}.\")\nprint(f\"  68% of the prior mass is between {mu_0 - sigma_0} and {mu_0 + sigma_0}.\")\nprint(f\"  95% of the prior mass is between {mu_0 - 2*sigma_0} and {mu_0 + 2*sigma_0}.\")\nprint(f\"  The distribution drops quickly beyond ¬±3œÉ.\")\nprint(f\"\\n  üìñ Recall the 68-95-99.7 rule from Chapter 3:\")\nprint(f\"     This is a standard Gaussian N(0,1), so it follows the empirical rule perfectly!\")\nprint(f\"     [Review: Tutorial 2, Chapter 3 - Gaussian Distribution]\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 1(b): One Datum Update\n",
    "\n",
    "Calculate and plot the posterior and predictive distributions after observing $x_1 = 2$ for:\n",
    "- $\\sigma_x^2 = 0.25$ (small variance, precise measurements)\n",
    "- $\\sigma_x^2 = 4$ (large variance, noisy measurements)\n",
    "\n",
    "**Question**: How does changing the variance of the likelihood affect the distributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior parameters\n",
    "mu_0 = 0\n",
    "sigma_0_squared = 1\n",
    "\n",
    "# Observation\n",
    "x_1 = 2\n",
    "\n",
    "# Likelihood variances to compare\n",
    "sigma_x_squared_values = [0.25, 4]\n",
    "\n",
    "# Plot range\n",
    "x_min = -8\n",
    "x_max = 8\n",
    "x_range = np.linspace(x_min, x_max, 1000)\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for i, sigma_x_squared in enumerate(sigma_x_squared_values):\n",
    "    # Update\n",
    "    posterior_mu, posterior_pdf, predictive_mu, predictive_pdf = update_posterior(\n",
    "        mu_0, sigma_0_squared, [x_1], sigma_x_squared, x_min, x_max\n",
    "    )\n",
    "    \n",
    "    # Posterior distribution\n",
    "    axes[i, 0].plot(x_range, posterior_pdf, label=f'Posterior (œÉ¬≤_x={sigma_x_squared})', \n",
    "                   color='blue', linewidth=2)\n",
    "    axes[i, 0].axvline(posterior_mu, color='blue', linestyle='--', linewidth=1.5, \n",
    "                      label=f'Posterior mean = {posterior_mu:.2f}')\n",
    "    axes[i, 0].axvline(x_1, color='red', linestyle=':', linewidth=1.5, \n",
    "                      label=f'Observation: {x_1}')\n",
    "    \n",
    "    # Add prior for comparison\n",
    "    prior_pdf = norm.pdf(x_range, mu_0, np.sqrt(sigma_0_squared))\n",
    "    axes[i, 0].plot(x_range, prior_pdf, 'k--', linewidth=1.5, alpha=0.5, label='Prior')\n",
    "    \n",
    "    axes[i, 0].set_title(f'Posterior Distribution (œÉ¬≤_x = {sigma_x_squared})', fontsize=13)\n",
    "    axes[i, 0].set_xlabel('Œº', fontsize=12)\n",
    "    axes[i, 0].set_ylabel('Density', fontsize=12)\n",
    "    axes[i, 0].legend(fontsize=10)\n",
    "    axes[i, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Predictive distribution\n",
    "    axes[i, 1].plot(x_range, predictive_pdf, label=f'Predictive (œÉ¬≤_x={sigma_x_squared})', \n",
    "                   color='orange', linewidth=2)\n",
    "    axes[i, 1].axvline(predictive_mu, color='orange', linestyle='--', linewidth=1.5, \n",
    "                      label=f'Predictive mean = {predictive_mu:.2f}')\n",
    "    axes[i, 1].axvline(x_1, color='red', linestyle=':', linewidth=1.5, \n",
    "                      label=f'Observation: {x_1}')\n",
    "    \n",
    "    axes[i, 1].set_title(f'Predictive Distribution (œÉ¬≤_x = {sigma_x_squared})', fontsize=13)\n",
    "    axes[i, 1].set_xlabel('x', fontsize=12)\n",
    "    axes[i, 1].set_ylabel('Density', fontsize=12)\n",
    "    axes[i, 1].legend(fontsize=10)\n",
    "    axes[i, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "**Effect of Likelihood Variance:**\n",
    "\n",
    "According to the update formula for posterior distributions:\n",
    "\n",
    "$$\\sigma_N^2 = \\left[ \\sigma_0^{-2} + N \\sigma_x^{-2} \\right]^{-1}$$\n",
    "\n",
    "A **smaller likelihood variance** ($\\sigma_x^2$) results in:\n",
    "- **Greater shrinkage**: The posterior concentrates more sharply around its peak\n",
    "- **Stronger data influence**: The posterior mean moves closer to the observed data\n",
    "- This is because the posterior mean is a precision-weighted average:\n",
    "  $$\\mu_N = \\frac{\\text{precision}_{\\text{prior}} \\times \\mu_0 + \\text{precision}_{\\text{data}} \\times \\bar{x}}{\\text{precision}_{\\text{prior}} + \\text{precision}_{\\text{data}}}$$\n",
    "  \n",
    "Where precision = $1/\\text{variance}$. Smaller $\\sigma_x^2$ means higher data precision, so the data gets more weight.\n",
    "\n",
    "**Key Observations:**\n",
    "- With $\\sigma_x^2 = 0.25$ (precise data): Posterior is narrow and close to $x_1 = 2$\n",
    "- With $\\sigma_x^2 = 4$ (noisy data): Posterior is wider and stays closer to prior mean (0)\n",
    "- The predictive distribution is always more dispersed than the posterior (adds $\\sigma_x^2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical comparison\n",
    "print(\"\\nüìä Numerical Comparison:\\n\")\n",
    "print(f\"{'Quantity':<30} {'œÉ¬≤_x = 0.25':<20} {'œÉ¬≤_x = 4':<20}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for sigma_x_squared in sigma_x_squared_values:\n",
    "    post_mu, _, pred_mu, _ = update_posterior(mu_0, sigma_0_squared, [x_1], sigma_x_squared, x_min, x_max)\n",
    "    \n",
    "    # Calculate variances\n",
    "    post_var = 1 / (1/sigma_0_squared + 1/sigma_x_squared)\n",
    "    pred_var = post_var + sigma_x_squared\n",
    "    \n",
    "    col_name = f\"œÉ¬≤_x = {sigma_x_squared}\"\n",
    "    \n",
    "    if sigma_x_squared == 0.25:\n",
    "        print(f\"{'Posterior mean:':<30} {post_mu:<20.2f}\", end=\"\")\n",
    "    else:\n",
    "        post_mu_prev, _, _, _ = update_posterior(mu_0, sigma_0_squared, [x_1], 0.25, x_min, x_max)\n",
    "        print(f\"{post_mu:<20.2f}\")\n",
    "        \n",
    "        print(f\"{'Posterior variance:':<30} {1/(1/sigma_0_squared + 1/0.25):<20.2f} {post_var:<20.2f}\")\n",
    "        print(f\"{'Predictive variance:':<30} {1/(1/sigma_0_squared + 1/0.25) + 0.25:<20.2f} {pred_var:<20.2f}\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n‚úÖ Conclusion:\")\n",
    "print(\"  ‚Ä¢ Smaller œÉ¬≤_x ‚Üí Posterior closer to data, more concentrated\")\n",
    "print(\"  ‚Ä¢ Larger œÉ¬≤_x ‚Üí Posterior closer to prior, more dispersed\")\n",
    "print(\"  ‚Ä¢ Predictive always has larger variance than posterior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 1(c): Multiple Data Update\n",
    "\n",
    "Calculate and plot the posterior and predictive distributions given:\n",
    "$$(x_1, \\ldots, x_5) = (2.1, 2.5, 1.4, 2.2, 1.8)$$\n",
    "\n",
    "for $\\sigma_x^2 = 0.25$ and $\\sigma_x^2 = 4$.\n",
    "\n",
    "**Question**: How does this compare to the single observation case? Note that the average is 2.0 in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior parameters\n",
    "mu_0 = 0\n",
    "sigma_0_squared = 1\n",
    "\n",
    "# Observations\n",
    "x_values = np.array([2.1, 2.5, 1.4, 2.2, 1.8])\n",
    "N = len(x_values)\n",
    "sample_mean = np.mean(x_values)\n",
    "\n",
    "print(f\"Observations: {x_values}\")\n",
    "print(f\"Sample size: N = {N}\")\n",
    "print(f\"Sample mean: {sample_mean:.2f}\")\n",
    "print(f\"\\nNote: In part (b), we had 1 observation at x‚ÇÅ = 2.0\")\n",
    "print(f\"      In part (c), we have 5 observations with mean = 2.0\")\n",
    "print(f\"      Both have the same average value!\\n\")\n",
    "\n",
    "# Likelihood variances\n",
    "sigma_x_squared_values = [0.25, 4]\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for i, sigma_x_squared in enumerate(sigma_x_squared_values):\n",
    "    # Update with 1 observation (from part b)\n",
    "    posterior_mu_1, posterior_pdf_1, predictive_mu_1, predictive_pdf_1 = update_posterior(\n",
    "        mu_0, sigma_0_squared, [x_1], sigma_x_squared, x_min, x_max\n",
    "    )\n",
    "    \n",
    "    # Update with 5 observations (part c)\n",
    "    posterior_mu, posterior_pdf, predictive_mu, predictive_pdf = update_posterior(\n",
    "        mu_0, sigma_0_squared, x_values, sigma_x_squared, x_min, x_max\n",
    "    )\n",
    "    \n",
    "    # Posterior distribution comparison\n",
    "    axes[i, 0].plot(x_range, posterior_pdf, label=f'Posterior (N=5, œÉ¬≤_x={sigma_x_squared})', \n",
    "                   color='blue', linewidth=2)\n",
    "    axes[i, 0].plot(x_range, posterior_pdf_1, label=f'Posterior (N=1, œÉ¬≤_x={sigma_x_squared})', \n",
    "                   color='blue', linewidth=2, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    axes[i, 0].axvline(posterior_mu, color='blue', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "    axes[i, 0].axvline(posterior_mu_1, color='blue', linestyle='--', linewidth=1, alpha=0.3)\n",
    "    axes[i, 0].axvline(sample_mean, color='red', linestyle=':', linewidth=2, \n",
    "                      label=f'Sample mean: {sample_mean:.2f}')\n",
    "    \n",
    "    axes[i, 0].set_title(f'Posterior Distribution (œÉ¬≤_x = {sigma_x_squared})', fontsize=13)\n",
    "    axes[i, 0].set_xlabel('Œº', fontsize=12)\n",
    "    axes[i, 0].set_ylabel('Density', fontsize=12)\n",
    "    axes[i, 0].legend(fontsize=9)\n",
    "    axes[i, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Predictive distribution comparison\n",
    "    axes[i, 1].plot(x_range, predictive_pdf, label=f'Predictive (N=5, œÉ¬≤_x={sigma_x_squared})', \n",
    "                   color='orange', linewidth=2)\n",
    "    axes[i, 1].plot(x_range, predictive_pdf_1, label=f'Predictive (N=1, œÉ¬≤_x={sigma_x_squared})', \n",
    "                   color='orange', linewidth=2, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    axes[i, 1].axvline(predictive_mu, color='orange', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "    axes[i, 1].axvline(predictive_mu_1, color='orange', linestyle='--', linewidth=1, alpha=0.3)\n",
    "    axes[i, 1].axvline(sample_mean, color='red', linestyle=':', linewidth=2, \n",
    "                      label=f'Sample mean: {sample_mean:.2f}')\n",
    "    \n",
    "    axes[i, 1].set_title(f'Predictive Distribution (œÉ¬≤_x = {sigma_x_squared})', fontsize=13)\n",
    "    axes[i, 1].set_xlabel('x', fontsize=12)\n",
    "    axes[i, 1].set_ylabel('Density', fontsize=12)\n",
    "    axes[i, 1].legend(fontsize=9)\n",
    "    axes[i, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "**Comparison: 1 observation vs 5 observations (both with mean = 2.0)**\n",
    "\n",
    "The key insight is that the posterior variance depends on **both** the likelihood variance AND the number of observations:\n",
    "\n",
    "$$\\sigma_N^2 = \\left[ \\sigma_0^{-2} + \\frac{N}{\\sigma_x^2} \\right]^{-1}$$\n",
    "\n",
    "**Effect of increasing N (given fixed $\\sigma_x^2$):**\n",
    "\n",
    "1. **Posterior becomes more concentrated**: More observations ‚Üí smaller $\\sigma_N^2$\n",
    "2. **Posterior mean moves closer to sample mean**: Higher data precision\n",
    "3. **Predictive distribution also becomes more concentrated**: Smaller $\\sigma_N^2$ component\n",
    "\n",
    "**Why the difference between N=1 and N=5?**\n",
    "\n",
    "Even though both have the same mean (2.0), the **effective precision** of the data is different:\n",
    "- N=1: Data precision = $1/\\sigma_x^2$\n",
    "- N=5: Data precision = $5/\\sigma_x^2$ (5√ó higher!)\n",
    "\n",
    "**Specific Observations:**\n",
    "- With $\\sigma_x^2 = 0.25$: Both posteriors are narrow, but N=5 is much sharper\n",
    "- With $\\sigma_x^2 = 4$: N=1 posterior stays close to prior; N=5 shifts significantly toward data\n",
    "- The posterior mean is the same for both (because sample mean is the same)\n",
    "- But the **confidence** (inverse of variance) is much higher with N=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical comparison\n",
    "print(\"\\nüìä Numerical Comparison: N=1 vs N=5\\n\")\n",
    "print(f\"{'Quantity':<35} {'N=1, œÉ¬≤_x=0.25':<18} {'N=5, œÉ¬≤_x=0.25':<18} {'N=1, œÉ¬≤_x=4':<18} {'N=5, œÉ¬≤_x=4':<18}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "results = {}\n",
    "for n_obs in [1, 5]:\n",
    "    obs = [x_1] if n_obs == 1 else x_values\n",
    "    for sigma_x_sq in sigma_x_squared_values:\n",
    "        post_mu, _, pred_mu, _ = update_posterior(mu_0, sigma_0_squared, obs, sigma_x_sq, x_min, x_max)\n",
    "        post_var = 1 / (1/sigma_0_squared + n_obs/sigma_x_sq)\n",
    "        pred_var = post_var + sigma_x_sq\n",
    "        results[(n_obs, sigma_x_sq)] = {\n",
    "            'post_mu': post_mu,\n",
    "            'post_var': post_var,\n",
    "            'post_std': np.sqrt(post_var),\n",
    "            'pred_var': pred_var,\n",
    "            'pred_std': np.sqrt(pred_var)\n",
    "        }\n",
    "\n",
    "print(f\"{'Posterior mean:':<35} {results[(1,0.25)]['post_mu']:<18.3f} {results[(5,0.25)]['post_mu']:<18.3f} \"\n",
    "      f\"{results[(1,4)]['post_mu']:<18.3f} {results[(5,4)]['post_mu']:<18.3f}\")\n",
    "print(f\"{'Posterior std dev:':<35} {results[(1,0.25)]['post_std']:<18.3f} {results[(5,0.25)]['post_std']:<18.3f} \"\n",
    "      f\"{results[(1,4)]['post_std']:<18.3f} {results[(5,4)]['post_std']:<18.3f}\")\n",
    "print(f\"{'Predictive std dev:':<35} {results[(1,0.25)]['pred_std']:<18.3f} {results[(5,0.25)]['pred_std']:<18.3f} \"\n",
    "      f\"{results[(1,4)]['pred_std']:<18.3f} {results[(5,4)]['pred_std']:<18.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*110)\n",
    "print(\"\\n‚úÖ Key Findings:\")\n",
    "print(f\"  ‚Ä¢ Posterior mean is similar for N=1 and N=5 (both ‚âà {sample_mean:.1f}) because sample mean is the same\")\n",
    "print(f\"  ‚Ä¢ Posterior std dev DECREASES with more observations (N‚Üë ‚Üí uncertainty‚Üì)\")\n",
    "print(f\"  ‚Ä¢ With œÉ¬≤_x=0.25 (precise data): N=5 gives much sharper posterior than N=1\")\n",
    "print(f\"  ‚Ä¢ With œÉ¬≤_x=4 (noisy data): Effect is less dramatic but still significant\")\n",
    "print(f\"  ‚Ä¢ More data = more confidence, even if the mean stays the same!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GenJAX Verification\n",
    "\n",
    "Let's verify our analytical results using GenJAX simulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify with GenJAX simulation\nprint(\"üî¨ GenJAX Verification: Posterior Predictive Sampling\\n\")\n\n# Use the N=5, œÉ¬≤_x=0.25 case\nsigma_x_squared = 0.25\nsigma_x = np.sqrt(sigma_x_squared)\n\n# Analytical results\npost_mu_analytical, _, pred_mu_analytical, _ = update_posterior(\n    mu_0, sigma_0_squared, x_values, sigma_x_squared, x_min, x_max\n)\npost_var_analytical = 1 / (1/sigma_0_squared + N/sigma_x_squared)\npost_std_analytical = np.sqrt(post_var_analytical)\npred_var_analytical = post_var_analytical + sigma_x_squared\npred_std_analytical = np.sqrt(pred_var_analytical)\n\nprint(f\"Analytical results (N={N}, œÉ¬≤_x={sigma_x_squared}):\")\nprint(f\"  Posterior: N({post_mu_analytical:.3f}, {post_var_analytical:.3f})\")\nprint(f\"  Predictive: N({pred_mu_analytical:.3f}, {pred_var_analytical:.3f})\")\nprint()\n\n# GenJAX simulation\nkey = random.PRNGKey(42)\nn_samples = 5000\n\npredictions = []\nfor _ in range(n_samples):\n    key, subkey = random.split(key)\n    trace = posterior_predictive.simulate(subkey, (post_mu_analytical, post_std_analytical, sigma_x))\n    predictions.append(float(trace.get_retval()))\n\npredictions = np.array(predictions)\n\nprint(f\"GenJAX simulation results ({n_samples} samples):\")\nprint(f\"  Predictive mean: {np.mean(predictions):.3f} (analytical: {pred_mu_analytical:.3f})\")\nprint(f\"  Predictive std: {np.std(predictions):.3f} (analytical: {pred_std_analytical:.3f})\")\nprint()\n\n# Plot comparison\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\n\n# Histogram of samples\nax.hist(predictions, bins=50, density=True, alpha=0.6, color='skyblue', \n        edgecolor='black', label='GenJAX samples')\n\n# Analytical PDF\nx_plot = np.linspace(-2, 6, 1000)\nanalytical_pdf = norm.pdf(x_plot, pred_mu_analytical, pred_std_analytical)\nax.plot(x_plot, analytical_pdf, 'r-', linewidth=2, \n        label=f'Analytical: N({pred_mu_analytical:.2f}, {pred_var_analytical:.2f})')\n\nax.axvline(np.mean(predictions), color='blue', linestyle='--', linewidth=1.5, \n          label=f'Sample mean: {np.mean(predictions):.2f}')\nax.axvline(pred_mu_analytical, color='red', linestyle='--', linewidth=1.5, alpha=0.5,\n          label=f'Analytical mean: {pred_mu_analytical:.2f}')\n\nax.set_xlabel('x (predicted observation)', fontsize=12)\nax.set_ylabel('Density', fontsize=12)\nax.set_title('GenJAX Posterior Predictive vs Analytical', fontsize=14)\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚úÖ GenJAX simulation matches analytical results!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Insights from Problem 1:\n",
    "\n",
    "1. **Effect of Likelihood Variance ($\\sigma_x^2$)**:\n",
    "   - Smaller variance ‚Üí data more influential ‚Üí posterior closer to data\n",
    "   - Larger variance ‚Üí prior more influential ‚Üí posterior closer to prior\n",
    "   - This is captured by the **precision-weighted average** formula\n",
    "\n",
    "2. **Effect of Number of Observations (N)**:\n",
    "   - More observations ‚Üí higher effective data precision ($N/\\sigma_x^2$)\n",
    "   - Posterior becomes more concentrated (smaller variance)\n",
    "   - Posterior mean converges to sample mean as N ‚Üí ‚àû\n",
    "\n",
    "3. **Predictive Distribution**:\n",
    "   - Always more dispersed than posterior (adds $\\sigma_x^2$)\n",
    "   - Accounts for both parameter uncertainty AND data variability\n",
    "   - Mean is same as posterior mean\n",
    "\n",
    "4. **Precision Interpretation**:\n",
    "   - Prior precision: $1/\\sigma_0^2$\n",
    "   - Data precision: $N/\\sigma_x^2$\n",
    "   - Posterior precision: sum of the two\n",
    "   - Higher precision = more certainty\n",
    "\n",
    "### Mathematical Framework:\n",
    "\n",
    "**Posterior**:\n",
    "$$\\mu_N = \\frac{\\frac{1}{\\sigma_0^2} \\mu_0 + \\frac{N}{\\sigma_x^2} \\bar{x}}{\\frac{1}{\\sigma_0^2} + \\frac{N}{\\sigma_x^2}}, \\quad \\sigma_N^2 = \\frac{1}{\\frac{1}{\\sigma_0^2} + \\frac{N}{\\sigma_x^2}}$$\n",
    "\n",
    "**Predictive**:\n",
    "$$x_{N+1} \\sim \\mathcal{N}(\\mu_N, \\sigma_N^2 + \\sigma_x^2)$$\n",
    "\n",
    "This elegant framework allows us to:\n",
    "- Update beliefs sequentially as data arrives\n",
    "- Balance prior knowledge with observed data\n",
    "- Quantify uncertainty about future observations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}