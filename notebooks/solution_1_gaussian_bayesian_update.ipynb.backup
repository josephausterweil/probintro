{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution: Gaussian Bayesian Update (Problem 1)\n",
    "\n",
    "## Problem Setup\n",
    "\n",
    "In class, we derived the posterior and predictive distributions for a Gaussian-Gaussian model:\n",
    "\n",
    "**Generative Process:**\n",
    "$$\\mu \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$$\n",
    "$$x_1, \\ldots, x_N | \\mu, \\sigma_x^2 \\overset{iid}{\\sim} \\mathcal{N}(\\mu, \\sigma_x^2)$$\n",
    "\n",
    "**Posterior Distribution:**\n",
    "$$\\mu | x_1, \\ldots, x_N \\sim \\mathcal{N}\\left( \\frac{\\mu_0 \\sigma_0^{-2} + \\sigma_x^{-2} \\sum_{n=1}^N x_n}{\\sigma_0^{-2} + N \\sigma_x^{-2}}, \\left[ \\sigma_0^{-2} + N \\sigma_x^{-2} \\right]^{-1} \\right)$$\n",
    "\n",
    "**Predictive Distribution:**\n",
    "$$x_{N+1} | x_1, \\ldots, x_N \\sim \\mathcal{N}\\left( \\frac{\\mu_0 \\sigma_0^{-2} + \\sigma_x^{-2} \\sum_{n=1}^N x_n}{\\sigma_0^{-2} + N \\sigma_x^{-2}}, \\left[ \\sigma_0^{-2} + N \\sigma_x^{-2} \\right]^{-1} + \\sigma_x^2 \\right)$$\n",
    "\n",
    "For this problem, use $\\mu_0 = 0$ and $\\sigma_0^2 = 1$.\n",
    "\n",
    "We will explore how the number of data points and variance of the likelihood affect the posterior and predictive distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "from genjax import gen, simulate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "key = random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_posterior(mu_0, sigma_0_squared, x_s, sigma_x_squared, x_min, x_max):\n",
    "    \"\"\"\n",
    "    Analytical Bayesian update for Gaussian-Gaussian conjugate prior.\n",
    "    \n",
    "    Args:\n",
    "        mu_0: Prior mean\n",
    "        sigma_0_squared: Prior variance\n",
    "        x_s: List or array of observations\n",
    "        sigma_x_squared: Likelihood variance (known)\n",
    "        x_min: Minimum x value for plotting\n",
    "        x_max: Maximum x value for plotting\n",
    "    \n",
    "    Returns:\n",
    "        posterior_mu: Posterior mean\n",
    "        posterior_pdf: Posterior PDF values\n",
    "        predictive_mu: Predictive mean\n",
    "        predictive_pdf: Predictive PDF values\n",
    "    \"\"\"\n",
    "    n = len(x_s)\n",
    "    \n",
    "    # Posterior parameters\n",
    "    posterior_mu = (mu_0 / sigma_0_squared + sum(x_s) / sigma_x_squared) / \\\n",
    "                   (1 / sigma_0_squared + n / sigma_x_squared)\n",
    "    posterior_sigma_squared = 1 / (1 / sigma_0_squared + n / sigma_x_squared)\n",
    "    posterior_sigma = np.sqrt(posterior_sigma_squared)\n",
    "    \n",
    "    # Predictive parameters\n",
    "    predictive_mu = posterior_mu\n",
    "    predictive_sigma_squared = posterior_sigma_squared + sigma_x_squared\n",
    "    predictive_sigma = np.sqrt(predictive_sigma_squared)\n",
    "    \n",
    "    # Compute PDFs for plotting\n",
    "    x_range = np.linspace(x_min, x_max, 1000)\n",
    "    posterior_pdf = norm.pdf(x_range, posterior_mu, posterior_sigma)\n",
    "    predictive_pdf = norm.pdf(x_range, predictive_mu, predictive_sigma)\n",
    "    \n",
    "    return posterior_mu, posterior_pdf, predictive_mu, predictive_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GenJAX Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def gaussian_learning_model(observations, mu_0, sigma_0, sigma_x):\n",
    "    \"\"\"\n",
    "    GenJAX generative model for Gaussian learning.\n",
    "    \n",
    "    Args:\n",
    "        observations: Observed data points\n",
    "        mu_0: Prior mean\n",
    "        sigma_0: Prior standard deviation\n",
    "        sigma_x: Likelihood standard deviation (known)\n",
    "    \"\"\"\n",
    "    # Prior on unknown mean\n",
    "    mu = jnp.normal(mu_0, sigma_0) @ \"mu\"\n",
    "    \n",
    "    # Generate observations\n",
    "    for i in range(len(observations)):\n",
    "        x = jnp.normal(mu, sigma_x) @ f\"obs_{i}\"\n",
    "    \n",
    "    return mu\n",
    "\n",
    "@gen\n",
    "def posterior_predictive(posterior_mu, posterior_sigma, sigma_x):\n",
    "    \"\"\"\n",
    "    Sample from posterior predictive distribution.\n",
    "    \n",
    "    Args:\n",
    "        posterior_mu: Posterior mean for mu\n",
    "        posterior_sigma: Posterior std dev for mu\n",
    "        sigma_x: Likelihood standard deviation\n",
    "    \"\"\"\n",
    "    # Sample mu from posterior\n",
    "    mu = jnp.normal(posterior_mu, posterior_sigma) @ \"mu\"\n",
    "    \n",
    "    # Sample new observation\n",
    "    x_new = jnp.normal(mu, sigma_x) @ \"x_new\"\n",
    "    \n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 1(a): Prior Distribution\n",
    "\n",
    "Plot the prior distribution to provide a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior parameters\n",
    "mu_0 = 0\n",
    "sigma_0 = 1\n",
    "\n",
    "# Axis range\n",
    "x = np.linspace(-8, 8, 1000)\n",
    "\n",
    "# Density\n",
    "y = norm.pdf(x, mu_0, sigma_0)\n",
    "\n",
    "# Plot prior\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, label=r'$\\mathcal{N}(\\mu_0=0, \\sigma_0^2=1)$', color='red', linewidth=2)\n",
    "plt.axvline(mu_0, color='red', linestyle='--', linewidth=1, alpha=0.5, label=f'Prior mean: {mu_0}')\n",
    "\n",
    "# Mark 68-95-99.7 regions\n",
    "plt.axvline(mu_0 - sigma_0, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
    "plt.axvline(mu_0 + sigma_0, color='gray', linestyle=':', linewidth=1, alpha=0.5, label='Â±1Ïƒ (68%)')\n",
    "plt.axvline(mu_0 - 2*sigma_0, color='gray', linestyle=':', linewidth=1, alpha=0.3)\n",
    "plt.axvline(mu_0 + 2*sigma_0, color='gray', linestyle=':', linewidth=1, alpha=0.3, label='Â±2Ïƒ (95%)')\n",
    "\n",
    "plt.xlabel(r'$\\mu$', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.title('Prior Distribution', fontsize=16)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Interpretation:\")\n",
    "print(f\"  The prior distribution is centered at Î¼ = {mu_0}, with standard deviation Ïƒ = {sigma_0}.\")\n",
    "print(f\"  68% of the prior mass is between {mu_0 - sigma_0} and {mu_0 + sigma_0}.\")\n",
    "print(f\"  95% of the prior mass is between {mu_0 - 2*sigma_0} and {mu_0 + 2*sigma_0}.\")\n",
    "print(f\"  The distribution drops quickly beyond Â±3Ïƒ.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 1(b): One Datum Update\n",
    "\n",
    "Calculate and plot the posterior and predictive distributions after observing $x_1 = 2$ for:\n",
    "- $\\sigma_x^2 = 0.25$ (small variance, precise measurements)\n",
    "- $\\sigma_x^2 = 4$ (large variance, noisy measurements)\n",
    "\n",
    "**Question**: How does changing the variance of the likelihood affect the distributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior parameters\n",
    "mu_0 = 0\n",
    "sigma_0_squared = 1\n",
    "\n",
    "# Observation\n",
    "x_1 = 2\n",
    "\n",
    "# Likelihood variances to compare\n",
    "sigma_x_squared_values = [0.25, 4]\n",
    "\n",
    "# Plot range\n",
    "x_min = -8\n",
    "x_max = 8\n",
    "x_range = np.linspace(x_min, x_max, 1000)\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for i, sigma_x_squared in enumerate(sigma_x_squared_values):\n",
    "    # Update\n",
    "    posterior_mu, posterior_pdf, predictive_mu, predictive_pdf = update_posterior(\n",
    "        mu_0, sigma_0_squared, [x_1], sigma_x_squared, x_min, x_max\n",
    "    )\n",
    "    \n",
    "    # Posterior distribution\n",
    "    axes[i, 0].plot(x_range, posterior_pdf, label=f'Posterior (ÏƒÂ²_x={sigma_x_squared})', \n",
    "                   color='blue', linewidth=2)\n",
    "    axes[i, 0].axvline(posterior_mu, color='blue', linestyle='--', linewidth=1.5, \n",
    "                      label=f'Posterior mean = {posterior_mu:.2f}')\n",
    "    axes[i, 0].axvline(x_1, color='red', linestyle=':', linewidth=1.5, \n",
    "                      label=f'Observation: {x_1}')\n",
    "    \n",
    "    # Add prior for comparison\n",
    "    prior_pdf = norm.pdf(x_range, mu_0, np.sqrt(sigma_0_squared))\n",
    "    axes[i, 0].plot(x_range, prior_pdf, 'k--', linewidth=1.5, alpha=0.5, label='Prior')\n",
    "    \n",
    "    axes[i, 0].set_title(f'Posterior Distribution (ÏƒÂ²_x = {sigma_x_squared})', fontsize=13)\n",
    "    axes[i, 0].set_xlabel('Î¼', fontsize=12)\n",
    "    axes[i, 0].set_ylabel('Density', fontsize=12)\n",
    "    axes[i, 0].legend(fontsize=10)\n",
    "    axes[i, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Predictive distribution\n",
    "    axes[i, 1].plot(x_range, predictive_pdf, label=f'Predictive (ÏƒÂ²_x={sigma_x_squared})', \n",
    "                   color='orange', linewidth=2)\n",
    "    axes[i, 1].axvline(predictive_mu, color='orange', linestyle='--', linewidth=1.5, \n",
    "                      label=f'Predictive mean = {predictive_mu:.2f}')\n",
    "    axes[i, 1].axvline(x_1, color='red', linestyle=':', linewidth=1.5, \n",
    "                      label=f'Observation: {x_1}')\n",
    "    \n",
    "    axes[i, 1].set_title(f'Predictive Distribution (ÏƒÂ²_x = {sigma_x_squared})', fontsize=13)\n",
    "    axes[i, 1].set_xlabel('x', fontsize=12)\n",
    "    axes[i, 1].set_ylabel('Density', fontsize=12)\n",
    "    axes[i, 1].legend(fontsize=10)\n",
    "    axes[i, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "**Effect of Likelihood Variance:**\n",
    "\n",
    "According to the update formula for posterior distributions:\n",
    "\n",
    "$$\\sigma_N^2 = \\left[ \\sigma_0^{-2} + N \\sigma_x^{-2} \\right]^{-1}$$\n",
    "\n",
    "A **smaller likelihood variance** ($\\sigma_x^2$) results in:\n",
    "- **Greater shrinkage**: The posterior concentrates more sharply around its peak\n",
    "- **Stronger data influence**: The posterior mean moves closer to the observed data\n",
    "- This is because the posterior mean is a precision-weighted average:\n",
    "  $$\\mu_N = \\frac{\\text{precision}_{\\text{prior}} \\times \\mu_0 + \\text{precision}_{\\text{data}} \\times \\bar{x}}{\\text{precision}_{\\text{prior}} + \\text{precision}_{\\text{data}}}$$\n",
    "  \n",
    "Where precision = $1/\\text{variance}$. Smaller $\\sigma_x^2$ means higher data precision, so the data gets more weight.\n",
    "\n",
    "**Key Observations:**\n",
    "- With $\\sigma_x^2 = 0.25$ (precise data): Posterior is narrow and close to $x_1 = 2$\n",
    "- With $\\sigma_x^2 = 4$ (noisy data): Posterior is wider and stays closer to prior mean (0)\n",
    "- The predictive distribution is always more dispersed than the posterior (adds $\\sigma_x^2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical comparison\n",
    "print(\"\\nðŸ“Š Numerical Comparison:\\n\")\n",
    "print(f\"{'Quantity':<30} {'ÏƒÂ²_x = 0.25':<20} {'ÏƒÂ²_x = 4':<20}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for sigma_x_squared in sigma_x_squared_values:\n",
    "    post_mu, _, pred_mu, _ = update_posterior(mu_0, sigma_0_squared, [x_1], sigma_x_squared, x_min, x_max)\n",
    "    \n",
    "    # Calculate variances\n",
    "    post_var = 1 / (1/sigma_0_squared + 1/sigma_x_squared)\n",
    "    pred_var = post_var + sigma_x_squared\n",
    "    \n",
    "    col_name = f\"ÏƒÂ²_x = {sigma_x_squared}\"\n",
    "    \n",
    "    if sigma_x_squared == 0.25:\n",
    "        print(f\"{'Posterior mean:':<30} {post_mu:<20.2f}\", end=\"\")\n",
    "    else:\n",
    "        post_mu_prev, _, _, _ = update_posterior(mu_0, sigma_0_squared, [x_1], 0.25, x_min, x_max)\n",
    "        print(f\"{post_mu:<20.2f}\")\n",
    "        \n",
    "        print(f\"{'Posterior variance:':<30} {1/(1/sigma_0_squared + 1/0.25):<20.2f} {post_var:<20.2f}\")\n",
    "        print(f\"{'Predictive variance:':<30} {1/(1/sigma_0_squared + 1/0.25) + 0.25:<20.2f} {pred_var:<20.2f}\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nâœ… Conclusion:\")\n",
    "print(\"  â€¢ Smaller ÏƒÂ²_x â†’ Posterior closer to data, more concentrated\")\n",
    "print(\"  â€¢ Larger ÏƒÂ²_x â†’ Posterior closer to prior, more dispersed\")\n",
    "print(\"  â€¢ Predictive always has larger variance than posterior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 1(c): Multiple Data Update\n",
    "\n",
    "Calculate and plot the posterior and predictive distributions given:\n",
    "$$(x_1, \\ldots, x_5) = (2.1, 2.5, 1.4, 2.2, 1.8)$$\n",
    "\n",
    "for $\\sigma_x^2 = 0.25$ and $\\sigma_x^2 = 4$.\n",
    "\n",
    "**Question**: How does this compare to the single observation case? Note that the average is 2.0 in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior parameters\n",
    "mu_0 = 0\n",
    "sigma_0_squared = 1\n",
    "\n",
    "# Observations\n",
    "x_values = np.array([2.1, 2.5, 1.4, 2.2, 1.8])\n",
    "N = len(x_values)\n",
    "sample_mean = np.mean(x_values)\n",
    "\n",
    "print(f\"Observations: {x_values}\")\n",
    "print(f\"Sample size: N = {N}\")\n",
    "print(f\"Sample mean: {sample_mean:.2f}\")\n",
    "print(f\"\\nNote: In part (b), we had 1 observation at xâ‚ = 2.0\")\n",
    "print(f\"      In part (c), we have 5 observations with mean = 2.0\")\n",
    "print(f\"      Both have the same average value!\\n\")\n",
    "\n",
    "# Likelihood variances\n",
    "sigma_x_squared_values = [0.25, 4]\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for i, sigma_x_squared in enumerate(sigma_x_squared_values):\n",
    "    # Update with 1 observation (from part b)\n",
    "    posterior_mu_1, posterior_pdf_1, predictive_mu_1, predictive_pdf_1 = update_posterior(\n",
    "        mu_0, sigma_0_squared, [x_1], sigma_x_squared, x_min, x_max\n",
    "    )\n",
    "    \n",
    "    # Update with 5 observations (part c)\n",
    "    posterior_mu, posterior_pdf, predictive_mu, predictive_pdf = update_posterior(\n",
    "        mu_0, sigma_0_squared, x_values, sigma_x_squared, x_min, x_max\n",
    "    )\n",
    "    \n",
    "    # Posterior distribution comparison\n",
    "    axes[i, 0].plot(x_range, posterior_pdf, label=f'Posterior (N=5, ÏƒÂ²_x={sigma_x_squared})', \n",
    "                   color='blue', linewidth=2)\n",
    "    axes[i, 0].plot(x_range, posterior_pdf_1, label=f'Posterior (N=1, ÏƒÂ²_x={sigma_x_squared})', \n",
    "                   color='blue', linewidth=2, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    axes[i, 0].axvline(posterior_mu, color='blue', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "    axes[i, 0].axvline(posterior_mu_1, color='blue', linestyle='--', linewidth=1, alpha=0.3)\n",
    "    axes[i, 0].axvline(sample_mean, color='red', linestyle=':', linewidth=2, \n",
    "                      label=f'Sample mean: {sample_mean:.2f}')\n",
    "    \n",
    "    axes[i, 0].set_title(f'Posterior Distribution (ÏƒÂ²_x = {sigma_x_squared})', fontsize=13)\n",
    "    axes[i, 0].set_xlabel('Î¼', fontsize=12)\n",
    "    axes[i, 0].set_ylabel('Density', fontsize=12)\n",
    "    axes[i, 0].legend(fontsize=9)\n",
    "    axes[i, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Predictive distribution comparison\n",
    "    axes[i, 1].plot(x_range, predictive_pdf, label=f'Predictive (N=5, ÏƒÂ²_x={sigma_x_squared})', \n",
    "                   color='orange', linewidth=2)\n",
    "    axes[i, 1].plot(x_range, predictive_pdf_1, label=f'Predictive (N=1, ÏƒÂ²_x={sigma_x_squared})', \n",
    "                   color='orange', linewidth=2, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    axes[i, 1].axvline(predictive_mu, color='orange', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "    axes[i, 1].axvline(predictive_mu_1, color='orange', linestyle='--', linewidth=1, alpha=0.3)\n",
    "    axes[i, 1].axvline(sample_mean, color='red', linestyle=':', linewidth=2, \n",
    "                      label=f'Sample mean: {sample_mean:.2f}')\n",
    "    \n",
    "    axes[i, 1].set_title(f'Predictive Distribution (ÏƒÂ²_x = {sigma_x_squared})', fontsize=13)\n",
    "    axes[i, 1].set_xlabel('x', fontsize=12)\n",
    "    axes[i, 1].set_ylabel('Density', fontsize=12)\n",
    "    axes[i, 1].legend(fontsize=9)\n",
    "    axes[i, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "**Comparison: 1 observation vs 5 observations (both with mean = 2.0)**\n",
    "\n",
    "The key insight is that the posterior variance depends on **both** the likelihood variance AND the number of observations:\n",
    "\n",
    "$$\\sigma_N^2 = \\left[ \\sigma_0^{-2} + \\frac{N}{\\sigma_x^2} \\right]^{-1}$$\n",
    "\n",
    "**Effect of increasing N (given fixed $\\sigma_x^2$):**\n",
    "\n",
    "1. **Posterior becomes more concentrated**: More observations â†’ smaller $\\sigma_N^2$\n",
    "2. **Posterior mean moves closer to sample mean**: Higher data precision\n",
    "3. **Predictive distribution also becomes more concentrated**: Smaller $\\sigma_N^2$ component\n",
    "\n",
    "**Why the difference between N=1 and N=5?**\n",
    "\n",
    "Even though both have the same mean (2.0), the **effective precision** of the data is different:\n",
    "- N=1: Data precision = $1/\\sigma_x^2$\n",
    "- N=5: Data precision = $5/\\sigma_x^2$ (5Ã— higher!)\n",
    "\n",
    "**Specific Observations:**\n",
    "- With $\\sigma_x^2 = 0.25$: Both posteriors are narrow, but N=5 is much sharper\n",
    "- With $\\sigma_x^2 = 4$: N=1 posterior stays close to prior; N=5 shifts significantly toward data\n",
    "- The posterior mean is the same for both (because sample mean is the same)\n",
    "- But the **confidence** (inverse of variance) is much higher with N=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical comparison\n",
    "print(\"\\nðŸ“Š Numerical Comparison: N=1 vs N=5\\n\")\n",
    "print(f\"{'Quantity':<35} {'N=1, ÏƒÂ²_x=0.25':<18} {'N=5, ÏƒÂ²_x=0.25':<18} {'N=1, ÏƒÂ²_x=4':<18} {'N=5, ÏƒÂ²_x=4':<18}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "results = {}\n",
    "for n_obs in [1, 5]:\n",
    "    obs = [x_1] if n_obs == 1 else x_values\n",
    "    for sigma_x_sq in sigma_x_squared_values:\n",
    "        post_mu, _, pred_mu, _ = update_posterior(mu_0, sigma_0_squared, obs, sigma_x_sq, x_min, x_max)\n",
    "        post_var = 1 / (1/sigma_0_squared + n_obs/sigma_x_sq)\n",
    "        pred_var = post_var + sigma_x_sq\n",
    "        results[(n_obs, sigma_x_sq)] = {\n",
    "            'post_mu': post_mu,\n",
    "            'post_var': post_var,\n",
    "            'post_std': np.sqrt(post_var),\n",
    "            'pred_var': pred_var,\n",
    "            'pred_std': np.sqrt(pred_var)\n",
    "        }\n",
    "\n",
    "print(f\"{'Posterior mean:':<35} {results[(1,0.25)]['post_mu']:<18.3f} {results[(5,0.25)]['post_mu']:<18.3f} \"\n",
    "      f\"{results[(1,4)]['post_mu']:<18.3f} {results[(5,4)]['post_mu']:<18.3f}\")\n",
    "print(f\"{'Posterior std dev:':<35} {results[(1,0.25)]['post_std']:<18.3f} {results[(5,0.25)]['post_std']:<18.3f} \"\n",
    "      f\"{results[(1,4)]['post_std']:<18.3f} {results[(5,4)]['post_std']:<18.3f}\")\n",
    "print(f\"{'Predictive std dev:':<35} {results[(1,0.25)]['pred_std']:<18.3f} {results[(5,0.25)]['pred_std']:<18.3f} \"\n",
    "      f\"{results[(1,4)]['pred_std']:<18.3f} {results[(5,4)]['pred_std']:<18.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*110)\n",
    "print(\"\\nâœ… Key Findings:\")\n",
    "print(f\"  â€¢ Posterior mean is similar for N=1 and N=5 (both â‰ˆ {sample_mean:.1f}) because sample mean is the same\")\n",
    "print(f\"  â€¢ Posterior std dev DECREASES with more observations (Nâ†‘ â†’ uncertaintyâ†“)\")\n",
    "print(f\"  â€¢ With ÏƒÂ²_x=0.25 (precise data): N=5 gives much sharper posterior than N=1\")\n",
    "print(f\"  â€¢ With ÏƒÂ²_x=4 (noisy data): Effect is less dramatic but still significant\")\n",
    "print(f\"  â€¢ More data = more confidence, even if the mean stays the same!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GenJAX Verification\n",
    "\n",
    "Let's verify our analytical results using GenJAX simulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify with GenJAX simulation\n",
    "print(\"ðŸ”¬ GenJAX Verification: Posterior Predictive Sampling\\n\")\n",
    "\n",
    "# Use the N=5, ÏƒÂ²_x=0.25 case\n",
    "sigma_x_squared = 0.25\n",
    "sigma_x = np.sqrt(sigma_x_squared)\n",
    "\n",
    "# Analytical results\n",
    "post_mu_analytical, _, pred_mu_analytical, _ = update_posterior(\n",
    "    mu_0, sigma_0_squared, x_values, sigma_x_squared, x_min, x_max\n",
    ")\n",
    "post_var_analytical = 1 / (1/sigma_0_squared + N/sigma_x_squared)\n",
    "post_std_analytical = np.sqrt(post_var_analytical)\n",
    "pred_var_analytical = post_var_analytical + sigma_x_squared\n",
    "pred_std_analytical = np.sqrt(pred_var_analytical)\n",
    "\n",
    "print(f\"Analytical results (N={N}, ÏƒÂ²_x={sigma_x_squared}):\")\n",
    "print(f\"  Posterior: N({post_mu_analytical:.3f}, {post_var_analytical:.3f})\")\n",
    "print(f\"  Predictive: N({pred_mu_analytical:.3f}, {pred_var_analytical:.3f})\")\n",
    "print()\n",
    "\n",
    "# GenJAX simulation\n",
    "key = random.PRNGKey(42)\n",
    "n_samples = 5000\n",
    "\n",
    "predictions = []\n",
    "for _ in range(n_samples):\n",
    "    key, subkey = random.split(key)\n",
    "    trace = simulate(posterior_predictive)(subkey, post_mu_analytical, post_std_analytical, sigma_x)\n",
    "    predictions.append(float(trace.get_retval()))\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "print(f\"GenJAX simulation results ({n_samples} samples):\")\n",
    "print(f\"  Predictive mean: {np.mean(predictions):.3f} (analytical: {pred_mu_analytical:.3f})\")\n",
    "print(f\"  Predictive std: {np.std(predictions):.3f} (analytical: {pred_std_analytical:.3f})\")\n",
    "print()\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Histogram of samples\n",
    "ax.hist(predictions, bins=50, density=True, alpha=0.6, color='skyblue', \n",
    "        edgecolor='black', label='GenJAX samples')\n",
    "\n",
    "# Analytical PDF\n",
    "x_plot = np.linspace(-2, 6, 1000)\n",
    "analytical_pdf = norm.pdf(x_plot, pred_mu_analytical, pred_std_analytical)\n",
    "ax.plot(x_plot, analytical_pdf, 'r-', linewidth=2, \n",
    "        label=f'Analytical: N({pred_mu_analytical:.2f}, {pred_var_analytical:.2f})')\n",
    "\n",
    "ax.axvline(np.mean(predictions), color='blue', linestyle='--', linewidth=1.5, \n",
    "          label=f'Sample mean: {np.mean(predictions):.2f}')\n",
    "ax.axvline(pred_mu_analytical, color='red', linestyle='--', linewidth=1.5, alpha=0.5,\n",
    "          label=f'Analytical mean: {pred_mu_analytical:.2f}')\n",
    "\n",
    "ax.set_xlabel('x (predicted observation)', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.set_title('GenJAX Posterior Predictive vs Analytical', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… GenJAX simulation matches analytical results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Insights from Problem 1:\n",
    "\n",
    "1. **Effect of Likelihood Variance ($\\sigma_x^2$)**:\n",
    "   - Smaller variance â†’ data more influential â†’ posterior closer to data\n",
    "   - Larger variance â†’ prior more influential â†’ posterior closer to prior\n",
    "   - This is captured by the **precision-weighted average** formula\n",
    "\n",
    "2. **Effect of Number of Observations (N)**:\n",
    "   - More observations â†’ higher effective data precision ($N/\\sigma_x^2$)\n",
    "   - Posterior becomes more concentrated (smaller variance)\n",
    "   - Posterior mean converges to sample mean as N â†’ âˆž\n",
    "\n",
    "3. **Predictive Distribution**:\n",
    "   - Always more dispersed than posterior (adds $\\sigma_x^2$)\n",
    "   - Accounts for both parameter uncertainty AND data variability\n",
    "   - Mean is same as posterior mean\n",
    "\n",
    "4. **Precision Interpretation**:\n",
    "   - Prior precision: $1/\\sigma_0^2$\n",
    "   - Data precision: $N/\\sigma_x^2$\n",
    "   - Posterior precision: sum of the two\n",
    "   - Higher precision = more certainty\n",
    "\n",
    "### Mathematical Framework:\n",
    "\n",
    "**Posterior**:\n",
    "$$\\mu_N = \\frac{\\frac{1}{\\sigma_0^2} \\mu_0 + \\frac{N}{\\sigma_x^2} \\bar{x}}{\\frac{1}{\\sigma_0^2} + \\frac{N}{\\sigma_x^2}}, \\quad \\sigma_N^2 = \\frac{1}{\\frac{1}{\\sigma_0^2} + \\frac{N}{\\sigma_x^2}}$$\n",
    "\n",
    "**Predictive**:\n",
    "$$x_{N+1} \\sim \\mathcal{N}(\\mu_N, \\sigma_N^2 + \\sigma_x^2)$$\n",
    "\n",
    "This elegant framework allows us to:\n",
    "- Update beliefs sequentially as data arrives\n",
    "- Balance prior knowledge with observed data\n",
    "- Quantify uncertainty about future observations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
