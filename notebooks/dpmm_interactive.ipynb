{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Interactive Dirichlet Process Mixture Model Explorer\n\nThis notebook demonstrates **Bayesian inference for Dirichlet Process Mixture Models (DPMM)** using JAX.\n\n**What you'll learn:**\n- How DPMMs automatically discover the number of clusters in data\n- The stick-breaking construction for infinite mixture models\n- How the concentration parameter Î± controls cluster formation\n- Posterior inference using importance resampling\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/josephausterweil/probintro/blob/amplify/notebooks/dpmm_interactive.ipynb)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\nFirst, let's install the required packages if running on Google Colab:\n\n**Note**: After running the installation cell below, you may need to restart the runtime (Runtime â†’ Restart runtime) before proceeding."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if running on Colab\ntry:\n    import google.colab\n    IN_COLAB = True\nexcept:\n    IN_COLAB = False\n\nif IN_COLAB:\n    print(\"Running on Google Colab - installing dependencies...\")\n    # Install compatible versions: numpy 2.0.x works with most Colab packages\n    # Upgrade JAX to satisfy flax and orbax requirements\n    !pip install -q --upgrade \"jax>=0.6.0\" \"jaxlib>=0.6.0\" \"numpy>=2.0,<2.1\" scipy ipywidgets\n    print(\"âœ“ Dependencies installed\")\n    print(\"âš ï¸  Please restart runtime (Runtime â†’ Restart runtime) before continuing\")\nelse:\n    print(\"Running locally\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import jax\nimport jax.numpy as jnp\nimport jax.random as random\nfrom jax.scipy.stats import norm, beta, dirichlet\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import gaussian_kde\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\n# Enable widgets in Colab\ntry:\n    import google.colab\n    from google.colab import output\n    output.enable_custom_widget_manager()\nexcept:\n    pass\n\n# Set random seed for reproducibility\nkey = random.PRNGKey(42)\n\nprint(\"âœ“ Imports successful\")\nprint(f\"JAX version: {jax.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dirichlet Process Mixture Model\n",
    "\n",
    "A DPMM is an infinite mixture model that can automatically discover the number of clusters in data.\n",
    "\n",
    "### Model Structure\n",
    "\n",
    "For each cluster $k = 1, 2, \\ldots, K_{\\text{max}}$:\n",
    "1. Draw cluster center: $\\mu_k \\sim \\text{Normal}(\\mu_0, \\sigma_0^2)$\n",
    "2. Draw stick-breaking weight: $\\beta_k \\sim \\text{Beta}(1, \\alpha)$\n",
    "3. Compute mixture probability: $\\pi_k = \\beta_k \\prod_{j<k}(1-\\beta_j)$\n",
    "\n",
    "Then normalize to get $\\theta \\sim \\text{Dirichlet}(\\pi_1, \\ldots, \\pi_K)$\n",
    "\n",
    "For each observation $i = 1, \\ldots, N$:\n",
    "1. Draw cluster assignment: $z_i \\sim \\text{Categorical}(\\theta)$\n",
    "2. Draw observation: $x_i \\sim \\text{Normal}(\\mu_{z_i}, \\sigma_x^2)$\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- **Î± (concentration)**: Controls how spread out the mixture is\n",
    "  - Small Î± â†’ Few active clusters\n",
    "  - Large Î± â†’ Many clusters with similar weights\n",
    "- **$K_{\\text{max}}$**: Truncation level (upper bound on clusters)\n",
    "- **$\\mu_0, \\sigma_0$**: Prior on cluster centers\n",
    "- **$\\sigma_x$**: Observation noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Let's implement the DPMM using pure JAX (without GenJAX for now, to keep it simple and debuggable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stick_breaking_weights(key, alpha, K):\n",
    "    \"\"\"Generate stick-breaking weights for DPMM.\n",
    "    \n",
    "    Args:\n",
    "        key: JAX random key\n",
    "        alpha: Concentration parameter\n",
    "        K: Number of components\n",
    "    \n",
    "    Returns:\n",
    "        pis: Mixture probabilities (sum to ~1)\n",
    "    \"\"\"\n",
    "    betas = random.beta(key, 1.0, alpha, shape=(K,))\n",
    "    \n",
    "    # Stick-breaking: Ï€_k = Î²_k * âˆ_{j<k}(1-Î²_j)\n",
    "    pis = jnp.zeros(K)\n",
    "    remaining_stick = 1.0\n",
    "    \n",
    "    for k in range(K):\n",
    "        pis = pis.at[k].set(betas[k] * remaining_stick)\n",
    "        remaining_stick *= (1.0 - betas[k])\n",
    "    \n",
    "    # Ensure no zero probabilities (for numerical stability)\n",
    "    pis = jnp.maximum(pis, 1e-6)\n",
    "    pis = pis / jnp.sum(pis)  # Renormalize\n",
    "    \n",
    "    return pis\n",
    "\n",
    "def sample_dpmm_prior(key, alpha, K, mu0, sig0, sigx, N):\n",
    "    \"\"\"Sample from DPMM prior.\n",
    "    \n",
    "    Returns:\n",
    "        mus: Cluster centers\n",
    "        thetas: Mixture probabilities\n",
    "        zs: Cluster assignments\n",
    "        xs: Generated data\n",
    "    \"\"\"\n",
    "    keys = random.split(key, 4)\n",
    "    \n",
    "    # Sample cluster centers\n",
    "    mus = random.normal(keys[0], shape=(K,)) * sig0 + mu0\n",
    "    \n",
    "    # Generate stick-breaking weights\n",
    "    pis = stick_breaking_weights(keys[1], alpha, K)\n",
    "    \n",
    "    # Sample from Dirichlet to get final mixture weights\n",
    "    # (This adds additional variability on top of stick-breaking)\n",
    "    thetas = random.dirichlet(keys[2], pis * 100)  # Scale up for concentration\n",
    "    \n",
    "    # Sample cluster assignments\n",
    "    zs = random.categorical(keys[3], jnp.log(thetas), shape=(N,))\n",
    "    \n",
    "    # Sample observations\n",
    "    key_xs = random.split(keys[3], N)\n",
    "    xs = jnp.array([random.normal(key_xs[i]) * sigx + mus[zs[i]] for i in range(N)])\n",
    "    \n",
    "    return mus, thetas, zs, xs\n",
    "\n",
    "def log_likelihood(xs, mus, thetas, sigx, K):\n",
    "    \"\"\"Compute log likelihood of data given parameters.\"\"\"\n",
    "    N = len(xs)\n",
    "    ll = 0.0\n",
    "    \n",
    "    for i in range(N):\n",
    "        # p(x_i | mus, thetas) = âˆ‘_k Î¸_k * N(x_i | Î¼_k, Ïƒ_xÂ²)\n",
    "        component_lls = jnp.array([norm.logpdf(xs[i], mus[k], sigx) for k in range(K)])\n",
    "        ll += jax.scipy.special.logsumexp(jnp.log(thetas) + component_lls)\n",
    "    \n",
    "    return ll\n",
    "\n",
    "def importance_sampling(key, obs_xs, alpha, K, mu0, sig0, sigx, num_samples):\n",
    "    \"\"\"Perform importance sampling for posterior inference.\n",
    "    \n",
    "    Args:\n",
    "        obs_xs: Observed data\n",
    "        alpha, K, mu0, sig0, sigx: Model parameters\n",
    "        num_samples: Number of importance samples\n",
    "    \n",
    "    Returns:\n",
    "        mus_samples: Posterior samples of cluster centers\n",
    "        thetas_samples: Posterior samples of mixture weights\n",
    "        weights: Importance weights (normalized)\n",
    "    \"\"\"\n",
    "    N = len(obs_xs)\n",
    "    keys = random.split(key, num_samples)\n",
    "    \n",
    "    # Sample from prior\n",
    "    all_mus = []\n",
    "    all_thetas = []\n",
    "    log_weights = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        mus, thetas, _, _ = sample_dpmm_prior(keys[i], alpha, K, mu0, sig0, sigx, N)\n",
    "        all_mus.append(mus)\n",
    "        all_thetas.append(thetas)\n",
    "        \n",
    "        # Compute likelihood as importance weight\n",
    "        ll = log_likelihood(obs_xs, mus, thetas, sigx, K)\n",
    "        log_weights.append(ll)\n",
    "    \n",
    "    # Normalize weights\n",
    "    log_weights = jnp.array(log_weights)\n",
    "    log_weights = log_weights - jax.scipy.special.logsumexp(log_weights)\n",
    "    weights = jnp.exp(log_weights)\n",
    "    \n",
    "    return jnp.array(all_mus), jnp.array(all_thetas), weights\n",
    "\n",
    "def importance_resampling(key, mus_samples, thetas_samples, weights, num_resamples):\n",
    "    \"\"\"Resample from importance samples to get posterior samples.\"\"\"\n",
    "    indices = random.choice(key, len(weights), shape=(num_resamples,), p=weights)\n",
    "    return mus_samples[indices], thetas_samples[indices]\n",
    "\n",
    "print(\"âœ“ DPMM functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Exploration\n",
    "\n",
    "Now let's create an interactive widget to explore how DPMMs work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default observed data (3 clear clusters)\n",
    "default_data = np.array([-10.4, -10., -9.4, -10.1, -9.9, 0., 9.5, 9.9, 10., 10.1, 10.5])\n",
    "\n",
    "# Create widgets\n",
    "alpha_slider = widgets.FloatSlider(\n",
    "    value=2.0,\n",
    "    min=0.1,\n",
    "    max=10.0,\n",
    "    step=0.1,\n",
    "    description='Î± (concentration):',\n",
    "    style={'description_width': '150px'},\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "K_slider = widgets.IntSlider(\n",
    "    value=10,\n",
    "    min=3,\n",
    "    max=20,\n",
    "    step=1,\n",
    "    description='K_max (clusters):',\n",
    "    style={'description_width': '150px'},\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "samples_slider = widgets.IntSlider(\n",
    "    value=500,\n",
    "    min=100,\n",
    "    max=2000,\n",
    "    step=100,\n",
    "    description='Samples:',\n",
    "    style={'description_width': '150px'},\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "data_text = widgets.Textarea(\n",
    "    value=', '.join(map(str, default_data)),\n",
    "    description='Data points:',\n",
    "    style={'description_width': '150px'},\n",
    "    layout=widgets.Layout(width='500px', height='80px')\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(\n",
    "    description='Run Inference',\n",
    "    button_style='success',\n",
    "    icon='play'\n",
    ")\n",
    "\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "def parse_data(data_str):\n",
    "    \"\"\"Parse comma-separated data string.\"\"\"\n",
    "    try:\n",
    "        return jnp.array([float(x.strip()) for x in data_str.split(',')])\n",
    "    except:\n",
    "        return default_data\n",
    "\n",
    "def run_inference(button):\n",
    "    \"\"\"Run DPMM inference and visualize results.\"\"\"\n",
    "    with output_widget:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Parse parameters\n",
    "        alpha = alpha_slider.value\n",
    "        K = K_slider.value\n",
    "        num_samples = samples_slider.value\n",
    "        obs_xs = parse_data(data_text.value)\n",
    "        N = len(obs_xs)\n",
    "        \n",
    "        print(f\"Running inference with Î±={alpha:.1f}, K_max={K}, {num_samples} samples...\")\n",
    "        print(f\"Data: {N} observations\\n\")\n",
    "        \n",
    "        # Fixed hyperparameters\n",
    "        mu0 = 0.0\n",
    "        sig0 = 4.0\n",
    "        sigx = 0.05\n",
    "        \n",
    "        # Run importance sampling\n",
    "        key_local = random.PRNGKey(np.random.randint(0, 10000))\n",
    "        mus_samples, thetas_samples, weights = importance_sampling(\n",
    "            key_local, obs_xs, alpha, K, mu0, sig0, sigx, num_samples\n",
    "        )\n",
    "        \n",
    "        # Resample to get posterior samples\n",
    "        key_resample = random.PRNGKey(np.random.randint(0, 10000))\n",
    "        mus_post, thetas_post = importance_resampling(\n",
    "            key_resample, mus_samples, thetas_samples, weights, num_samples\n",
    "        )\n",
    "        \n",
    "        # Generate posterior predictive samples\n",
    "        key_pred = random.PRNGKey(np.random.randint(0, 10000))\n",
    "        pred_samples = []\n",
    "        for i in range(num_samples):\n",
    "            # Sample cluster assignment from posterior\n",
    "            z = random.categorical(key_pred, jnp.log(thetas_post[i]))\n",
    "            # Sample from that cluster\n",
    "            x = random.normal(key_pred) * sigx + mus_post[i, z]\n",
    "            pred_samples.append(x)\n",
    "            key_pred = random.split(key_pred, 1)[0]\n",
    "        \n",
    "        pred_samples = jnp.array(pred_samples)\n",
    "        \n",
    "        # Flatten mus and thetas for visualization\n",
    "        mus_flat = mus_post.flatten()\n",
    "        thetas_flat = thetas_post.flatten()\n",
    "        \n",
    "        print(\"âœ“ Inference complete\\n\")\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # Histogram of observed data\n",
    "        ax.hist(obs_xs, bins=int(np.sqrt(N)), density=True, alpha=0.5, \n",
    "                color='gray', label='Observed data', edgecolor='black')\n",
    "        \n",
    "        # Posterior predictive density\n",
    "        x_range = np.linspace(float(obs_xs.min()) - 2, float(obs_xs.max()) + 2, 200)\n",
    "        kde_pred = gaussian_kde(np.array(pred_samples))\n",
    "        ax.plot(x_range, kde_pred(x_range), 'b-', linewidth=3, \n",
    "                label='Posterior predictive p(xÌ‚|data)', alpha=0.8)\n",
    "        \n",
    "        # Posterior of cluster centers (weighted by mixture probabilities)\n",
    "        kde_mus = gaussian_kde(np.array(mus_flat), weights=np.array(thetas_flat))\n",
    "        ax.plot(x_range, kde_mus(x_range), 'r-', linewidth=3,\n",
    "                label='Posterior p(Î¼|data)', alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('x', fontsize=12)\n",
    "        ax.set_ylabel('Density', fontsize=12)\n",
    "        ax.set_title(f'DPMM Inference (Î±={alpha:.1f}, K_max={K})', fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print statistics\n",
    "        effective_clusters = jnp.sum(thetas_post.mean(axis=0) > 0.01)\n",
    "        print(f\"\\nEstimated active clusters: {int(effective_clusters)} (with Î¸ > 0.01)\")\n",
    "        print(f\"Top 3 cluster weights: {sorted(thetas_post.mean(axis=0), reverse=True)[:3]}\")\n",
    "\n",
    "# Connect button to function\n",
    "run_button.on_click(run_inference)\n",
    "\n",
    "# Display widgets\n",
    "print(\"Interactive DPMM Explorer\")\n",
    "print(\"=\" * 50)\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Parameters</h3>\"),\n",
    "    alpha_slider,\n",
    "    K_slider,\n",
    "    samples_slider,\n",
    "    widgets.HTML(\"<h3>Data</h3>\"),\n",
    "    widgets.HTML(\"<p>Enter comma-separated values (e.g., -10, -9.5, 0, 9.5, 10):</p>\"),\n",
    "    data_text,\n",
    "    run_button,\n",
    "    output_widget\n",
    "]))\n",
    "\n",
    "print(\"\\nðŸ’¡ Try changing Î± to see how it affects clustering!\")\n",
    "print(\"   â€¢ Small Î± (0.1-1.0): Favors fewer clusters\")\n",
    "print(\"   â€¢ Large Î± (5.0-10.0): Allows more clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these experiments:\n",
    "\n",
    "1. **Effect of Î±**:\n",
    "   - Set Î±=0.5 and run inference. How many clusters are active?\n",
    "   - Set Î±=5.0 and run inference. How does the posterior change?\n",
    "\n",
    "2. **Different data**:\n",
    "   - Two clusters: `-5, -4.8, -5.2, 5, 4.8, 5.2`\n",
    "   - Four clusters: `-10, -9, 0, 1, 10, 11, 20, 21`\n",
    "   - Single cluster: `0, 0.1, -0.1, 0.2, -0.2`\n",
    "\n",
    "3. **Truncation level**:\n",
    "   - Use data with 2 clear clusters but set K_max=20\n",
    "   - What happens to the unused clusters?\n",
    "\n",
    "4. **Sample size**:\n",
    "   - Run with 100 samples vs 1000 samples\n",
    "   - How does it affect the smoothness of posteriors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **Automatic discovery**: DPMMs automatically discover the number of clusters without specifying K in advance\n",
    "\n",
    "2. **Concentration parameter**: Î± controls the \"richness\" of the mixture:\n",
    "   - Small Î± â†’ Few large clusters (concentrated)\n",
    "   - Large Î± â†’ Many small clusters (dispersed)\n",
    "\n",
    "3. **Posterior uncertainty**: The red curve shows uncertainty about cluster locations, not just point estimates\n",
    "\n",
    "4. **Predictive distribution**: The blue curve shows what new data might look like, accounting for both parameter uncertainty and cluster structure\n",
    "\n",
    "## Connection to Tutorial\n",
    "\n",
    "This notebook demonstrates the concepts from **Chapter 6: Dirichlet Process Mixture Models** in the tutorial. See the tutorial for:\n",
    "- Detailed explanation of stick-breaking\n",
    "- Chinese Restaurant Process interpretation\n",
    "- Comparison with fixed-K GMMs\n",
    "- GenJAX implementation details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}