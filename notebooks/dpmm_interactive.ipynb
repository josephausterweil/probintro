{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Interactive Dirichlet Process Mixture Model Explorer (GenJAX)\n\nThis notebook demonstrates **Bayesian inference for Dirichlet Process Mixture Models (DPMM)** using **GenJAX** - Gen's JAX backend.\n\n**What you'll learn:**\n- How to define generative models with GenJAX's `@gen` decorator\n- How DPMMs automatically discover the number of clusters in data\n- The stick-breaking construction for infinite mixture models\n- How the concentration parameter Œ± controls cluster formation\n- Posterior inference using importance resampling with GenJAX\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/josephausterweil/probintro/blob/amplify/notebooks/dpmm_interactive.ipynb)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\nFirst, let's install the required packages if running on Google Colab:\n\n**Note**: After running the installation cell below, you may need to restart the runtime (Runtime ‚Üí Restart runtime) before proceeding."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if running on Colab\ntry:\n    import google.colab\n    IN_COLAB = True\nexcept:\n    IN_COLAB = False\n\nif IN_COLAB:\n    print(\"Running on Google Colab - installing dependencies...\")\n    # Install GenJAX with compatible versions\n    # GenJAX requires specific JAX and numpy versions\n    !pip install -q --upgrade \"jax>=0.4.20,<0.5\" \"jaxlib>=0.4.20,<0.5\" \"numpy>=1.22,<2.0\" genjax scipy ipywidgets\n    print(\"‚úì Dependencies installed\")\n    print(\"‚ö†Ô∏è  Please restart runtime (Runtime ‚Üí Restart runtime) before continuing\")\nelse:\n    print(\"Running locally\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Force JAX to use CPU to avoid CUDA plugin conflicts\nimport os\nos.environ['JAX_PLATFORMS'] = 'cpu'\n\nimport jax\nimport jax.numpy as jnp\nimport jax.random as random\nfrom jax.scipy.stats import norm\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import gaussian_kde\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\n# Import GenJAX\nimport genjax\nfrom genjax import gen\n\n# Enable widgets in Colab\ntry:\n    import google.colab\n    from google.colab import output\n    output.enable_custom_widget_manager()\nexcept:\n    pass\n\n# Set random seed for reproducibility\nkey = random.PRNGKey(42)\n\nprint(\"‚úì Imports successful (CPU mode)\")\nprint(f\"JAX version: {jax.__version__}\")\nprint(f\"GenJAX version: {genjax.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dirichlet Process Mixture Model\n",
    "\n",
    "A DPMM is an infinite mixture model that can automatically discover the number of clusters in data.\n",
    "\n",
    "### Model Structure\n",
    "\n",
    "For each cluster $k = 1, 2, \\ldots, K_{\\text{max}}$:\n",
    "1. Draw cluster center: $\\mu_k \\sim \\text{Normal}(\\mu_0, \\sigma_0^2)$\n",
    "2. Draw stick-breaking weight: $\\beta_k \\sim \\text{Beta}(1, \\alpha)$\n",
    "3. Compute mixture probability: $\\pi_k = \\beta_k \\prod_{j<k}(1-\\beta_j)$\n",
    "\n",
    "Then normalize to get $\\theta \\sim \\text{Dirichlet}(\\pi_1, \\ldots, \\pi_K)$\n",
    "\n",
    "For each observation $i = 1, \\ldots, N$:\n",
    "1. Draw cluster assignment: $z_i \\sim \\text{Categorical}(\\theta)$\n",
    "2. Draw observation: $x_i \\sim \\text{Normal}(\\mu_{z_i}, \\sigma_x^2)$\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- **Œ± (concentration)**: Controls how spread out the mixture is\n",
    "  - Small Œ± ‚Üí Few active clusters\n",
    "  - Large Œ± ‚Üí Many clusters with similar weights\n",
    "- **$K_{\\text{max}}$**: Truncation level (upper bound on clusters)\n",
    "- **$\\mu_0, \\sigma_0$**: Prior on cluster centers\n",
    "- **$\\sigma_x$**: Observation noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## GenJAX Implementation\n\nLet's implement the DPMM using **proper GenJAX patterns**:\n\n### Key GenJAX Concepts\n\n1. **`@gen` decorator**: Marks a function as a generative model\n2. **`@ \"address\"` syntax**: Every random choice needs a unique string address\n3. **`Target` + `ChoiceMap`**: Define posterior by conditioning on observations\n4. **`ImportanceK`**: GenJAX's importance sampling algorithm\n5. **`jax.vmap`**: Parallel sampling for efficiency\n\n### Why GenJAX?\n\n- **Programmable inference**: Mix different inference methods (importance sampling, MCMC, variational)\n- **JAX integration**: JIT compilation and automatic differentiation\n- **Composable models**: Build complex models from simple generative functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from genjax import normal, beta as beta_dist, dirichlet, categorical\nfrom genjax.inference.smc import ImportanceK\nfrom genjax import Target, ChoiceMap\n\ndef make_dpmm_model(K, N):\n    \"\"\"Factory function to create a DPMM model with fixed K and N.\n    \n    This is necessary because GenJAX can't trace functions with dynamic loops.\n    We fix K and N at model creation time as closure variables.\n    \"\"\"\n    @gen\n    def dpmm_model(alpha, mu0, sig0, sigx):\n        \"\"\"DPMM generative model using stick-breaking construction with GenJAX.\n        \n        K and N are fixed at model creation (closure variables).\n        alpha, mu0, sig0, sigx are traced parameters.\n        \"\"\"\n        # Sample cluster centers (K is fixed, so this loop is unrollable)\n        mus = []\n        for k in range(K):\n            mu_k = normal(mu0, sig0) @ f\"mu_{k}\"\n            mus.append(mu_k)\n        \n        # Stick-breaking process for mixture weights\n        betas = []\n        pis = []\n        remaining_stick = 1.0\n        for k in range(K):\n            beta_k = beta_dist(1.0, alpha) @ f\"beta_{k}\"\n            betas.append(beta_k)\n            pi_k = beta_k * remaining_stick\n            pis.append(pi_k)\n            remaining_stick *= (1.0 - beta_k)\n        \n        # Normalize mixture weights\n        pis_array = jnp.array(pis)\n        pis_array = jnp.maximum(pis_array, 1e-6)\n        pis_array = pis_array / jnp.sum(pis_array)\n        \n        # Sample from Dirichlet to get final mixture weights\n        thetas = dirichlet(pis_array * 100.0) @ \"thetas\"\n        \n        # Sample cluster assignments and observations (N is fixed)\n        zs = []\n        xs = []\n        mus_array = jnp.array(mus)\n        for i in range(N):\n            z_i = categorical(thetas) @ f\"z_{i}\"\n            x_i = normal(mus_array[z_i], sigx) @ f\"x_{i}\"\n            zs.append(z_i)\n            xs.append(x_i)\n        \n        return {\n            'mus': mus_array,\n            'thetas': thetas,\n            'zs': jnp.array(zs),\n            'xs': jnp.array(xs),\n            'pis': pis_array,\n            'betas': jnp.array(betas)\n        }\n    \n    return dpmm_model\n\ndef importance_sampling_genjax(key, obs_xs, alpha, K, mu0, sig0, sigx, num_samples):\n    \"\"\"Perform importance sampling using GenJAX's ImportanceK algorithm.\"\"\"\n    N = len(obs_xs)\n    \n    # Create the model with fixed K and N\n    dpmm_model = make_dpmm_model(K, N)\n    \n    # Create observations ChoiceMap (condition on observed data)\n    obs_dict = {f\"x_{i}\": float(obs_xs[i]) for i in range(N)}\n    observations = ChoiceMap.d(obs_dict)\n    \n    # Create target distribution (posterior)\n    posterior_target = Target(\n        dpmm_model,\n        (alpha, mu0, sig0, sigx),\n        observations\n    )\n    \n    # Run importance sampling with SMALL k_particles\n    # Note: k_particles is the number of particles PER CALL to random_weighted\n    # Total particles = k_particles * num_samples\n    # Using k_particles=10 means 10 * 50 = 500 particles total (reasonable)\n    alg = ImportanceK(posterior_target, k_particles=10)\n    \n    # Generate samples sequentially\n    all_mus = []\n    all_thetas = []\n    all_zs = []\n    log_weights = []\n    \n    keys = random.split(key, num_samples)\n    \n    print(f\"   ‚è≥ Generating {num_samples} samples (10 particles each)...\")\n    for i in range(num_samples):\n        if i % 10 == 0:\n            print(f\"      Sample {i}/{num_samples}...\")\n        \n        # random_weighted returns (log_weight, choice_map)\n        log_weight, choice_map = alg.random_weighted(keys[i], posterior_target)\n        \n        # Extract values from the choice map\n        mus_i = jnp.array([choice_map[f\"mu_{k}\"] for k in range(K)])\n        thetas_i = choice_map[\"thetas\"]\n        zs_i = jnp.array([choice_map[f\"z_{j}\"] for j in range(N)])\n        \n        all_mus.append(mus_i)\n        all_thetas.append(thetas_i)\n        all_zs.append(zs_i)\n        log_weights.append(float(log_weight))\n    \n    # Compute normalized importance weights\n    log_weights = jnp.array(log_weights)\n    log_weights = log_weights - jax.scipy.special.logsumexp(log_weights)\n    weights = jnp.exp(log_weights)\n    \n    return jnp.array(all_mus), jnp.array(all_thetas), jnp.array(all_zs), weights\n\ndef importance_resampling_genjax(key, mus_samples, thetas_samples, weights, num_resamples):\n    \"\"\"Resample from importance samples to get posterior samples.\"\"\"\n    indices = random.choice(key, len(weights), shape=(num_resamples,), p=weights)\n    return mus_samples[indices], thetas_samples[indices]\n\nprint(\"‚úì GenJAX DPMM model factory defined\")\nprint(\"‚úì GenJAX importance sampling functions defined\")\nprint(\"‚ÑπÔ∏è  Using k_particles=10 per sample (avoids timeout)\")\nprint(\"‚ÑπÔ∏è  Progress messages show sampling progress\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Interactive Exploration\n\nNow let's create an interactive function to explore how DPMMs work! The cell below defines `run_dpmm_inference()` which you can call with different parameters."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Default observed data (3 clear clusters)\ndefault_data = np.array([-10.4, -10., -9.4, -10.1, -9.9, 0., 9.5, 9.9, 10., 10.1, 10.5])\n\ndef run_dpmm_inference(alpha=2.0, K_max=10, num_samples=500, data_str=None):\n    \"\"\"Run DPMM inference with GenJAX and visualize results.\n    \n    Parameters:\n    -----------\n    alpha : float\n        Concentration parameter (0.1 to 10.0)\n    K_max : int\n        Maximum number of clusters (3 to 20)\n    num_samples : int\n        Number of importance samples (100 to 2000)\n    data_str : str, optional\n        Comma-separated data points. If None, uses default 3-cluster data.\n    \"\"\"\n    # Parse data\n    if data_str is None or data_str == \"\":\n        obs_xs = jnp.array(default_data)\n    else:\n        try:\n            obs_xs = jnp.array([float(x.strip()) for x in data_str.split(',')])\n        except:\n            print(\"‚ö†Ô∏è  Error parsing data, using default\")\n            obs_xs = jnp.array(default_data)\n    \n    N = len(obs_xs)\n    print(f\"üîÑ Running GenJAX DPMM inference with Œ±={alpha:.1f}, K_max={K_max}, {num_samples} samples...\")\n    print(f\"   Data: {N} observations\")\n    \n    # Fixed hyperparameters\n    mu0 = 0.0\n    sig0 = 4.0\n    sigx = 0.05\n    \n    # Run GenJAX importance sampling\n    print(f\"   ‚è≥ Sampling from GenJAX model with ImportanceK...\")\n    key_local = random.PRNGKey(np.random.randint(0, 10000))\n    mus_samples, thetas_samples, zs_samples, weights = importance_sampling_genjax(\n        key_local, obs_xs, alpha, K_max, mu0, sig0, sigx, num_samples\n    )\n    \n    # Resample to get posterior samples\n    print(f\"   ‚è≥ Resampling to get posterior...\")\n    key_resample = random.PRNGKey(np.random.randint(0, 10000))\n    mus_post, thetas_post = importance_resampling_genjax(\n        key_resample, mus_samples, thetas_samples, weights, num_samples\n    )\n    \n    # Generate posterior predictive samples\n    print(f\"   ‚è≥ Generating posterior predictive samples...\")\n    key_pred = random.PRNGKey(np.random.randint(0, 10000))\n    pred_samples = []\n    for i in range(num_samples):\n        z = random.categorical(key_pred, jnp.log(thetas_post[i]))\n        x = random.normal(key_pred) * sigx + mus_post[i, z]\n        pred_samples.append(x)\n        key_pred = random.split(key_pred, 1)[0]\n    \n    pred_samples = jnp.array(pred_samples)\n    \n    # Flatten for visualization\n    mus_flat = mus_post.flatten()\n    thetas_flat = thetas_post.flatten()\n    \n    print(\"   ‚úÖ Inference complete!\\n\")\n    \n    # Create visualization\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    # Histogram of observed data\n    ax.hist(obs_xs, bins=max(3, int(np.sqrt(N))), density=True, alpha=0.5, \n            color='gray', label='Observed data', edgecolor='black')\n    \n    # Posterior predictive density\n    x_range = np.linspace(float(obs_xs.min()) - 2, float(obs_xs.max()) + 2, 200)\n    kde_pred = gaussian_kde(np.array(pred_samples))\n    ax.plot(x_range, kde_pred(x_range), 'b-', linewidth=3, \n            label='Posterior predictive p(xÃÇ|data)', alpha=0.8)\n    \n    # Posterior of cluster centers (weighted by mixture probabilities)\n    kde_mus = gaussian_kde(np.array(mus_flat), weights=np.array(thetas_flat))\n    ax.plot(x_range, kde_mus(x_range), 'r-', linewidth=3,\n            label='Posterior p(Œº|data)', alpha=0.8)\n    \n    ax.set_xlabel('x', fontsize=12)\n    ax.set_ylabel('Density', fontsize=12)\n    ax.set_title(f'GenJAX DPMM Inference (Œ±={alpha:.1f}, K_max={K_max})', fontsize=14, fontweight='bold')\n    ax.legend(fontsize=11)\n    ax.grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print statistics\n    effective_clusters = jnp.sum(thetas_post.mean(axis=0) > 0.01)\n    print(f\"üìä Estimated active clusters: {int(effective_clusters)} (with Œ∏ > 0.01)\")\n    print(f\"üìä Top 3 cluster weights: {[f'{w:.3f}' for w in sorted(thetas_post.mean(axis=0), reverse=True)[:3]]}\")\n\nprint(\"‚úì GenJAX DPMM inference function defined\")"
  },
  {
   "cell_type": "markdown",
   "source": "# Interactive widget interface\nfrom ipywidgets import interact, FloatSlider, IntSlider, Text\n\nprint(\"üéõÔ∏è  Interactive DPMM Explorer\")\nprint(\"=\"*70)\nprint(\"Adjust the sliders below and the visualization will update automatically!\\n\")\n\ninteract(\n    run_dpmm_inference,\n    alpha=FloatSlider(value=2.0, min=0.1, max=10.0, step=0.1, description='Œ± (concentration):'),\n    K_max=IntSlider(value=10, min=3, max=20, step=1, description='K_max (clusters):'),\n    num_samples=IntSlider(value=500, min=100, max=2000, step=100, description='Samples:'),\n    data_str=Text(value='', description='Custom data:', placeholder='Leave empty for default, or enter: -5, -4.8, 5, 4.8')\n);",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercises\n\nUse the interactive sliders above or call the function directly to try these experiments:\n\n1. **Effect of Œ±**:\n   - Set Œ±=0.5: How many clusters are active?\n   - Set Œ±=5.0: How does the posterior change?\n\n2. **Different data** (enter in Custom data field or call function):\n   - Two clusters: `-5, -4.8, -5.2, 5, 4.8, 5.2`\n   - Four clusters: `-10, -9, 0, 1, 10, 11, 20, 21`\n   - Single cluster: `0, 0.1, -0.1, 0.2, -0.2`\n\n3. **Truncation level**:\n   - Use default data (3 clusters) but set K_max=20\n   - What happens to the unused clusters?\n\n4. **Sample size**:\n   - Run with 100 samples vs 1000 samples\n   - How does it affect the smoothness of posteriors?"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **Automatic discovery**: DPMMs automatically discover the number of clusters without specifying K in advance\n",
    "\n",
    "2. **Concentration parameter**: Œ± controls the \"richness\" of the mixture:\n",
    "   - Small Œ± ‚Üí Few large clusters (concentrated)\n",
    "   - Large Œ± ‚Üí Many small clusters (dispersed)\n",
    "\n",
    "3. **Posterior uncertainty**: The red curve shows uncertainty about cluster locations, not just point estimates\n",
    "\n",
    "4. **Predictive distribution**: The blue curve shows what new data might look like, accounting for both parameter uncertainty and cluster structure\n",
    "\n",
    "## Connection to Tutorial\n",
    "\n",
    "This notebook demonstrates the concepts from **Chapter 6: Dirichlet Process Mixture Models** in the tutorial. See the tutorial for:\n",
    "- Detailed explanation of stick-breaking\n",
    "- Chinese Restaurant Process interpretation\n",
    "- Comparison with fixed-K GMMs\n",
    "- GenJAX implementation details"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Advanced: GenJAX Implementation (Optional)\n\nThe implementation above uses pure JAX for simplicity and reliability. For those interested in using GenJAX (Gen's JAX backend), here's how you would implement the same model using generative functions.\n\n**Note**: GenJAX requires `numpy<2.0.0` which conflicts with some Colab packages. The pure JAX implementation above is recommended for most users.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# GenJAX implementation (requires: pip install genjax)\n# Uncomment to install: !pip install \"genjax\" \"numpy<2.0.0\"\n\n# import genjax\n# from genjax import gen, static_check, Pytree\n\n# @gen\n# def dpmm_model(alpha: float, K: int, N: int, mu0: float, sig0: float, sigx: float):\n#     \"\"\"DPMM generative model in GenJAX.\"\"\"\n#     # Sample cluster centers\n#     mus = jnp.zeros(K)\n#     for k in range(K):\n#         mus = mus.at[k].set(genjax.normal(mu0, sig0) @ f\"mu_{k}\")\n    \n#     # Stick-breaking process\n#     pis = jnp.zeros(K)\n#     remaining = 1.0\n#     for k in range(K):\n#         beta_k = genjax.beta(1.0, alpha) @ f\"beta_{k}\"\n#         pis = pis.at[k].set(beta_k * remaining)\n#         remaining *= (1.0 - beta_k)\n    \n#     # Normalize to get mixture weights\n#     pis = pis / jnp.sum(pis)\n#     thetas = genjax.dirichlet(pis * 100) @ \"thetas\"\n    \n#     # Sample observations\n#     xs = jnp.zeros(N)\n#     for i in range(N):\n#         z = genjax.categorical(thetas) @ f\"z_{i}\"\n#         x = genjax.normal(mus[z], sigx) @ f\"x_{i}\"\n#         xs = xs.at[i].set(x)\n    \n#     return xs\n\n# @gen\n# def dpmm_inference(observed_data, alpha, K):\n#     \"\"\"Run importance resampling inference.\"\"\"\n#     # Create observations dict\n#     observations = {f\"x_{i}\": x for i, x in enumerate(observed_data)}\n    \n#     # Importance sampling\n#     num_samples = 500\n#     traces = []\n#     log_weights = []\n    \n#     for _ in range(num_samples):\n#         trace = genjax.simulate(dpmm_model, (alpha, K, len(observed_data), 0.0, 4.0, 0.05))\n#         conditioned = genjax.condition(trace, observations)\n#         traces.append(conditioned)\n#         log_weights.append(conditioned.get_score())\n    \n#     # Normalize weights and resample\n#     weights = jax.nn.softmax(jnp.array(log_weights))\n#     indices = random.choice(random.PRNGKey(0), len(weights), shape=(num_samples,), p=weights)\n    \n#     return [traces[i] for i in indices]\n\n# # Example usage:\n# # posterior_traces = dpmm_inference(default_data, alpha=2.0, K=10)\n# # mus_posterior = jnp.array([t[\"mu_0\"] for t in posterior_traces])\n\nprint(\"GenJAX implementation shown (commented out due to dependency conflicts)\")\nprint(\"The pure JAX implementation above is functionally equivalent and more reliable.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}