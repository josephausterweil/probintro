{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Interactive Dirichlet Process Mixture Model Explorer\n\nThis notebook demonstrates **Bayesian inference for Dirichlet Process Mixture Models (DPMM)** using JAX.\n\n**What you'll learn:**\n- How DPMMs automatically discover the number of clusters in data\n- The stick-breaking construction for infinite mixture models\n- How the concentration parameter Œ± controls cluster formation\n- Posterior inference using importance resampling\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/josephausterweil/probintro/blob/amplify/notebooks/dpmm_interactive.ipynb)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\nFirst, let's install the required packages if running on Google Colab:\n\n**Note**: After running the installation cell below, you may need to restart the runtime (Runtime ‚Üí Restart runtime) before proceeding."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if running on Colab\ntry:\n    import google.colab\n    IN_COLAB = True\nexcept:\n    IN_COLAB = False\n\nif IN_COLAB:\n    print(\"Running on Google Colab - installing dependencies...\")\n    # Install compatible versions: numpy 2.0.x works with most Colab packages\n    # Upgrade JAX to satisfy flax and orbax requirements\n    !pip install -q --upgrade \"jax>=0.6.0\" \"jaxlib>=0.6.0\" \"numpy>=2.0,<2.1\" scipy ipywidgets\n    print(\"‚úì Dependencies installed\")\n    print(\"‚ö†Ô∏è  Please restart runtime (Runtime ‚Üí Restart runtime) before continuing\")\nelse:\n    print(\"Running locally\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import jax\nimport jax.numpy as jnp\nimport jax.random as random\nfrom jax.scipy.stats import norm, beta, dirichlet\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import gaussian_kde\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\n# Enable widgets in Colab\ntry:\n    import google.colab\n    from google.colab import output\n    output.enable_custom_widget_manager()\nexcept:\n    pass\n\n# Set random seed for reproducibility\nkey = random.PRNGKey(42)\n\nprint(\"‚úì Imports successful\")\nprint(f\"JAX version: {jax.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dirichlet Process Mixture Model\n",
    "\n",
    "A DPMM is an infinite mixture model that can automatically discover the number of clusters in data.\n",
    "\n",
    "### Model Structure\n",
    "\n",
    "For each cluster $k = 1, 2, \\ldots, K_{\\text{max}}$:\n",
    "1. Draw cluster center: $\\mu_k \\sim \\text{Normal}(\\mu_0, \\sigma_0^2)$\n",
    "2. Draw stick-breaking weight: $\\beta_k \\sim \\text{Beta}(1, \\alpha)$\n",
    "3. Compute mixture probability: $\\pi_k = \\beta_k \\prod_{j<k}(1-\\beta_j)$\n",
    "\n",
    "Then normalize to get $\\theta \\sim \\text{Dirichlet}(\\pi_1, \\ldots, \\pi_K)$\n",
    "\n",
    "For each observation $i = 1, \\ldots, N$:\n",
    "1. Draw cluster assignment: $z_i \\sim \\text{Categorical}(\\theta)$\n",
    "2. Draw observation: $x_i \\sim \\text{Normal}(\\mu_{z_i}, \\sigma_x^2)$\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- **Œ± (concentration)**: Controls how spread out the mixture is\n",
    "  - Small Œ± ‚Üí Few active clusters\n",
    "  - Large Œ± ‚Üí Many clusters with similar weights\n",
    "- **$K_{\\text{max}}$**: Truncation level (upper bound on clusters)\n",
    "- **$\\mu_0, \\sigma_0$**: Prior on cluster centers\n",
    "- **$\\sigma_x$**: Observation noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Let's implement the DPMM using pure JAX (without GenJAX for now, to keep it simple and debuggable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stick_breaking_weights(key, alpha, K):\n",
    "    \"\"\"Generate stick-breaking weights for DPMM.\n",
    "    \n",
    "    Args:\n",
    "        key: JAX random key\n",
    "        alpha: Concentration parameter\n",
    "        K: Number of components\n",
    "    \n",
    "    Returns:\n",
    "        pis: Mixture probabilities (sum to ~1)\n",
    "    \"\"\"\n",
    "    betas = random.beta(key, 1.0, alpha, shape=(K,))\n",
    "    \n",
    "    # Stick-breaking: œÄ_k = Œ≤_k * ‚àè_{j<k}(1-Œ≤_j)\n",
    "    pis = jnp.zeros(K)\n",
    "    remaining_stick = 1.0\n",
    "    \n",
    "    for k in range(K):\n",
    "        pis = pis.at[k].set(betas[k] * remaining_stick)\n",
    "        remaining_stick *= (1.0 - betas[k])\n",
    "    \n",
    "    # Ensure no zero probabilities (for numerical stability)\n",
    "    pis = jnp.maximum(pis, 1e-6)\n",
    "    pis = pis / jnp.sum(pis)  # Renormalize\n",
    "    \n",
    "    return pis\n",
    "\n",
    "def sample_dpmm_prior(key, alpha, K, mu0, sig0, sigx, N):\n",
    "    \"\"\"Sample from DPMM prior.\n",
    "    \n",
    "    Returns:\n",
    "        mus: Cluster centers\n",
    "        thetas: Mixture probabilities\n",
    "        zs: Cluster assignments\n",
    "        xs: Generated data\n",
    "    \"\"\"\n",
    "    keys = random.split(key, 4)\n",
    "    \n",
    "    # Sample cluster centers\n",
    "    mus = random.normal(keys[0], shape=(K,)) * sig0 + mu0\n",
    "    \n",
    "    # Generate stick-breaking weights\n",
    "    pis = stick_breaking_weights(keys[1], alpha, K)\n",
    "    \n",
    "    # Sample from Dirichlet to get final mixture weights\n",
    "    # (This adds additional variability on top of stick-breaking)\n",
    "    thetas = random.dirichlet(keys[2], pis * 100)  # Scale up for concentration\n",
    "    \n",
    "    # Sample cluster assignments\n",
    "    zs = random.categorical(keys[3], jnp.log(thetas), shape=(N,))\n",
    "    \n",
    "    # Sample observations\n",
    "    key_xs = random.split(keys[3], N)\n",
    "    xs = jnp.array([random.normal(key_xs[i]) * sigx + mus[zs[i]] for i in range(N)])\n",
    "    \n",
    "    return mus, thetas, zs, xs\n",
    "\n",
    "def log_likelihood(xs, mus, thetas, sigx, K):\n",
    "    \"\"\"Compute log likelihood of data given parameters.\"\"\"\n",
    "    N = len(xs)\n",
    "    ll = 0.0\n",
    "    \n",
    "    for i in range(N):\n",
    "        # p(x_i | mus, thetas) = ‚àë_k Œ∏_k * N(x_i | Œº_k, œÉ_x¬≤)\n",
    "        component_lls = jnp.array([norm.logpdf(xs[i], mus[k], sigx) for k in range(K)])\n",
    "        ll += jax.scipy.special.logsumexp(jnp.log(thetas) + component_lls)\n",
    "    \n",
    "    return ll\n",
    "\n",
    "def importance_sampling(key, obs_xs, alpha, K, mu0, sig0, sigx, num_samples):\n",
    "    \"\"\"Perform importance sampling for posterior inference.\n",
    "    \n",
    "    Args:\n",
    "        obs_xs: Observed data\n",
    "        alpha, K, mu0, sig0, sigx: Model parameters\n",
    "        num_samples: Number of importance samples\n",
    "    \n",
    "    Returns:\n",
    "        mus_samples: Posterior samples of cluster centers\n",
    "        thetas_samples: Posterior samples of mixture weights\n",
    "        weights: Importance weights (normalized)\n",
    "    \"\"\"\n",
    "    N = len(obs_xs)\n",
    "    keys = random.split(key, num_samples)\n",
    "    \n",
    "    # Sample from prior\n",
    "    all_mus = []\n",
    "    all_thetas = []\n",
    "    log_weights = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        mus, thetas, _, _ = sample_dpmm_prior(keys[i], alpha, K, mu0, sig0, sigx, N)\n",
    "        all_mus.append(mus)\n",
    "        all_thetas.append(thetas)\n",
    "        \n",
    "        # Compute likelihood as importance weight\n",
    "        ll = log_likelihood(obs_xs, mus, thetas, sigx, K)\n",
    "        log_weights.append(ll)\n",
    "    \n",
    "    # Normalize weights\n",
    "    log_weights = jnp.array(log_weights)\n",
    "    log_weights = log_weights - jax.scipy.special.logsumexp(log_weights)\n",
    "    weights = jnp.exp(log_weights)\n",
    "    \n",
    "    return jnp.array(all_mus), jnp.array(all_thetas), weights\n",
    "\n",
    "def importance_resampling(key, mus_samples, thetas_samples, weights, num_resamples):\n",
    "    \"\"\"Resample from importance samples to get posterior samples.\"\"\"\n",
    "    indices = random.choice(key, len(weights), shape=(num_resamples,), p=weights)\n",
    "    return mus_samples[indices], thetas_samples[indices]\n",
    "\n",
    "print(\"‚úì DPMM functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Interactive Exploration\n\nNow let's create an interactive function to explore how DPMMs work! The cell below defines `run_dpmm_inference()` which you can call with different parameters."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Default observed data (3 clear clusters)\ndefault_data = np.array([-10.4, -10., -9.4, -10.1, -9.9, 0., 9.5, 9.9, 10., 10.1, 10.5])\n\ndef run_dpmm_inference(alpha=2.0, K_max=10, num_samples=500, data_str=None):\n    \"\"\"Run DPMM inference and visualize results.\n    \n    Parameters:\n    -----------\n    alpha : float\n        Concentration parameter (0.1 to 10.0)\n    K_max : int\n        Maximum number of clusters (3 to 20)\n    num_samples : int\n        Number of importance samples (100 to 2000)\n    data_str : str, optional\n        Comma-separated data points. If None, uses default 3-cluster data.\n    \"\"\"\n    # Parse data\n    if data_str is None or data_str == \"\":\n        obs_xs = default_data\n    else:\n        try:\n            obs_xs = jnp.array([float(x.strip()) for x in data_str.split(',')])\n        except:\n            print(\"‚ö†Ô∏è  Error parsing data, using default\")\n            obs_xs = default_data\n    \n    N = len(obs_xs)\n    print(f\"üîÑ Running inference with Œ±={alpha:.1f}, K_max={K_max}, {num_samples} samples...\")\n    print(f\"   Data: {N} observations\")\n    \n    # Fixed hyperparameters\n    mu0 = 0.0\n    sig0 = 4.0\n    sigx = 0.05\n    \n    # Run importance sampling\n    print(f\"   ‚è≥ Sampling from prior and computing likelihoods...\")\n    key_local = random.PRNGKey(np.random.randint(0, 10000))\n    mus_samples, thetas_samples, weights = importance_sampling(\n        key_local, obs_xs, alpha, K_max, mu0, sig0, sigx, num_samples\n    )\n    \n    # Resample to get posterior samples\n    print(f\"   ‚è≥ Resampling to get posterior...\")\n    key_resample = random.PRNGKey(np.random.randint(0, 10000))\n    mus_post, thetas_post = importance_resampling(\n        key_resample, mus_samples, thetas_samples, weights, num_samples\n    )\n    \n    # Generate posterior predictive samples\n    print(f\"   ‚è≥ Generating posterior predictive samples...\")\n    key_pred = random.PRNGKey(np.random.randint(0, 10000))\n    pred_samples = []\n    for i in range(num_samples):\n        z = random.categorical(key_pred, jnp.log(thetas_post[i]))\n        x = random.normal(key_pred) * sigx + mus_post[i, z]\n        pred_samples.append(x)\n        key_pred = random.split(key_pred, 1)[0]\n    \n    pred_samples = jnp.array(pred_samples)\n    \n    # Flatten for visualization\n    mus_flat = mus_post.flatten()\n    thetas_flat = thetas_post.flatten()\n    \n    print(\"   ‚úÖ Inference complete!\\n\")\n    \n    # Create visualization\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    # Histogram of observed data\n    ax.hist(obs_xs, bins=max(3, int(np.sqrt(N))), density=True, alpha=0.5, \n            color='gray', label='Observed data', edgecolor='black')\n    \n    # Posterior predictive density\n    x_range = np.linspace(float(obs_xs.min()) - 2, float(obs_xs.max()) + 2, 200)\n    kde_pred = gaussian_kde(np.array(pred_samples))\n    ax.plot(x_range, kde_pred(x_range), 'b-', linewidth=3, \n            label='Posterior predictive p(xÃÇ|data)', alpha=0.8)\n    \n    # Posterior of cluster centers (weighted by mixture probabilities)\n    kde_mus = gaussian_kde(np.array(mus_flat), weights=np.array(thetas_flat))\n    ax.plot(x_range, kde_mus(x_range), 'r-', linewidth=3,\n            label='Posterior p(Œº|data)', alpha=0.8)\n    \n    ax.set_xlabel('x', fontsize=12)\n    ax.set_ylabel('Density', fontsize=12)\n    ax.set_title(f'DPMM Inference (Œ±={alpha:.1f}, K_max={K_max})', fontsize=14, fontweight='bold')\n    ax.legend(fontsize=11)\n    ax.grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print statistics\n    effective_clusters = jnp.sum(thetas_post.mean(axis=0) > 0.01)\n    print(f\"üìä Estimated active clusters: {int(effective_clusters)} (with Œ∏ > 0.01)\")\n    print(f\"üìä Top 3 cluster weights: {[f'{w:.3f}' for w in sorted(thetas_post.mean(axis=0), reverse=True)[:3]]}\")\n\nprint(\"‚úì DPMM inference function defined\")"
  },
  {
   "cell_type": "markdown",
   "source": "# Interactive widget interface\nfrom ipywidgets import interact, FloatSlider, IntSlider, Text\n\nprint(\"üéõÔ∏è  Interactive DPMM Explorer\")\nprint(\"=\"*70)\nprint(\"Adjust the sliders below and the visualization will update automatically!\\n\")\n\ninteract(\n    run_dpmm_inference,\n    alpha=FloatSlider(value=2.0, min=0.1, max=10.0, step=0.1, description='Œ± (concentration):'),\n    K_max=IntSlider(value=10, min=3, max=20, step=1, description='K_max (clusters):'),\n    num_samples=IntSlider(value=500, min=100, max=2000, step=100, description='Samples:'),\n    data_str=Text(value='', description='Custom data:', placeholder='Leave empty for default, or enter: -5, -4.8, 5, 4.8')\n);",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercises\n\nUse the interactive sliders above or call the function directly to try these experiments:\n\n1. **Effect of Œ±**:\n   - Set Œ±=0.5: How many clusters are active?\n   - Set Œ±=5.0: How does the posterior change?\n\n2. **Different data** (enter in Custom data field or call function):\n   - Two clusters: `-5, -4.8, -5.2, 5, 4.8, 5.2`\n   - Four clusters: `-10, -9, 0, 1, 10, 11, 20, 21`\n   - Single cluster: `0, 0.1, -0.1, 0.2, -0.2`\n\n3. **Truncation level**:\n   - Use default data (3 clusters) but set K_max=20\n   - What happens to the unused clusters?\n\n4. **Sample size**:\n   - Run with 100 samples vs 1000 samples\n   - How does it affect the smoothness of posteriors?"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **Automatic discovery**: DPMMs automatically discover the number of clusters without specifying K in advance\n",
    "\n",
    "2. **Concentration parameter**: Œ± controls the \"richness\" of the mixture:\n",
    "   - Small Œ± ‚Üí Few large clusters (concentrated)\n",
    "   - Large Œ± ‚Üí Many small clusters (dispersed)\n",
    "\n",
    "3. **Posterior uncertainty**: The red curve shows uncertainty about cluster locations, not just point estimates\n",
    "\n",
    "4. **Predictive distribution**: The blue curve shows what new data might look like, accounting for both parameter uncertainty and cluster structure\n",
    "\n",
    "## Connection to Tutorial\n",
    "\n",
    "This notebook demonstrates the concepts from **Chapter 6: Dirichlet Process Mixture Models** in the tutorial. See the tutorial for:\n",
    "- Detailed explanation of stick-breaking\n",
    "- Chinese Restaurant Process interpretation\n",
    "- Comparison with fixed-K GMMs\n",
    "- GenJAX implementation details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}