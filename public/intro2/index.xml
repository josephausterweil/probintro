<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Continuous Probability and Bayesian Learning :: Probability &amp; Probabilistic Computing Tutorial</title><link>https://josephausterweil.github.io/probintro/intro2/index.html</link><description>From Discrete to Continuous with GenJAX In the first two tutorials, you learned:
Tutorial 1: Probability theory using discrete outcomes (sets and counting) Tutorial 2: How to express probability in code using GenJAX Now you’ll learn to work with continuous probability distributions and perform Bayesian learning on real-valued data, all using the GenJAX tools you’ve already learned!</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 01 Jan 0001 00:00:00 +0000</lastBuildDate><atom:link href="https://josephausterweil.github.io/probintro/intro2/index.xml" rel="self" type="application/rss+xml"/><item><title>Chibany's Mystery Bentos</title><link>https://josephausterweil.github.io/probintro/intro2/01_mystery_bentos/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://josephausterweil.github.io/probintro/intro2/01_mystery_bentos/index.html</guid><description>The Weight of the Matter It’s a new semester at the university, and Professor Chibany’s students have been bringing him bentos again. But this time, something is different.
Last semester, Chibany could choose his bento (tonkatsu or hamburger) and he learned about probability by counting his choices. But this semester, the students have been secretly choosing for him. Every day, a mysterious bento box appears on his desk during office hours.</description></item><item><title>The Continuum: Continuous Probability</title><link>https://josephausterweil.github.io/probintro/intro2/02_continuous/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://josephausterweil.github.io/probintro/intro2/02_continuous/index.html</guid><description>From Counting to Measuring Chibany stares at his histogram. He understands expected value now. The 455g average makes sense as a mixture of 500g tonkatsu and 350g hamburger bentos.
But something still bothers him.
Look at these actual measurements from his first week:
Monday: 520g (tonkatsu) Tuesday: 348g (hamburger) Wednesday: 505g (tonkatsu) Thursday: 362g (hamburger) Friday: 488g (tonkatsu) The weights aren’t exactly 500g and 350g! They vary.
And here’s the deeper question: What’s the probability that a bento weighs exactly 505.000000… grams?</description></item><item><title>The Gaussian Distribution</title><link>https://josephausterweil.github.io/probintro/intro2/03_gaussian/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://josephausterweil.github.io/probintro/intro2/03_gaussian/index.html</guid><description>The Bell Curve After learning about the uniform distribution in Chapter 2, Chibany realizes something: real measurements rarely spread evenly across a range. When he measures 1000 tonkatsu bentos carefully, the weights don’t spread uniformly between 495g and 505g. Instead, most cluster near 500g, with fewer and fewer measurements appearing as you move away from that center value.
This pattern appears everywhere in nature:
Heights of people Measurement errors Test scores Daily temperatures And yes, bento weights! This is the Gaussian distribution (also called the Normal distribution), and it’s arguably the most important probability distribution in statistics.</description></item><item><title>Bayesian Learning with Gaussians</title><link>https://josephausterweil.github.io/probintro/intro2/04_bayesian_learning/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://josephausterweil.github.io/probintro/intro2/04_bayesian_learning/index.html</guid><description>The Learning Problem Chibany has a new challenge. He receives shipments from a new supplier, but doesn’t know the mean weight of their tonkatsu bentos. He believes they’re trying to hit 500g (like his usual supplier), but he’s not certain. Maybe they aim for 495g? Or 505g?
The question: How can he learn the true mean weight from observations?
This is Bayesian learning: starting with a prior belief, observing data, and updating to a posterior belief.</description></item><item><title>Gaussian Mixture Models</title><link>https://josephausterweil.github.io/probintro/intro2/05_mixture_models/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://josephausterweil.github.io/probintro/intro2/05_mixture_models/index.html</guid><description>Returning to the Mystery Remember Chibany’s original puzzle from Chapter 1? He had mystery bentos with two peaks in their weight distribution, but the average fell in a valley where no individual bento existed.
We now have all the tools to solve this completely:
Chapter 1: Expected value paradox in mixtures Chapter 2: Continuous probability (PDFs, CDFs) Chapter 3: Gaussian distributions Chapter 4: Bayesian learning for parameters Now we combine them: What if we have multiple Gaussian distributions mixed together, and we need to figure out both which component each observation belongs to AND the parameters of each component?</description></item><item><title>Dirichlet Process Mixture Models</title><link>https://josephausterweil.github.io/probintro/intro2/06_dpmm/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://josephausterweil.github.io/probintro/intro2/06_dpmm/index.html</guid><description>The Problem with Fixed K In Chapter 5, we solved Chibany’s bento mystery using a Gaussian Mixture Model (GMM) with K=2 components. But we had to specify K in advance and use BIC to validate our choice.
What if:
We don’t know how many types exist? The number of types changes over time? We want the model to discover the number of clusters automatically? Enter the Dirichlet Process Mixture Model (DPMM): A Bayesian nonparametric approach that learns the number of components from the data.</description></item></channel></rss>