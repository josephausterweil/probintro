<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Probability &amp; Probabilistic Computing Tutorial</title>
    <link>http://localhost:1313/probintro/index.html</link>
    <description></description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Nov 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/probintro/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A Narrative Introduction to Probability</title>
      <link>http://localhost:1313/probintro/intro/index.html</link>
      <pubDate>Fri, 07 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/probintro/intro/index.html</guid>
      <description>Welcome! This tutorial teaches probability theory through the story of Chibany (pictured below) who loves tonkatsu and wants to understand how likely he will get it in his daily meals. Along the way, you’ll learn to think about probability using sets — a perspective that makes complex concepts intuitive and prepares you for probabilistic programming and advanced applications.</description>
    </item>
    <item>
      <title>Chibany&#39;s Mystery Bentos</title>
      <link>http://localhost:1313/probintro/intro2/01_mystery_bentos/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/probintro/intro2/01_mystery_bentos/index.html</guid>
      <description>The Weight of the Matter It’s a new semester at the university, and Professor Chibany’s students have been bringing him bentos again. But this time, something is different.&#xA;Last semester, Chibany could choose his bento (tonkatsu or hamburger) and he learned about probability by counting his choices. But this semester, the students have been secretly choosing for him. Every day, a mysterious bento box appears on his desk during office hours.</description>
    </item>
    <item>
      <title>Probabilistic Programming with GenJAX</title>
      <link>http://localhost:1313/probintro/genjax/index.html</link>
      <pubDate>Thu, 06 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/probintro/genjax/index.html</guid>
      <description>From Probability Theory to Probabilistic Code In the previous tutorial, you learned to think about probability using sets and counting. Chibany showed you that probability questions are really about:&#xA;What’s possible? (Define the outcome space) What am I interested in? (Define the event) Count them! (Calculate the ratio) Now you’ll learn to express those same ideas in code using GenJAX — a probabilistic programming language that lets computers do the counting for you!</description>
    </item>
    <item>
      <title>Intro2</title>
      <link>http://localhost:1313/probintro/intro2/index.html</link>
      <pubDate>Mon, 18 Aug 2025 17:37:54 +0900</pubDate>
      <guid>http://localhost:1313/probintro/intro2/index.html</guid>
      <description></description>
    </item>
    <item>
      <title>The Continuum: Continuous Probability</title>
      <link>http://localhost:1313/probintro/intro2/02_continuous/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/probintro/intro2/02_continuous/index.html</guid>
      <description>From Counting to Measuring Chibany stares at his histogram. He understands expected value now. The 455g average makes sense as a mixture of 500g tonkatsu and 350g hamburger bentos.&#xA;But something still bothers him.&#xA;Look at these actual measurements from his first week:&#xA;Monday: 520g (tonkatsu) Tuesday: 348g (hamburger) Wednesday: 505g (tonkatsu) Thursday: 362g (hamburger) Friday: 488g (tonkatsu) The weights aren’t exactly 500g and 350g! They vary.&#xA;And here’s the deeper question: What’s the probability that a bento weighs exactly 505.000000… grams?</description>
    </item>
    <item>
      <title>The Gaussian Distribution</title>
      <link>http://localhost:1313/probintro/intro2/03_gaussian/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/probintro/intro2/03_gaussian/index.html</guid>
      <description>The Bell Curve After learning about the uniform distribution in Chapter 2, Chibany realizes something: real measurements rarely spread evenly across a range. When he measures 1000 tonkatsu bentos carefully, the weights don’t spread uniformly between 495g and 505g. Instead, most cluster near 500g, with fewer and fewer measurements appearing as you move away from that center value.&#xA;This pattern appears everywhere in nature:&#xA;Heights of people Measurement errors Test scores Daily temperatures And yes, bento weights! This is the Gaussian distribution (also called the Normal distribution), and it’s arguably the most important probability distribution in statistics.</description>
    </item>
    <item>
      <title>Bayesian Learning with Gaussians</title>
      <link>http://localhost:1313/probintro/intro2/04_bayesian_learning/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/probintro/intro2/04_bayesian_learning/index.html</guid>
      <description>The Learning Problem Chibany has a new challenge. He receives shipments from a new supplier, but doesn’t know the mean weight of their tonkatsu bentos. He believes they’re trying to hit 500g (like his usual supplier), but he’s not certain. Maybe they aim for 495g? Or 505g?&#xA;The question: How can he learn the true mean weight from observations?&#xA;This is Bayesian learning: starting with a prior belief, observing data, and updating to a posterior belief.</description>
    </item>
    <item>
      <title>Gaussian Mixture Models</title>
      <link>http://localhost:1313/probintro/intro2/05_mixture_models/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/probintro/intro2/05_mixture_models/index.html</guid>
      <description>Returning to the Mystery Remember Chibany’s original puzzle from Chapter 1? He had mystery bentos with two peaks in their weight distribution, but the average fell in a valley where no individual bento existed.&#xA;We now have all the tools to solve this completely:&#xA;Chapter 1: Expected value paradox in mixtures Chapter 2: Continuous probability (PDFs, CDFs) Chapter 3: Gaussian distributions Chapter 4: Bayesian learning for parameters Now we combine them: What if we have multiple Gaussian distributions mixed together, and we need to figure out both which component each observation belongs to AND the parameters of each component?</description>
    </item>
    <item>
      <title>Dirichlet Process Mixture Models</title>
      <link>http://localhost:1313/probintro/intro2/06_dpmm/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/probintro/intro2/06_dpmm/index.html</guid>
      <description>The Problem with Fixed K In Chapter 5, we solved Chibany’s bento mystery using a Gaussian Mixture Model (GMM) with K=2 components. But we had to specify K in advance and use BIC to validate our choice.&#xA;What if:&#xA;We don’t know how many types exist? The number of types changes over time? We want the model to discover the number of clusters automatically? Enter the Dirichlet Process Mixture Model (DPMM): A Bayesian nonparametric approach that learns the number of components from the data.</description>
    </item>
  </channel>
</rss>