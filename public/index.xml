<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Probability &amp; Probabilistic Computing Tutorial</title>
    <link>http://localhost:1313/probintro/index.html</link>
    <description></description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Nov 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/probintro/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A Narrative Introduction to Probability</title>
      <link>http://localhost:1313/probintro/intro/index.html</link>
      <pubDate>Wed, 26 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/probintro/intro/index.html</guid>
      <description>Welcome! This tutorial teaches probability theory through the story of Chibany (pictured below) who loves tonkatsu and wants to understand how likely they will get it in his daily meals. They hang out at the University cafeteria. Students offer him meals in bento boxes in the hopes that it will help them on their upcoming tests.&#xA;Along the way, youâ€™ll learn to think about probability using sets: a perspective that makes complex concepts intuitive and prepares you for probabilistic programming and advanced applications.</description>
    </item>
    <item>
      <title>Chibany&#39;s Mystery Bentos</title>
      <link>http://localhost:1313/probintro/intro2/01_mystery_bentos/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/probintro/intro2/01_mystery_bentos/index.html</guid>
      <description>The Weight of the Matter Itâ€™s a new semester at the university, and Chibanyâ€™s students have been bringing them bentos again. But this time, something is different.&#xA;Last semester, the bento boxes were transparent. So Chibany could see whether there was tonkatsu or hamburger inside. But this semester, the bento boxes are opaque. Thus, Chibany canâ€™t see what is inside when they receive the bento. They want to know IMMEDIATELY what the meal is, but it would be extremely rude to open the bento box as the students are watching. Thankfully, Chibany is curious, and theyâ€™re a probabilist.</description>
    </item>
    <item>
      <title>Intro2</title>
      <link>http://localhost:1313/probintro/intro2/index.html</link>
      <pubDate>Wed, 26 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/probintro/intro2/index.html</guid>
      <description></description>
    </item>
    <item>
      <title>Probabilistic Programming with GenJAX</title>
      <link>http://localhost:1313/probintro/genjax/index.html</link>
      <pubDate>Wed, 26 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/probintro/genjax/index.html</guid>
      <description>From Probability Theory to Probabilistic Code In the previous tutorial, you learned to think about probability using sets and counting. Chibany showed you that probability questions are really about:&#xA;Whatâ€™s possible? (Define the outcome space) What am I interested in? (Define the event) Count them! (Calculate the ratio) Now youâ€™ll learn to express those same ideas in code using GenJAX â€” a probabilistic programming language that lets computers do the counting for you!</description>
    </item>
    <item>
      <title>The Continuum: Continuous Probability</title>
      <link>http://localhost:1313/probintro/intro2/02_continuous/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/probintro/intro2/02_continuous/index.html</guid>
      <description>From Counting to Measuring Chibany stares at their histogram. They understand expected value now. The 455g average makes sense as a mixture of 500g tonkatsu and 350g hamburger bentos.&#xA;But something still bothers them.&#xA;Look at these actual measurements from their first week:&#xA;Monday: 520g (tonkatsu) Tuesday: 348g (hamburger) Wednesday: 505g (tonkatsu) Thursday: 362g (hamburger) Friday: 488g (tonkatsu) The weights arenâ€™t exactly 500g and 350g! They vary.&#xA;And hereâ€™s the deeper question: Whatâ€™s the probability that a bento weighs exactly 505.000000â€¦ grams?</description>
    </item>
    <item>
      <title>The Gaussian Distribution</title>
      <link>http://localhost:1313/probintro/intro2/03_gaussian/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/probintro/intro2/03_gaussian/index.html</guid>
      <description>The Bell Curve After learning about the uniform distribution in Chapter 2, Chibany realizes something: real measurements rarely spread evenly across a range. When they measure 1000 tonkatsu bentos carefully, the weights donâ€™t spread uniformly between 495g and 505g. Instead, most cluster near 500g, with fewer and fewer measurements appearing as you move away from that center value.&#xA;This pattern appears everywhere in nature:&#xA;Heights of people Measurement errors Test scores Daily temperatures And yes, bento weights! This is the Gaussian distribution (also called the Normal distribution), and itâ€™s arguably the most important probability distribution in statistics.</description>
    </item>
    <item>
      <title>Bayesian Learning with Gaussians</title>
      <link>http://localhost:1313/probintro/intro2/04_bayesian_learning/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/probintro/intro2/04_bayesian_learning/index.html</guid>
      <description>The Learning Problem Chibany has a new challenge. They receive shipments from a new supplier, but donâ€™t know the mean weight of their tonkatsu bentos. They believe the supplier is trying to hit 500g (like their usual supplier), but theyâ€™re not certain. Maybe the supplier aims for 495g? Or 505g?&#xA;The question: How can they learn the true mean weight from observations?&#xA;This is Bayesian learning: starting with a prior belief, observing data, and updating to a posterior belief.</description>
    </item>
    <item>
      <title>Gaussian Mixture Models</title>
      <link>http://localhost:1313/probintro/intro2/05_mixture_models/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/probintro/intro2/05_mixture_models/index.html</guid>
      <description>Returning to the Mystery Remember Chibanyâ€™s original puzzle from Chapter 1? They had mystery bentos with two peaks in their weight distribution, but the average fell in a valley where no individual bento existed.&#xA;We now have all the tools to solve this completely:&#xA;Chapter 1: Expected value paradox in mixtures Chapter 2: Continuous probability (PDFs, CDFs) Chapter 3: Gaussian distributions Chapter 4: Bayesian learning for parameters Now we combine them: What if we have multiple Gaussian distributions mixed together, and we need to figure out both which component each observation belongs to AND the parameters of each component?</description>
    </item>
    <item>
      <title>Dirichlet Process Mixture Models</title>
      <link>http://localhost:1313/probintro/intro2/06_dpmm/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/probintro/intro2/06_dpmm/index.html</guid>
      <description>The Problem with Fixed K In Chapter 5, we solved Chibanyâ€™s bento mystery using a Gaussian Mixture Model (GMM) with K=2 components. But we had to specify K in advance and use BIC to validate our choice.&#xA;What if:&#xA;We donâ€™t know how many types exist? The number of types changes over time? We want the model to discover the number of clusters automatically? Enter the Dirichlet Process Mixture Model (DPMM): A Bayesian nonparametric approach that learns the number of components from the data.</description>
    </item>
    <item>
      <title>Glossary - All Tutorials</title>
      <link>http://localhost:1313/probintro/glossary/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/probintro/glossary/index.html</guid>
      <description>How to Use This Glossary This glossary covers all three tutorials in the Probability with GenJAX series. Terms are tagged to show which tutorial introduces them:&#xA;ðŸ“˜ Tutorial 1 (Discrete Probability) - Sets and counting approach ðŸ’» Tutorial 2 (GenJAX Programming) - Probabilistic programming basics ðŸ“Š Tutorial 3 (Continuous Probability) - Advanced topics and Bayesian learning Click on any term to expand its definition with examples and code.&#xA;Core Concepts (Tutorial 1) Bayes Theorem ðŸ“˜ Bayes Theorem Bayes Theorem (or Bayesâ€™ rule) is a formula for reversing the order that variables are conditioned â€” how to go from $P(A \mid B)$ to $P(B \mid A)$.</description>
    </item>
  </channel>
</rss>