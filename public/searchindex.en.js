var relearn_searchindex = [
  {
    "breadcrumb": "Narrative Introduction to Probabilistic Computing",
    "content": "Introduction acknowledgements",
    "description": "Introduction acknowledgements",
    "tags": [],
    "title": "A Narrative Introduction to Probability",
    "uri": "/intro/index.html"
  },
  {
    "breadcrumb": "Narrative Introduction to Probabilistic Computing \u003e A Narrative Introduction to Probability",
    "content": "Introduce probability theory and Bayesian inference from a set-based perspective. Uses real-world examples and examples to introduce key concepts, such as conditional and marginal probability, independence, and Bayes’ rule. Tailored examples to highlight common misconceptions (e.g., base-rate neglect with taxicab problem) and how thinking through it clearly using tools from tutorial. Provide the knowledge and confidence to start learning probabilistic computing and transition to that in the next tutorial Present everything in a friendly narrative for approachability",
    "description": "Introduce probability theory and Bayesian inference from a set-based perspective. Uses real-world examples and examples to introduce key concepts, such as conditional and marginal probability, independence, and Bayes’ rule. Tailored examples to highlight common misconceptions (e.g., base-rate neglect with taxicab problem) and how thinking through it clearly using tools from tutorial. Provide the knowledge and confidence to start learning probabilistic computing and transition to that in the next tutorial Present everything in a friendly narrative for approachability",
    "tags": [],
    "title": "Goals",
    "uri": "/intro/01_goals/index.html"
  },
  {
    "breadcrumb": "Narrative Introduction to Probabilistic Computing",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Intro2",
    "uri": "/intro2/index.html"
  },
  {
    "breadcrumb": "Narrative Introduction to Probabilistic Computing \u003e A Narrative Introduction to Probability",
    "content": "Chibany wakes up from dreaming of the delicious meals he will get later today. Twice per day, a student brings a bento box with a meal as an offering to Chibany. One student brings him a bento box in the early afternoon for lunch and a different student brings him a bento box in the evening for dinner. The meal is either a Hamburger or a Tonkatsu (pork cutlet) . To keep track of his meal possibilities, his lists out the four possibilities:\nblock-beta block columns 2 a[\"H(amburger) H(amburger)\"] b[\"H(amburger) T(onkastu)\"] c[\"T(onkastu) H(amburger)\"] d[\"T(onkastu) T(onkastu)\"] end Sets This forms a set of four elements. A set is a collection of elements or members. In this case, an element is defined by the two meals given to Chibany that day. Sets are defined by the elements they do or do not contain. The elements are listed with commas between them and “$\\{$” denotes the start of a set and “$\\}$” the end of a set.\nOutcome Space In the context of probability theory, the basic elements of what can occur are called outcomes. Outcomes are the fundamental building blocks probabilities are from. As they are fundamental, the Greek letter $\\Omega$ is frequently used to refer to this set of possibile outcomes. Diligently noting his daily offerings, Chibany defines $\\Omega = \\{HH, HT, TH, TT \\} $. The first letter defines his lunch offering, and the second letter defines his dinner offering. He notes that $H$ now always refers to hamburgers and $T$ to tonkatsu.\nNote that technically, the elements of a set are unique. So, if Chibany writes down getting a pair of hamburgers twice and a hamburger and a tonkatsu ($\\{HH, HH, HT\\}$), he’s gotten the same set of possibilities as if he only got one pair of hamburgers and a hamburger and tonkatsu ($\\{HH, HT\\}$). In other words, $\\{HH, HH, HT\\} = \\{HH, HT\\}$.\nChibany is skeptical, but will try to keep it in mind. It can be confusing!\nPossibilities vs. Events So far, we have discussed sets, possibile outcomes and the set of all possible outcomes $\\Omega$. Chibany is interested in the set of possible meals that include Tonkatsu. What is this set?\n${HT, TH, TT}$\nThis is an example of an event. Technically, an event or a set that is none, some, or all of the possible outcomes.\nQuick Check Is $\\Omega$ an event?\nsolution Yes – it is the event that contains all possible outcomes. Is $\\Omega$ the set of all possible events?\nsolution No What is the set of all possible events for Chibany’s situation?\nsolution $\\{ \\{ \\}, \\{ HH \\}, \\{ HT\\}, \\{TH \\}, \\{TT\\}, \\{HH,HT\\}, \\{HH,TH\\}, \\{HH,TT\\}, \\{HT, TH\\}, \\{HT, TT \\}, \\{TH, TT\\} \\{HH, HT, TH\\}, \\{HH, HT, TT \\}, \\{HH, TH, TT\\}, \\{HT, TH, TT\\}, \\{HH, HT, TH, TT\\} \\}$\nNote that $\\{ \\}$ is called the empty or null set and is a special set that contains no elements.",
    "description": "Chibany wakes up from dreaming of the delicious meals he will get later today. Twice per day, a student brings a bento box with a meal as an offering to Chibany. One student brings him a bento box in the early afternoon for lunch and a different student brings him a bento box in the evening for dinner. The meal is either a Hamburger or a Tonkatsu (pork cutlet) . To keep track of his meal possibilities, his lists out the four possibilities:",
    "tags": [],
    "title": "Chibany is hungry",
    "uri": "/intro/02_hungry/index.html"
  },
  {
    "breadcrumb": "Narrative Introduction to Probabilistic Computing \u003e A Narrative Introduction to Probability",
    "content": "One goal of this tutorial is to show you that probability is counting. When every possibility is equally likely, probability is defined as the relative number of possibilities in each set. When possibilities are not equally likely, it is only slightly more complicated. Rather than each possibility counting one towards the size of a set it is in, you count the possibility according to its relative weight.\nCounting The basic operation that we use to define probabilities is counting the number of elements in a set. If $A$ is a set, then $|A|$ is the cardinality or size of the set. For example, the set of Chibany’s lunch options is $\\{H,T\\}$. Counting the number of elements determines its size, which is $\\left|\\{H, T\\} \\right| = 2$. The set of Chibany’s meal offerings for a day, $\\Omega = \\{HH, HT, TH, TT \\}$. There are four possibilities, so its size $|\\Omega|$ is $ 4$.\nChibany is still hungry… and desires Tonkatsu Chibany is still hungry and wondering what his meal possibilities are for the day. He wonders, what is the probability that students appease him today by giving him Tonkatsu?\nTo make this calculation, Chibany lists out the outcome space $\\Omega$ again. He then forms the event “Tonkatsu offering today”. He defines the set of possible outcomes with a Tonkatsu as $A = \\{HT, TH, TT\\}$ to encode the event. He highlights those in red. Chibany thinks “wow… three of the four possible outcomes are red. Fortune must favor me today, right?”\nblock-beta block columns 2 a[\"H(amburger) H(amburger)\"] b[\"H(amburger) T(onkastu)\"] c[\"T(onkastu) H(amburger)\"] d[\"T(onkastu) T(onkastu)\"] end style b stroke: #f33, stroke-width:4px style c stroke: #f33, stroke-width:4px style d stroke: #f33, stroke-width:4px Yes, Chibany, it does as it always should. Your chance of getting Tonkatsu is three out of four or 0.75. He calculated the probability exactly as he should!\nProbability as Counting The probability of an event $A$ is $\\frac{|A|}{|\\Omega|}$. It is written as $P(A)$. In the prior example, $|A| = | \\{HT, TH, TT\\} |$ and $|\\Omega| = | \\{HH, HT, TH, TT\\}|$ have three and four elements, respectively. Note that if the possible outcomes were not equally likely, we would sum their individual probabilities to calculate the cardinality. But everything works in the same way – the probability of the event is the total “size” or “weight” of the possible outcomes in the event as compared to the total size or weight of all possible outcomes.\nWhat is the probability that Chibany gets Tonkatsu for his first offering? Well the possible outcomes with Tonkatsu are $\\{TH, TT\\}$. There are four possible outcomes for his offerings $\\Omega = \\{HH,HT, TH, TT\\}$. So the probability he gets Tonkatsu for his first offering is $|\\{TH, TT\\}|/|\\{HH,HT, TH, TT\\}| = 2/4=1/2$. Chibany draws the following table to illustrate his counting:\nblock-beta block columns 2 a[\"H(amburger) H(amburger)\"] b[\"H(amburger) T(onkastu)\"] c[\"T(onkastu) H(amburger)\"] d[\"T(onkastu) T(onkastu)\"] end style c stroke: #f33, stroke-width:4px style d stroke: #f33, stroke-width:4px Random Variables Chibany wants to know… how much Tonkatsu? Chibany wants to know how much Tonkatsu he gets each day. To do so, he converts each possibility to a whole number: the number of Tonkatsu in that possibility. He calls this a function $f : \\Omega \\rightarrow \\{0, 1, 2, \\ldots\\}$, meaning it takes a possibility out of the outcome space and maps it (changes it into) a number. He notes: mapping every possibility to a whole number is like making each whole number an event! His Tonkatsu counter $f$ is defined as $f(HH) = 0$, $f(HT) = 1$, $f(TH)=1$, and $f(TT) = 2$. Chibany defined his first random variable.\nblock-beta block columns 2 a[\"HH: 0\"] space b[\"HT: 1\"] c[\"TH: 1\"] d[\"TT: 2\"] space end style b stroke: #44c, stroke-width:4px style c stroke: #44c, stroke-width:4px style d stroke: #f33, stroke-width:4px What is the probability of having two tonkatsus? We count the number of outcomes with two tonkatsus ($\\{TT\\}$ highlighted in red) and divide by the number of possible outcomes ($|\\Omega|=4$). So, it is 1 out of 4 or 1/4.\nWhat about the probability of having exactly one tonkatsu? We count the number of outcomes with exactly one tonkatsu ($\\{HT, TH\\}$) and divide by the number of possible outcomes ($|\\Omega|=4$). So it is 2/4 or 1/2.",
    "description": "One goal of this tutorial is to show you that probability is counting. When every possibility is equally likely, probability is defined as the relative number of possibilities in each set. When possibilities are not equally likely, it is only slightly more complicated. Rather than each possibility counting one towards the size of a set it is in, you count the possibility according to its relative weight.\nCounting The basic operation that we use to define probabilities is counting the number of elements in a set. If $A$ is a set, then $|A|$ is the cardinality or size of the set. For example, the set of Chibany’s lunch options is $\\{H,T\\}$. Counting the number of elements determines its size, which is $\\left|\\{H, T\\} \\right| = 2$. The set of Chibany’s meal offerings for a day, $\\Omega = \\{HH, HT, TH, TT \\}$. There are four possibilities, so its size $|\\Omega|$ is $ 4$.",
    "tags": [],
    "title": "Probability and Counting",
    "uri": "/intro/03_prob_count/index.html"
  },
  {
    "breadcrumb": "Narrative Introduction to Probabilistic Computing \u003e A Narrative Introduction to Probability",
    "content": "Chibany wants a tonkatsu dinner A graduate of Chiba Tech, Tanaka-san, visits Chibany one day and tells Chinbany that he knows that there will be at least one tonkatsu in tomorrow’s offering. Chibany is excited. They wants to know how likely it is that the second meal is a Tonkatsu. They quiz Tanaka-san. He say it’s just as likely as before, so it should be 1/2. Chibany disagrees. Chibany says “I learned something because I knows I will get at least one tonkatsu”. Also, Chibany is an optimist and deserves to have all the tonkatsu. Who’s right!? Let’s check the chart…\nblock-beta block columns 1 a[\"HH\"] block:group1 b[\"HT\"] c[\"TH\"] d[\"TT\"] space end end style a fill:#999, text-decoration:line-through style group1 stroke: #33f, stroke-width: 6px style c stroke: #f33, stroke-width: 4px style d stroke: #f33, stroke-width: 4px In the case where there is at least one tonkatsu, the space of possible outcomes is $\\{HT, TH, TT\\}$, which is outlined in blue. The event of interest for Chibany is outlined in red. It turns out Chibany is correct! There is a two in three chance that he gets a tonkatsu dinner. That’s larger than one in two.\nChibany kindly reminds Tanaka-san that you never stop learning and to consider taking one of Joe’s classes at Chiba Tech. Chibany hears great things about them!\nDefining conditional probability as set restriction What Chibany calculated is a conditional probability – the probability of an event (two tonkatsus) conditioned on knowledge of another event (at least one tonkatsu). Conditioning on an event means that the possible outcomes in that event form the set of possibilities or outcome space. We then calculate probabilities as normal within that restricted outcome space. In our example, we’re interested in the probability of the event $A= \\{TT\\}$ conditioned on the knowledge that there’s at least one tonkatsu, $ B = \\Omega_{\\geq 1 T}= \\{HT, TH, TT\\}$. Formally, this is written as $P(A \\mid B) = \\frac{|A|}{|B|}$, where everything to the left of the $\\mid$ is what we’re interested in knowing the probability of and everything to the right of the $\\mid$ is what we know to be true.\nNote that this is a different, yet equivalent perspective to how conditional probability is traditionally taught.\nDependence and independence Tanaka-san explains to Chibany his reasoning: He did not think whether Chibany received a tonkatsu (T) for their first offering influenced whether they receive a tonkatsu (T) for their second offering.\nChibany is curious. Tanaka-san’s logic seems sound, but it sounds like a slightly different question. Chibany asks Tanaka-san to draw out the outcome space and events for this question to help clarify what is different. Tanaka-san states his question formally: What is the probability of getting a second tonkatsu ($\\{TT\\}$) given the first offering was a tonkatsu ($\\{TH, TT\\}$) or $P(\\{HT, TT\\} \\mid \\{TH, TT\\})$\nblock-beta block columns 1 block:group1 a[\"HH\"] b[\"HT\"] end block:group2 c[\"TH\"] d[\"TT\"] end end style group1 fill:#999, text-decoration:line-through style a fill:#999, text-decoration:line-through style b fill:#999, text-decoration:line-through style group2 stroke: #33f, stroke-width: 6px style d stroke: #f33, stroke-width: 4px There’s one outcome ($TT$) out of two possible outcomes ($\\{TH, TT\\}$). Thus the probability is $1/2$: $P(\\{HT, TT\\} \\mid \\{TH, TT\\}) = 1/2$.\nTanaka-san says this time the result is what I expected. he says “If I just think about what the probability of the second meal is and make that my outcome space, then the probability of the second meal being tonkatsu should just be one-half. Chibany asks Tanaka-san the draw out this outcome space and calculate the probability this way instead. Chibany notes that probability is much more fun when you ask your friends to help you do the hard parts!\nblock-beta block:group2 columns 2 c[\"H\"] d[\"T\"] end style group2 stroke: #33f, stroke-width: 6px style d stroke: #f33, stroke-width: 4px Look at that – It’s one half! Chibany prefers learning that there will be at least one tonkatsu because it makes it more likely that he will get a tonkatsu for his second offering.\nWe saw in one case that conditioning on an event (that there will be one tonkatsu) influenced the probability of another event (that the second offering will be tonkatsu). But in a different case, conditioning on a slighlty differnt event (that the first meal will be a tonkatsu) did not influence the probability of another event (again, that the second offering will be a tonkatsu).\nWhen conditioning on one event $A$ influences the probability of another event $B$, those two events are called dependent. This is denoted as $A \\not\\perp B$. If they do not influence each other they are called independent, which is denoted as $A \\perp B$.\nMarginal and joint probabilities Chibany is sad (marginalization) The student that normally gives Chibany his second offering is out sick. Now Chibany only gets one offering per day. Chibany lists out the new set of possibilities $\\Omega_1 = \\{H, T\\}$.\nblock-beta block columns 2 a[\"H(amburger)\"] b[\"T(onkastu)\"] end style b stroke: #f33, stroke-width:4px He notes this is a much sadder set of possibilities. At least the probability of getting Tonkatsu isn’t too low! It’s one of two possibilities.\nThankfully, on the next day, the student is healthy again and Chibany is back to getting two offerings each day. This changes the set of possibilities back to the original one $\\Omega_2 = \\{HH,HT, TH, TT \\}$. Chibany realizes he can calculate the probability of the first offering being Tonkatsu. Getting his second meal shouldn’t influence the chance the first one is Tonkatsu, right? Let’s check!\nblock-beta block columns 2 a[\"H(amburger) H(amburger)\"] b[\"H(amburger) T(onkastu)\"] c[\"T(onkastu) H(amburger)\"] d[\"T(onkastu) T(onkastu)\"] end style c stroke: #f33, stroke-width:4px style d stroke: #f33, stroke-width:4px In this case, he is interested $P(\\{TH, TT \\}) = 2/4 = 1/2$. Phew!\nWhat happened here? In both cases, we are interested in the same event – the probability the first meal is a Tonkatsu. In the first case, we did not include the second meal. This is called using marginal probability. In the second case, we did include the second meal. This is called using joint probability. Technically it counts the number of outcomes in the intersection of the different events being considered jointly. This means the number of outcomes that are in all the events under consideration.\nThe Sum Rule: More on Marginalization and Marginal Probabilities Intuitively, the following two ways of calculating the probability a variable takes a value should give the same answer: (marginal probability) list the possible outcomes containing only that variable and count those where it has the specified value, and (2) enumerate the possible outcomes containing that variable and another variable and count all of those where the first variable has the value of interest (joint probability).\nFormally, if we have two random variables $A$ and $B$, the marginal probability of $A$ $P(A)$ is\n$P(A) = \\sum_{b} P(A, B=b)$.\nIf you’re unfamiliar with the notation $\\sum_{b}$, $\\sum$ is a fancy way of saying “add the following up” and the $b$ tells you which values to add up over (in this case, the values $b$ that random variable $B$ could possibly be).\nIn the last example, $A$ was Chibany’s first meal and $B$ was Chibany’s second meal. We were interested in whether Chibany’s first meal was Tonkatsu or $P(A=T)$. The possible values for $B$ are Hamburger and Tonkatsu or ${H,T }$. What we showed was $ P(A=T) = \\sum_{b} P(A=T,B=b) = P(A=T, B=H) + P(A=T, B=T) = 1/4 + 1/4 = 2/4 = 1/2 $\nThe other definition of conditional probability Using joint and marginal probabilities, we can define conditional probability in a different manner – as the ratio of the joint probability to the marginal probability of the conditioned information. Or\n$P(A \\mid B) = \\frac{P(A,B)}{P(B)}$\nNote that the probability of $B$ must be greater than zero ($P(B) \u003e 0$). This makes sense to Chibany. How could he be given information that had zero chance of happening?\nChibany is no fan of this other way of calculating conditional probabilities, but he decides to practice using it. He goes back to his favorite example so far – the one where he had better than a one-half chance of getting two Tonkatsus. In that example, he learned he was going to get at least one Tonkatsu and was interested in finding the probability that there would be two Tonkatsus. So, $A$ is getting a tonkatsu dinner (second meal is tonkatsu) and $B$ is that there is at least one tonkatsu. So $A = \\{HT, TT\\}$ and $B=\\{HT, TH, TT\\}$. The intersection or common possibilities in $A$ and $B$ is $\\{HT,TT\\}$. Remember that there are four possible outcomes in the larger outcome space $\\Omega = \\{HH,HT,TH,TT\\}$ This means $P(A,B) = |\\{HT,TT\\}/ | \\{HH,HT,TH,TT\\} = 2/4$. $P(B) = |\\{HT,TH,TT\\}|/\\{HH,HT,TH,TT\\} = 3/4$. Putting these together we get $P(A \\mid B) = \\frac{P(A,B)}{P(B)} = \\frac{2/4}{3/4} = \\frac{2}{3}$\nAlthough Chibany is happy to see the same result of it being more likely than not he’ll have a second meal of Tonkatsu if he learns he gets at least one Tonkatsu, this felt a lot harder to him than the first way of doing things. It may have felt that way for you too (it does for me!). That’s why Chibany wants everyone to know the set-based perspective to probability.\nWeighted possibilities Chibany tells students that he likes Tonkatsu more Chibany is happy! He remembered that students love learning. He has important information for them: Chibany likes Tonkatsu more than Hamburgers.\nWhile wondering how to calculate probabilities taking this glorious news into account, Tanaka-san stops by. Tanaka-san lets Chibany know that the students coordinate to ensure that he gets at least one tonkatsu, but try not to make both offerings tonkatsu (that way he doesn’t get tired of Tonkatsu). Tanaka-san shares the following chart the students use to guide their daily offerings\nblock-beta block columns 2 a[\"HH: 4%\"] b[\"HT: 43%\"] c[\"TH: 43%\"] d[\"TT: 10%\"] style b stroke: #f33, stroke-width:4px style c stroke: #f33, stroke-width:4px style d stroke: #f33, stroke-width:4px end Chibany is confused at first, but he sticks with the rules he learned. I follow the same procedure as before, but add the weighted versions of each outcome rather than each outcome counting 1 automatically.\nSo he adds up the outcomes containing Tonkatsu (outlined in red) and divides it by the total amount: $P(\\textrm{Tonkatsu}) = \\frac{0.43+0.43+0.10}{0.04+0.43+0.43+0.10} = \\frac{0.96}{1}=0.96$\nHe gets a lot more Tonkatsu – Tonkatsu 96% of the time. Joyous times!\nPractice question: Can you determine whether the first and second meals are dependent? How would you do that? answer If $A$ and $B$ are random variables encoding Chibany’s first meal and second meals, we would want to see whether $P(A=a)$ is different from $P(A =a \\mid B=b)$ for any possible $a$ or $b$. Let’s consider whether the probability the first meal is Tonkatsu is influenced by the second meal being Tonkatsu. First let’s calculate $P(A=T)$. To do, we’ll use the sum rule, so $P(A=T) = \\sum_b{P(A=T, B= b)} = P(A=T, B=H) + P(A=T, B=T) = 0.43+0.10 = 0.53$. Is this different from the $P(A = T \\mid B=T)$? How do we calculate this in the weighted case? The same as before except the $| \\Omega|$ is the amount of weight for the conditioned event $B=T$. So, $P(A=T \\mid B=T) = \\frac{0.10}{0.43+0.10} = \\frac{0.1}{0.53} \\approx 0.19$.",
    "description": "Chibany wants a tonkatsu dinner A graduate of Chiba Tech, Tanaka-san, visits Chibany one day and tells Chinbany that he knows that there will be at least one tonkatsu in tomorrow’s offering. Chibany is excited. They wants to know how likely it is that the second meal is a Tonkatsu. They quiz Tanaka-san. He say it’s just as likely as before, so it should be 1/2. Chibany disagrees. Chibany says “I learned something because I knows I will get at least one tonkatsu”. Also, Chibany is an optimist and deserves to have all the tonkatsu. Who’s right!? Let’s check the chart…",
    "tags": [],
    "title": "Conditional probability as changing the possible outcomes",
    "uri": "/intro/04_conditional/index.html"
  },
  {
    "breadcrumb": "Narrative Introduction to Probabilistic Computing \u003e A Narrative Introduction to Probability",
    "content": "Bayes’ Theorem (Bayes’ rule) provides a way to update our beliefs in one random variable given information about a different random variable. Let’s say we have certain hypotheses about how the world works, which we denote as random variable $H$. Further, we have senses that provide us information. Let’s encode the information that we might get from our senses as $D$ (maybe an image from our eyes) and we currently observe $d$ (maybe a picture of tonkatsu).\nBayes Theorem tells us to update our beliefs in hypothesis $h$ being the way the world works after learning $D=d$ in the following manner:\n$P(H=h \\mid D = d) = \\frac{P(D=d\\mid H=h) P(H=h)}{P(D=d)}$\nwhere $P(D=d \\mid H=h)$ is called the likelihood, whic h is the probability of observing $d$ given $h$ is the true hypothesis for how the world works, $P(H=h)$ is called the prior, which tells us how likely it is that $h$ is the way the world works\nWe have all the information to prove this! Feel free to skip to the next subsection if you don’t care about proofs.\nProving Bayes’ rule Using the other definition of conditional probability, we know that $P(H \\mid D) = \\frac{P(H,D)}{P(D)}$. If we multiply both sides of the equation by $P(D)$, we get $P(H,D) = P(H \\mid D) P(D)$. We can do the same thing but for the opposite way of conditioning (the joint probability can be written in either order and it is the same as it is the common elements of two sets which is the same no matter which order you consider the two sets), so $P(D \\mid H) = \\frac{P(H,D)}{P(H)}$. We can can solve for $P(H,D)$ in a similar manner: multiply both sides of the equation by $P(H)$ and we get $P(H,D) = P(D \\mid H) P(H)$. Putting these together, we can prove Bayes’ rule:\n$P(H \\mid D) P(D) = P(H,D) = P(D \\mid H) P(H)$ $\\Rightarrow P(H \\mid D) = \\frac{P(H,D)}{P(D)} = \\frac{P(D \\mid H) P(H)}{P(D)}$\nThe Taxicab Problem TODO: Add picture with chibany seeing a hit and run with a taxi with fog/smoke\nIn Chibany’s hometown, there are two taxi companies: the Green and the Blue . All Green company’s taxis are painted green and all the Blue company’s taxis are painted blue .\n85% of the town’s taxis work for the Green company. So 15% fo the town’s taxis work for the company.\nLate one foggy evening, Chibany saw a cab perform a hit-and-run (hit another car and leave without providing any information). Chibany saw a taxi!\nChibany is an outstanding citizen and so they go to the police with this information. The police know it was foggy and dark, so it’s possible Chibany might not have seen the taxi’s color correctly. They test Chibany several times and find that Chibany reports the correct taxi color 80% of the time!\nTaking all of this information into account, how likely do you think it is that the cab involved in the hit-and-run was a Blue taxi ?\nanswer The correct answer is 41%, but most people think it is closer to 60-80%! This is known as the Taxicab Problem (Kahneman and Tversky, 1972; Bar-Hillel, 1980).\nA note: Kahneman and Tversky (and others) use this example (and others) to argue that people are not Bayesian at all! There are a number of replies through the years and it is an ongoing debate. Joe loves discussing it. If interested, please reach out and he would be more than happy to discuss it more.\nTaxicab Solution 1 One way to solve this is to use the outcome space perspective! Let us assume there are 100 taxis in Chibany’s hometown. That means the set of possibilities $\\Omega$ has 85 individual Green taxis and 15 individual Blue taxis block-beta block columns 10 g1[\"fa:fa-taxi\"] g2[\"fa:fa-taxi\"] g3[\"fa:fa-taxi\"] g4[\"fa:fa-taxi\"] g5[\"fa:fa-taxi\"] g6[\"fa:fa-taxi\"] g7[\"fa:fa-taxi\"] g8[\"fa:fa-taxi\"] g9[\"fa:fa-taxi\"] g10[\"fa:fa-taxi\"] g11[\"fa:fa-taxi\"] g12[\"fa:fa-taxi\"] g13[\"fa:fa-taxi\"] g14[\"fa:fa-taxi\"] g15[\"fa:fa-taxi\"] g16[\"fa:fa-taxi\"] g17[\"fa:fa-taxi\"] g18[\"fa:fa-taxi\"] g19[\"fa:fa-taxi\"] g20[\"fa:fa-taxi\"] g21[\"fa:fa-taxi\"] g22[\"fa:fa-taxi\"] g23[\"fa:fa-taxi\"] g24[\"fa:fa-taxi\"] g25[\"fa:fa-taxi\"] g26[\"fa:fa-taxi\"] g27[\"fa:fa-taxi\"] g28[\"fa:fa-taxi\"] g29[\"fa:fa-taxi\"] g30[\"fa:fa-taxi\"] g31[\"fa:fa-taxi\"] g32[\"fa:fa-taxi\"] g33[\"fa:fa-taxi\"] g34[\"fa:fa-taxi\"] g35[\"fa:fa-taxi\"] g36[\"fa:fa-taxi\"] g37[\"fa:fa-taxi\"] g38[\"fa:fa-taxi\"] g39[\"fa:fa-taxi\"] g40[\"fa:fa-taxi\"] g41[\"fa:fa-taxi\"] g42[\"fa:fa-taxi\"] g43[\"fa:fa-taxi\"] g44[\"fa:fa-taxi\"] g45[\"fa:fa-taxi\"] g46[\"fa:fa-taxi\"] g47[\"fa:fa-taxi\"] g48[\"fa:fa-taxi\"] g49[\"fa:fa-taxi\"] g50[\"fa:fa-taxi\"] g51[\"fa:fa-taxi\"] g52[\"fa:fa-taxi\"] g53[\"fa:fa-taxi\"] g54[\"fa:fa-taxi\"] g55[\"fa:fa-taxi\"] g56[\"fa:fa-taxi\"] g57[\"fa:fa-taxi\"] g58[\"fa:fa-taxi\"] g59[\"fa:fa-taxi\"] g60[\"fa:fa-taxi\"] g61[\"fa:fa-taxi\"] g62[\"fa:fa-taxi\"] g63[\"fa:fa-taxi\"] g64[\"fa:fa-taxi\"] g65[\"fa:fa-taxi\"] g66[\"fa:fa-taxi\"] g67[\"fa:fa-taxi\"] g68[\"fa:fa-taxi\"] g69[\"fa:fa-taxi\"] g70[\"fa:fa-taxi\"] g71[\"fa:fa-taxi\"] g72[\"fa:fa-taxi\"] g73[\"fa:fa-taxi\"] g74[\"fa:fa-taxi\"] g75[\"fa:fa-taxi\"] g76[\"fa:fa-taxi\"] g77[\"fa:fa-taxi\"] g78[\"fa:fa-taxi\"] g79[\"fa:fa-taxi\"] g80[\"fa:fa-taxi\"] g81[\"fa:fa-taxi\"] g82[\"fa:fa-taxi\"] g83[\"fa:fa-taxi\"] g84[\"fa:fa-taxi\"] g85[\"fa:fa-taxi\"] b11[\"fa:fa-taxi\"] b12[\"fa:fa-taxi\"] b13[\"fa:fa-taxi\"] b14[\"fa:fa-taxi\"] b15[\"fa:fa-taxi\"] b1[\"fa:fa-taxi\"] b2[\"fa:fa-taxi\"] b3[\"fa:fa-taxi\"] b4[\"fa:fa-taxi\"] b5[\"fa:fa-taxi\"] b6[\"fa:fa-taxi\"] b7[\"fa:fa-taxi\"] b8[\"fa:fa-taxi\"] b9[\"fa:fa-taxi\"] b10[\"fa:fa-taxi\"] classDef blueTaxi color: #06f, min-width:22px, font-size:18px classDef greenTaxi color: #0d2, min-width:22px, font-size:18px class b1,b2,b3,b4,b5,b6,b7,b8,b9,b10,b11,b12,b13,b14,b15 blueTaxi class g1,g2,g3,g4,g5,g6,g7,g8,g9,g10,g11,g12,g13,g14,g15,g16,g17,g18,g19,g20,g21,g22,g23,g24,g25,g26,g27,g28,g29,g30,g31,g32,g33,g34,g35,g36,g37,g38,g39,g40,g41,g42,g43,g44,g45,g46,g47,g48,g49,g50,g51,g52,g53,g54,g55,g56,g57,g58,g59,g60,g61,g62,g63,g64,g65,g66,g67,g68,g69,g70,g71,g72,g73,g74,g75,g76,g77,g78,g79,g80,g81,g82,g83,g84,g85 greenTaxi end Now I can make the outcome space include the taxi color and whether Chibany identifies the taxi as Blue in foggy nighttime conditions. As Chibany correctly identifies 80% of the Blue taxis as Blue, ($15 \\times 0.80=12$). This means 12 of the Blue taxis are identified as Blue and ($15 \\times 0.2 = 3$) 3 are incorrectly as Green. As Chibany incorrectly identifies 20% of the Green taxis as Blue. This means ($85 \\times 0.2 = 17$) 17 of the Green taxis are identified as Blue and ($85 \\times 0.8=68$) 68 are correctly identified as Green.\nblock-beta block columns 10 g1[\"fa:fa-taxi\"] g2[\"fa:fa-taxi\"] g3[\"fa:fa-taxi\"] g4[\"fa:fa-taxi\"] g5[\"fa:fa-taxi\"] g6[\"fa:fa-taxi\"] g7[\"fa:fa-taxi\"] g8[\"fa:fa-taxi\"] g9[\"fa:fa-taxi\"] g10[\"fa:fa-taxi\"] g11[\"fa:fa-taxi\"] g12[\"fa:fa-taxi\"] g13[\"fa:fa-taxi\"] g14[\"fa:fa-taxi\"] g15[\"fa:fa-taxi\"] g16[\"fa:fa-taxi\"] g17[\"fa:fa-taxi\"] g18[\"fa:fa-taxi\"] g19[\"fa:fa-taxi\"] g20[\"fa:fa-taxi\"] g21[\"fa:fa-taxi\"] g22[\"fa:fa-taxi\"] g23[\"fa:fa-taxi\"] g24[\"fa:fa-taxi\"] g25[\"fa:fa-taxi\"] g26[\"fa:fa-taxi\"] g27[\"fa:fa-taxi\"] g28[\"fa:fa-taxi\"] g29[\"fa:fa-taxi\"] g30[\"fa:fa-taxi\"] g31[\"fa:fa-taxi\"] g32[\"fa:fa-taxi\"] g33[\"fa:fa-taxi\"] g34[\"fa:fa-taxi\"] g35[\"fa:fa-taxi\"] g36[\"fa:fa-taxi\"] g37[\"fa:fa-taxi\"] g38[\"fa:fa-taxi\"] g39[\"fa:fa-taxi\"] g40[\"fa:fa-taxi\"] g41[\"fa:fa-taxi\"] g42[\"fa:fa-taxi\"] g43[\"fa:fa-taxi\"] g44[\"fa:fa-taxi\"] g45[\"fa:fa-taxi\"] g46[\"fa:fa-taxi\"] g47[\"fa:fa-taxi\"] g48[\"fa:fa-taxi\"] g49[\"fa:fa-taxi\"] g50[\"fa:fa-taxi\"] g51[\"fa:fa-taxi\"] g52[\"fa:fa-taxi\"] g53[\"fa:fa-taxi\"] g54[\"fa:fa-taxi\"] g55[\"fa:fa-taxi\"] g56[\"fa:fa-taxi\"] g57[\"fa:fa-taxi\"] g58[\"fa:fa-taxi\"] g59[\"fa:fa-taxi\"] g60[\"fa:fa-taxi\"] g61[\"fa:fa-taxi\"] g62[\"fa:fa-taxi\"] g63[\"fa:fa-taxi\"] g64[\"fa:fa-taxi\"] g65[\"fa:fa-taxi\"] g66[\"fa:fa-taxi\"] g67[\"fa:fa-taxi\"] g68[\"fa:fa-taxi\"] g69[\"fa:fa-taxi\"] g70[\"fa:fa-taxi\"] g71[\"fa:fa-taxi\"] g72[\"fa:fa-taxi\"] g73[\"fa:fa-taxi\"] g74[\"fa:fa-taxi\"] g75[\"fa:fa-taxi\"] g76[\"fa:fa-taxi\"] g77[\"fa:fa-taxi\"] g78[\"fa:fa-taxi\"] g79[\"fa:fa-taxi\"] g80[\"fa:fa-taxi\"] g81[\"fa:fa-taxi\"] g82[\"fa:fa-taxi\"] g83[\"fa:fa-taxi\"] g84[\"fa:fa-taxi\"] g85[\"fa:fa-taxi\"] b11[\"fa:fa-taxi\"] b12[\"fa:fa-taxi\"] b13[\"fa:fa-taxi\"] b14[\"fa:fa-taxi\"] b15[\"fa:fa-taxi\"] b1[\"fa:fa-taxi\"] b2[\"fa:fa-taxi\"] b3[\"fa:fa-taxi\"] b4[\"fa:fa-taxi\"] b5[\"fa:fa-taxi\"] b6[\"fa:fa-taxi\"] b7[\"fa:fa-taxi\"] b8[\"fa:fa-taxi\"] b9[\"fa:fa-taxi\"] b10[\"fa:fa-taxi\"] classDef blueTaxi color: #06f, min-width:22px, font-size:18px, stroke: #f33, stroke-width:2px classDef blueGrayTaxi color: #028, min-width:22px, font-size:18px classDef greenTaxi color: #0d2, min-width:22px, font-size:18px, stroke: #f33, stroke-width:2px classDef greenGrayTaxi color: #051, min-width:22px, font-size:18px class b1,b2,b3,b4,b5,b6,b7,b11,b12,b13,b14,b15 blueTaxi class b8,b9,b10 blueGrayTaxi class g1,g2,g3,g4,g5,g6,g7,g8,g9,g10,g11,g12,g13,g14,g15,g16,g17 greenTaxi class g18,g19,g20,g21,g22,g23,g24,g25,g26,g27,g28,g29,g30,g31,g32,g33,g34,g35,g36,g37,g38,g39,g40,g41,g42,g43,g44,g45,g46,g47,g48,g49,g50,g51,g52,g53,g54,g55,g56,g57,g58,g59,g60,g61,g62,g63,g64,g65,g66,g67,g68,g69,g70,g71,g72,g73,g74,g75,g76,g77,g78,g79,g80,g81,g82,g83,g84,g85 greenGrayTaxi end The brightly colored taxis that are outlined in red are those that Chibany reports as Blue in the difficult viewing conditions. We can already see there are more Green taxis tha Blue , so it is still more probable that the taxi involved in the hit-and-run was Green. We can get the exact probability that it was a Blue taxi by the same counting rule as before. There are 12 Blue taxis and 17 Green taxis. So, the probability that it was a blue taxi given Chibany reports it as Blue is $12/(12+17)=12/29 \\approx 0.41$.\nTaxicab Solution 2 We can also solve this without counting in a sample space following the rules of probability theory as described before. Let $X$ be the actual color of the taxi involved in the hit-and-run and $W$ be the color reported by Chibany. Based on the percentage of Blue and Green taxis in the city, we know that $P(X=G) = 0.85$ and $P(X=B)=0.15$. We also know that Chibany is accurate 80% of the time. So, $P(W = B \\mid X = B) = 0.8$ and $P(W=G \\mid X=G)=0.8$. This also means Chibany is inaccurate 20% fo the tim$e: $P(W = B \\mid X=G)=0.2$ and $P(W=G \\mid X=B)=0.2$.\nChibany said the taxi is Blue and given this, how likely it is that the taxi is Blue. So, we’re interested in $P(X=B \\mid W=B)$ We can solve this using Bayes’ rule and the sum rule.\n$P(X=B \\mid W=B) = \\frac{P(W =B \\mid X=B) P(X=B)}{P(W=B)}$\n$P(X=B \\mid W=B) = \\frac{P(W =B \\mid X=B) P(X=B)}{\\sum_c{P(W=B,X=c)}}$\n$P(X=B \\mid W=B) = \\frac{P(W =B \\mid X=B) P(X=B)}{\\sum_c{P(W=B \\mid X=c)P(X=c)}}$\n$P(X=B \\mid W=B) = \\frac{P(W =B \\mid X=B) P(X=B)}{P(W=B \\mid X=B)P(X=B) + P(W=B \\mid X=G)P(X=G)}$\n$P(X=B \\mid W=B) = \\frac{0.8 \\times 0.15 }{0.8 \\times 0.15 + 0.2 \\times 0.85} = \\frac{0.12}{0.12+0.17} = \\frac{0.12}{0.29} \\approx 0.41$\nWhy Learn the Set-Based Perspective to Probability Theory? If we can solve probability problems via symbol manipulation, why is learning the set-based perspective to probability theory?\nHere are some reasons:\nAs variables become more complex, explicitly solving problems becomes infeasible. Thinking through how to count is strong starting point for a generative process perspective, which discusses how outcomes are produced according to computer programs with random choices. These define proabilistic models! Probabilistic computing are programming languages for specifying probabilistic models and built to calculate different probabilities according to this model in an efficient manner. We will build to exploring how to do this over the next few tutorials. Many probability novices find the distinction between joint and conditional probabilities confusing and unintuitive. From the set-based perspective, their difference is clear. Joint probabilities count in the outcome space where multiple possible outcomes are being simultaneously. Conditional probabilities change the outcome space to be whatever is consistent with the conditioned information and then count in that new space. It forces you to think about how events and outcomes are represented. This can be obsfucated at times when thinking about probabilities from the rule-based perspective. They are formally equivalent. It is fun. It connects combinatorics and probability theory. It makes Chibany happy. Transfer additional practice questions Example with rare disease and not too diagnostic test.\nExample with organic fruit and made at a local place",
    "description": "Bayes’ Theorem (Bayes’ rule) provides a way to update our beliefs in one random variable given information about a different random variable. Let’s say we have certain hypotheses about how the world works, which we denote as random variable $H$. Further, we have senses that provide us information. Let’s encode the information that we might get from our senses as $D$ (maybe an image from our eyes) and we currently observe $d$ (maybe a picture of tonkatsu).",
    "tags": [],
    "title": "Bayes' Theorem or Bayes' Rule",
    "uri": "/intro/05_bayes/index.html"
  },
  {
    "breadcrumb": "Narrative Introduction to Probabilistic Computing \u003e A Narrative Introduction to Probability",
    "content": "set set A set is a collection of elements or members. Sets are defined by the elements they do or do not contain. The elements are listed with commas between them and “$\\{$” denotes the start of a set and “$\\}$” the end of a set. Note that the elements of a set are unique. event event An event is a set that is none, some, or all of the possible outcomes. cardinality cardinality The cardinality or size of a set is the number of elements it contains. If $A = \\{H, T\\}$, then the cardinality $A$ is $|A|=2$. probability probability The probability of an event $A$ relative to an outcome space $\\Omega$ is the ratio of their sizes or $\\frac{|A|}{|\\Omega|}$ random variable random variable A random variable is a function that maps from the set of possible outcomes to some set or space. The output or range of the function could be the set of outcomes again, a whole number based on the outcome (e.g., counting the number of Tonkatsu), or something more complex (e.g., the world’s friendship matrix, an 8-billion by 8-billion, binary matrix where $N$ where $N_{1,100}=1$ if person 1 is friends with person 100). Technically the output must be measurable. You shouldn’t worry about that distinction unless your random variable’s output gets really, really big (like continuous). We’ll talk more about probabilities over continuous random variables later. conditional probability conditional probability The conditional probability is the probability of an event conditioned on knowledge of another event. Conditioning on an event means that the possible outcomes in that event form the set of possibilities or outcome space. We then calculate probabilities as normal within that restricted outcome space. Formally, this is written as $P(A \\mid B) = \\frac{|A|}{|B|}$, where everything to the left of the $\\mid$ is what we’re interested in knowing the probability of and everything to the right of the $\\mid$ is what we know to be true. dependence dependence When knowing the outcome of one random variable or event influences the probability of another, those variables or events are called dependent. This is denoted as $A \\not\\perp B$. When they do not influence each other, they are called independent. This is denoted as $A \\perp B$. marginal probability marginal probability A marginal probability is the probability of a random variable that has been calculated by summing over the possible values of one or more other random variables. joint probability joint probability The joint probability is the probability of all considered events. This corresponds to the intersection of the events. Bayes theorem Bayes Theorem Bayes Theorem is a rule for reversing the order that variables are conditioned – how to go from $P(A \\mid B)$ to $P(B \\mid A)$ generative process generative process A generative process defines the probabilities for possible outcomes according to an algorithm with random choices. probabilistic computing probabilistic computing Probabilistic computing is a programming language for specifying probabilistic models and built to calculate different probabilities according to this model in an efficient manner",
    "description": "set set A set is a collection of elements or members. Sets are defined by the elements they do or do not contain. The elements are listed with commas between them and “$\\{$” denotes the start of a set and “$\\}$” the end of a set. Note that the elements of a set are unique. event event An event is a set that is none, some, or all of the possible outcomes. cardinality cardinality The cardinality or size of a set is the number of elements it contains. If $A = \\{H, T\\}$, then the cardinality $A$ is $|A|=2$. probability probability The probability of an event $A$ relative to an outcome space $\\Omega$ is the ratio of their sizes or $\\frac{|A|}{|\\Omega|}$ random variable random variable A random variable is a function that maps from the set of possible outcomes to some set or space. The output or range of the function could be the set of outcomes again, a whole number based on the outcome (e.g., counting the number of Tonkatsu), or something more complex (e.g., the world’s friendship matrix, an 8-billion by 8-billion, binary matrix where $N$ where $N_{1,100}=1$ if person 1 is friends with person 100). Technically the output must be measurable. You shouldn’t worry about that distinction unless your random variable’s output gets really, really big (like continuous). We’ll talk more about probabilities over continuous random variables later. conditional probability conditional probability The conditional probability is the probability of an event conditioned on knowledge of another event. Conditioning on an event means that the possible outcomes in that event form the set of possibilities or outcome space. We then calculate probabilities as normal within that restricted outcome space. Formally, this is written as $P(A \\mid B) = \\frac{|A|}{|B|}$, where everything to the left of the $\\mid$ is what we’re interested in knowing the probability of and everything to the right of the $\\mid$ is what we know to be true. dependence dependence When knowing the outcome of one random variable or event influences the probability of another, those variables or events are called dependent. This is denoted as $A \\not\\perp B$. When they do not influence each other, they are called independent. This is denoted as $A \\perp B$. marginal probability marginal probability A marginal probability is the probability of a random variable that has been calculated by summing over the possible values of one or more other random variables. joint probability joint probability The joint probability is the probability of all considered events. This corresponds to the intersection of the events. Bayes theorem Bayes Theorem Bayes Theorem is a rule for reversing the order that variables are conditioned – how to go from $P(A \\mid B)$ to $P(B \\mid A)$ generative process generative process A generative process defines the probabilities for possible outcomes according to an algorithm with random choices. probabilistic computing probabilistic computing Probabilistic computing is a programming language for specifying probabilistic models and built to calculate different probabilities according to this model in an efficient manner",
    "tags": [],
    "title": "Glossary",
    "uri": "/intro/06_glossary/index.html"
  },
  {
    "breadcrumb": "Narrative Introduction to Probabilistic Computing \u003e A Narrative Introduction to Probability",
    "content": "Written by Joe Austerweil. Thank you to Kyana Burhite, Hongtao Hao, and the many students who took Human and Machine Learning over the years who have provided invaluable feedback on an early draft of this tutorial. Further thanks to the Japan Probabilistic Computing Consortium Association (JPCCA) for funding my ability to polish and publish this tutorial series.\nPlease reach out to Joe via email if you have any constructive feedback (anything from X could be more clear or this is a great resource I will share with my class).",
    "description": "Written by Joe Austerweil. Thank you to Kyana Burhite, Hongtao Hao, and the many students who took Human and Machine Learning over the years who have provided invaluable feedback on an early draft of this tutorial. Further thanks to the Japan Probabilistic Computing Consortium Association (JPCCA) for funding my ability to polish and publish this tutorial series.\nPlease reach out to Joe via email if you have any constructive feedback (anything from X could be more clear or this is a great resource I will share with my class).",
    "tags": [],
    "title": "Acknowledgements",
    "uri": "/intro/07_ack/index.html"
  },
  {
    "breadcrumb": "Narrative Introduction to Probabilistic Computing",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "Narrative Introduction to Probabilistic Computing",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
