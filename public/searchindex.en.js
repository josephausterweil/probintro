var relearn_searchindex = [
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing Tutorial",
    "content": "Welcome! This tutorial teaches probability theory through the story of Chibany, a philosophical cat who loves tonkatsu and wants to understand the likelihood of his daily meals. Along the way, youâ€™ll learn to think about probability using sets â€” a perspective that makes complex concepts intuitive and prepares you for probabilistic programming and advanced applications.\nWho is this for? Designers and social scientists curious about probability and data Anyone who wants to understand Bayesian thinking Learners preparing to use probabilistic programming, machine learning, or statistical tools The curious who want to develop clearer intuitions about uncertainty No prior math background required â€” just curiosity and willingness to think carefully!\nWhat youâ€™ll learn By following Chibanyâ€™s journey, youâ€™ll discover:\nHow to think about probability as â€œcounting possibilitiesâ€ â€” making abstract concepts concrete The connection between sets and probabilities â€” a foundation that generalizes everywhere Conditional probability, independence, and Bayesâ€™ rule â€” the core tools of probabilistic reasoning How to avoid common misconceptions â€” through classic puzzles that trip up even experts The foundation for probabilistic computing â€” the mental models that make code-based approaches make sense Why the set-based perspective? Most probability courses jump straight into formulas and rules. This tutorial takes a different approach: probability is counting.\nWhen you ask â€œWhatâ€™s the probability of getting tonkatsu?â€, youâ€™re really asking:\nWhat are all the possibilities? (the outcome space) Which possibilities include tonkatsu? (the event) Whatâ€™s the ratio? (count them!) This perspective makes conditional probability, Bayesâ€™ rule, and even complex models feel natural instead of mysterious.\nA Note on Progress This tutorial is currently in draft form as part of a planned series. Your feedback is welcome and appreciated! If you find concepts unclear or have suggestions for improvement, please reach out.\nReady to begin? Letâ€™s meet Chibany and start thinking about probability!\nacknowledgements | Next: What Youâ€™ll Learn â†’",
    "description": "Welcome! This tutorial teaches probability theory through the story of Chibany, a philosophical cat who loves tonkatsu and wants to understand the likelihood of his daily meals. Along the way, youâ€™ll learn to think about probability using sets â€” a perspective that makes complex concepts intuitive and prepares you for probabilistic programming and advanced applications.\nWho is this for? Designers and social scientists curious about probability and data Anyone who wants to understand Bayesian thinking Learners preparing to use probabilistic programming, machine learning, or statistical tools The curious who want to develop clearer intuitions about uncertainty No prior math background required â€” just curiosity and willingness to think carefully!",
    "tags": [],
    "title": "A Narrative Introduction to Probability",
    "uri": "/probintro/intro/index.html"
  },
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing TutorialÂ \u003eÂ Probabilistic Programming with GenJAX",
    "content": "What is Google Colab? Google Colab (short for â€œColaboratoryâ€) is a free online tool that lets you write and run Python code in your web browser. Think of it like Google Docs, but for code!\nWhy we love it for beginners:\nâœ… No installation needed âœ… No setup required âœ… Pre-installed with common libraries âœ… Free GPU access (for fast computation) âœ… Easy sharing with others âœ… Automatic saving to Google Drive Step 1: Open Your First Notebook Click this link to open the first GenJAX notebook:\nğŸ““ Open: Chapter 2 - Your First GenJAX Model\nWhat just happened? You opened a Jupyter notebook â€” an interactive document that mixes:\nText (explanations, like this) Code (that you can run) Visualizations (graphs and plots) Think of it as a lab notebook for probability experiments!\nStep 2: Make a Copy (So You Can Edit) When the notebook opens, youâ€™ll see a yellow banner that says â€œYou are using Colab in playground mode.â€\nClick: â€œCopy to Driveâ€ (top right, or in the banner)\nThis creates your own copy that you can edit and save!\nImportant! Without copying to Drive, your changes wonâ€™t be saved when you close the browser!\nStep 3: Understanding the Interface Let me show you around:\nThe Notebook Structure A Colab notebook has cells â€” little boxes that contain either:\nText cells (like this explanation) Code cells (Python code you can run) Running Code Cells See a cell with code like this?\n1 print(\"Hello, Chibany!\") To run it:\nClick on the cell Press Shift + Enter (or click the â–¶ï¸ play button on the left) Try it! You should see â€œHello, Chibany!â€ appear below the cell.\nInteractive Widgets Throughout the notebooks, youâ€™ll see sliders and controls that let you change values and instantly see updated results. Weâ€™ll use these to explore probability!\nStep 4: Install GenJAX The first code cell in each notebook will look like this:\n1 2 # Install GenJAX (this takes about 1-2 minutes the first time) !pip install genjax What this does:\nThe ! tells Colab â€œrun this as a system commandâ€ pip install means â€œdownload and installâ€ genjax is the library weâ€™re installing To run it:\nClick the cell Press Shift + Enter Wait for it to finish (youâ€™ll see progress messages) Tip You only need to install GenJAX once per session. If you come back later and restart the notebook, youâ€™ll need to run this cell again.\nStep 5: Import Required Libraries After installation, youâ€™ll typically see a cell like:\n1 2 3 4 5 import jax import jax.numpy as jnp import genjax from genjax import gen, bernoulli import matplotlib.pyplot as plt What this does:\nimport means â€œload this library so we can use itâ€ Weâ€™re loading JAX (for computation), GenJAX (for probability), and matplotlib (for plotting) Just run it! You donâ€™t need to understand every line. Think of it like turning on the lights before you start working.\nStep 6: Your First Code! Now youâ€™re ready to run real GenJAX code! In the next chapter, youâ€™ll:\nWrite a generative function for Chibanyâ€™s meals Generate thousands of random days Visualize the results Use sliders to change probabilities and see what happens Common Issues \u0026 Solutions â€œRuntime disconnectedâ€ Problem: Colab disconnects after ~90 minutes of inactivity Solution: Just reconnect and re-run the cells (start to finish)\nâ€œRestart Runtimeâ€ button Problem: Something went wrong and you need a fresh start Solution: Click Runtime â†’ Restart runtime â†’ Re-run all cells\nCode wonâ€™t run Problem: Cells need to be run in order Solution: Click Runtime â†’ Run all (to run from top to bottom)\nKeyboard Shortcuts (Optional) These can make you faster, but theyâ€™re optional:\nAction Shortcut Run current cell Shift + Enter Insert cell below Ctrl + M, B Delete cell Ctrl + M, D Comment/uncomment line Ctrl + / Quick Reference Card Save this for later:\nRunning Code:\nClick cell Shift + Enter Saving Work:\nAuto-saves to Google Drive (if you copied to Drive) Manual save: File â†’ Save Fresh Start:\nRuntime â†’ Restart runtime Getting Help:\nIn code cell, type ?function_name and run to see documentation Or just Google your question! Youâ€™re Ready! Now you have: âœ… Google Colab open âœ… Your own copy of the notebook âœ… GenJAX installed âœ… Basic understanding of the interface\nNext steps:\nChapter 1: Python Essentials â†’ - Learn just enough Python Chapter 2: Your First GenJAX Model â†’ - Jump right into coding! Details Pro tip: Keep this tab open as a reference while you work through the notebooks!\nâ† Previous: Introduction Next: Python Essentials â†’",
    "description": "What is Google Colab? Google Colab (short for â€œColaboratoryâ€) is a free online tool that lets you write and run Python code in your web browser. Think of it like Google Docs, but for code!\nWhy we love it for beginners:\nâœ… No installation needed âœ… No setup required âœ… Pre-installed with common libraries âœ… Free GPU access (for fast computation) âœ… Easy sharing with others âœ… Automatic saving to Google Drive Step 1: Open Your First Notebook Click this link to open the first GenJAX notebook:",
    "tags": [],
    "title": "Getting Started with Google Colab",
    "uri": "/probintro/genjax/00_getting_started/index.html"
  },
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing TutorialÂ \u003eÂ A Narrative Introduction to Probability",
    "content": "This tutorial will give you a solid foundation in probability theory through a set-based approach. By the end, youâ€™ll be able to reason clearly about uncertainty and be prepared for advanced topics in probabilistic computing and Bayesian inference.\nLearning Goals 1. Think About Probability Using Sets Learn to see probability as counting â€” a concrete, visual approach that makes abstract concepts intuitive.\nYouâ€™ll be able to:\nDefine outcome spaces (What could happen?) Identify events as subsets (What am I interested in?) Calculate probabilities by counting (Whatâ€™s the ratio?) 2. Understand Core Concepts Through Narrative Follow Chibanyâ€™s story to master the fundamental building blocks of probability theory.\nKey concepts:\nEvents and outcome spaces â€” The foundation of probabilistic reasoning Conditional probability â€” How learning changes whatâ€™s possible Independence â€” When things donâ€™t affect each other Bayesâ€™ rule â€” Updating beliefs with new information Random variables â€” Functions on outcomes Joint and marginal probabilities â€” Relationships between variables 3. Avoid Common Misconceptions Understand where intuition misleads us and develop tools to think more clearly.\nYouâ€™ll tackle:\nThe Taxicab Problem â€” How base rates matter Base-rate neglect â€” Why rare events stay rare even with evidence The difference between $P(A \\mid B)$ and $P(B \\mid A)$ â€” A source of endless confusion 4. Build Foundation for Probabilistic Programming Develop the mental models that make probabilistic computing approachable.\nYouâ€™ll understand:\nHow outcome spaces become generative processes Why conditioning is fundamental to inference How counting scales to computation The connection between sets and code 5. Develop Confidence for Further Learning After this tutorial, youâ€™ll be ready to explore more advanced topics with a solid conceptual foundation.\nNext steps could include:\nContinuous probability distributions Bayesian inference and conjugate models Mixture models and hierarchical models Probabilistic programming with GenJAX Machine learning and generative AI How the Tutorial Works Each chapter builds on previous concepts using Chibanyâ€™s daily meal situation:\nStart with simple counting (two meals, two options) Introduce probability as ratios Explore what happens when you learn something new Apply Bayesâ€™ rule to real problems Connect to computational approaches The progression is gradual â€” from concrete examples to general principles, from small outcome spaces to the logic that scales to complex models.\nWhat You Need No math prerequisites â€” We introduce everything from scratch Curiosity â€” Willingness to think through examples carefully Patience â€” Some concepts feel strange at first but become natural with practice Ready to meet Chibany and start learning?\nâ† Previous: Introduction | Next: Chibany is Hungry â†’",
    "description": "This tutorial will give you a solid foundation in probability theory through a set-based approach. By the end, youâ€™ll be able to reason clearly about uncertainty and be prepared for advanced topics in probabilistic computing and Bayesian inference.\nLearning Goals 1. Think About Probability Using Sets Learn to see probability as counting â€” a concrete, visual approach that makes abstract concepts intuitive.\nYouâ€™ll be able to:\nDefine outcome spaces (What could happen?) Identify events as subsets (What am I interested in?) Calculate probabilities by counting (Whatâ€™s the ratio?) 2. Understand Core Concepts Through Narrative Follow Chibanyâ€™s story to master the fundamental building blocks of probability theory.",
    "tags": [],
    "title": "What You'll Learn",
    "uri": "/probintro/intro/01_goals/index.html"
  },
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing Tutorial",
    "content": "From Probability Theory to Probabilistic Code In the previous tutorial, you learned to think about probability using sets and counting. Chibany showed you that probability questions are really about:\nWhatâ€™s possible? (Define the outcome space) What am I interested in? (Define the event) Count them! (Calculate the ratio) Now youâ€™ll learn to express those same ideas in code using GenJAX â€” a probabilistic programming language that lets computers do the counting for you!\nWhat is GenJAX? GenJAX is a probabilistic programming language that lets you:\nDefine generative processes â€” Write code that describes how outcomes are produced Perform inference â€” Find probable explanations given observations Leverage powerful computation â€” Run fast on GPUs for complex problems The best part? You already understand the concepts! GenJAX just translates what you learned about sets into code that computers can execute.\nNo Coding Experience? No Problem! This tutorial is designed for complete beginners to programming. Weâ€™ll:\nâœ… Use Google Colab â€” Run code in your browser, no installation needed âœ… Provide interactive notebooks â€” Adjust sliders and see results change instantly âœ… Teach just enough Python â€” Only what you need to understand the code âœ… Connect everything to sets â€” Every line of code relates to concepts you know\nYou wonâ€™t become a programmer from this tutorial â€” but youâ€™ll be able to use probabilistic programming tools to explore probability and build models!\nTwo Ways to Follow Along Option 1: Google Colab (Recommended for Beginners) Pros:\nâœ… No installation required âœ… Runs in your web browser âœ… Interactive widgets and visualizations âœ… Works on any computer (Windows, Mac, Linux, Chromebook) âœ… Free GPU access Cons:\nâš ï¸ Requires internet connection âš ï¸ Sessions timeout after inactivity Perfect for: Complete beginners, trying things out, classroom settings\nOption 2: Local Installation (Optional) Pros:\nâœ… Works offline âœ… Faster for large computations âœ… Full control over environment Cons:\nâš ï¸ Requires installation and setup âš ï¸ More technical troubleshooting needed Perfect for: Those comfortable with software installation, serious projects\nTutorial Structure Chapter 0: Getting Started Set up your environment (Google Colab or local installation)\nChapter 1: Python Essentials Just enough Python to read and run GenJAX code\nChapter 2: Your First Generative Function Chibanyâ€™s meals in code â€” from sets to simulation\nChapter 3: Understanding Traces What GenJAX records when programs run\nChapter 4: Conditioning and Observations How to ask â€œwhat if I know this happened?â€\nChapter 5: Inference in Action The taxicab problem, now solved with code!\nChapter 6: Building Your Own Models Go beyond Chibanyâ€™s meals\nLearning Philosophy You already know the concepts from the probability tutorial. This tutorial just shows you how to:\nExpress outcome spaces as generative functions Express events as filters on outcomes Let computers do the counting (simulation) Ask conditional probability questions (inference) Every chapter includes:\nğŸ“– Explanation connecting to set-based probability ğŸ’» Interactive Colab notebook ğŸ® Widgets to play with parameters ğŸ“Š Visualizations that update automatically âœ… Exercises with solutions What Youâ€™ll Build By the end of this tutorial, youâ€™ll be able to:\nWrite simple generative models in GenJAX Run simulations to approximate probabilities Perform inference given observations Visualize results with interactive plots Understand the connection between theory and code Youâ€™ll see how the taxicab problem, Chibanyâ€™s meals, and other examples from the probability tutorial can be solved computationally!\nPrerequisites Required:\nâœ… Completed â€œA Narrative Introduction to Probabilityâ€ âœ… Understand sets, events, and conditional probability âœ… Know what Chibany likes to eat ğŸ˜Š Not Required:\nâŒ Programming experience âŒ Python knowledge âŒ Software installation (if using Colab) Ready to Start? Letâ€™s set up your environment and write your first probabilistic program!\nChoose your path:\nChapter 0: Getting Started with Google Colab â†’ (Recommended) Chapter 0b: Local Installation â†’ (Optional) Or jump to Python basics:\nChapter 1: Python Essentials â†’ Learning Tip Donâ€™t try to memorize Python syntax! Focus on understanding:\nWhat the code is trying to do (the purpose) How it connects to probability concepts (the mapping) What happens when you run it (the result) You can always copy-paste and modify examples. Understanding beats memorization!",
    "description": "From Probability Theory to Probabilistic Code In the previous tutorial, you learned to think about probability using sets and counting. Chibany showed you that probability questions are really about:\nWhatâ€™s possible? (Define the outcome space) What am I interested in? (Define the event) Count them! (Calculate the ratio) Now youâ€™ll learn to express those same ideas in code using GenJAX â€” a probabilistic programming language that lets computers do the counting for you!",
    "tags": [],
    "title": "Probabilistic Programming with GenJAX",
    "uri": "/probintro/genjax/index.html"
  },
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing Tutorial",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Intro2",
    "uri": "/probintro/intro2/index.html"
  },
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing TutorialÂ \u003eÂ A Narrative Introduction to Probability",
    "content": "Chibany wakes up from dreaming of the delicious meals he will get later today. Twice per day, a student brings him a bento box with a meal as an offering to Chibany. One student brings him a bento box in the early afternoon for lunch and a different student brings him a bento box in the evening for dinner. The meal is either a Hamburger or a Tonkatsu (pork cutlet) . To keep track of his meal possibilities, he lists out the four possibilities:\nblock-beta block columns 2 a[\"H(amburger) H(amburger)\"] b[\"H(amburger) T(onkatsu)\"] c[\"T(onkatsu) H(amburger)\"] d[\"T(onkatsu) T(onkatsu)\"] end Sets This forms a set of four elements. A set is a collection of elements or members. In this case, an element is defined by the two meals given to Chibany that day. Sets are defined by the elements they do or do not contain. The elements are listed with commas between them and â€œ$\\{$â€ denotes the start of a set and â€œ$\\}$â€ the end of a set.\nOutcome Space In the context of probability theory, the basic elements of what can occur are called outcomes. Outcomes are the fundamental building blocks that probabilities are built from. As they are fundamental, the Greek letter $\\Omega$ is frequently used to refer to this set of possible outcomes. Diligently noting his daily offerings, Chibany defines $\\Omega = \\{HH, HT, TH, TT \\}$. The first letter defines his lunch offering, and the second letter defines his dinner offering. He notes that $H$ now always refers to hamburgers and $T$ to tonkatsu.\nWhy Sets? Sets are perfect for probability because they let us visualize and count possibilities. Every probability question becomes:\nWhatâ€™s possible? (Define the outcome space) What am I interested in? (Define the event) Count both! (Calculate the ratio) This makes probability concrete instead of abstract.\nA Note on Unique Elements Technically, the elements of a set are unique. So, if Chibany writes down getting a pair of hamburgers twice and a hamburger and a tonkatsu ($\\{HH, HH, HT\\}$), heâ€™s gotten the same set of possibilities as if he only got one pair of hamburgers and a hamburger and tonkatsu ($\\{HH, HT\\}$). In other words, $\\{HH, HH, HT\\} = \\{HH, HT\\}$.\nThink of it like a list where duplicates automatically disappear â€” only whatâ€™s different matters.\nFor Chibanyâ€™s meals Each element in $\\Omega = \\{HH, HT, TH, TT\\}$ is already unique because the position matters (first meal vs. second meal). $HT$ â‰  $TH$ â€” getting tonkatsu for lunch is different from getting it for dinner!\nChibany is skeptical, but will try to keep it in mind. It can be confusing!\nPossibilities vs. Events So far, we have discussed sets, possible outcomes and the set of all possible outcomes $\\Omega$. Chibany is interested in the set of possible meals that include Tonkatsu. What is this set?\n${HT, TH, TT}$\nThis is an example of an event. Technically, an event is a set that contains none, some, or all of the possible outcomes.\nEvents are Subsets Any event $A$ is a subset of the outcome space $\\Omega$. This means:\nEvery element in $A$ is also in $\\Omega$ $A$ could be empty ($\\{\\}$ â€” nothing happens) $A$ could be all of $\\Omega$ (something definitely happens) $A$ could be anything in between For Chibanyâ€™s â€œcontains tonkatsuâ€ event: $A = \\{HT, TH, TT\\} \\subseteq \\Omega$\nQuick Check Is $\\Omega$ an event?\nsolution Yes â€” it is the event that contains all possible outcomes. This is sometimes called the certain event because something from $\\Omega$ must happen. Is $\\Omega$ the set of all possible events?\nsolution No â€” $\\Omega$ is one particular event (the event containing everything). The set of all possible events is much larger! What is the set of all possible events for Chibanyâ€™s situation?\nsolution $\\{ \\{ \\}, \\{ HH \\}, \\{ HT\\}, \\{TH \\}, \\{TT\\}, \\{HH,HT\\}, \\{HH,TH\\}, \\{HH,TT\\}, \\{HT, TH\\}, \\{HT, TT \\}, \\{TH, TT\\}, \\{HH, HT, TH\\}, \\{HH, HT, TT \\}, \\{HH, TH, TT\\}, \\{HT, TH, TT\\}, \\{HH, HT, TH, TT\\} \\}$\nNote that $\\{ \\}$ is called the empty or null set and is a special set that contains no elements. Itâ€™s the impossible event â€” nothing happens.\nCounting tip: For an outcome space with $n$ outcomes, there are $2^n$ possible events. Here: $2^4 = 16$ events.\nWhat Weâ€™ve Learned In this chapter, Chibany introduced us to the fundamental building blocks of probability:\nSets â€” Collections of distinct elements Outcome spaces ($\\Omega$) â€” All possible outcomes Events â€” Subsets of outcomes weâ€™re interested in Next, weâ€™ll see how to turn these into actual probabilities!\nâ† Previous: Goals Next: Probability and Counting â†’",
    "description": "Chibany wakes up from dreaming of the delicious meals he will get later today. Twice per day, a student brings him a bento box with a meal as an offering to Chibany. One student brings him a bento box in the early afternoon for lunch and a different student brings him a bento box in the evening for dinner. The meal is either a Hamburger or a Tonkatsu (pork cutlet) . To keep track of his meal possibilities, he lists out the four possibilities:",
    "tags": [],
    "title": "Chibany is hungry",
    "uri": "/probintro/intro/02_hungry/index.html"
  },
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing TutorialÂ \u003eÂ Probabilistic Programming with GenJAX",
    "content": "You Donâ€™t Need to Become a Programmer! This chapter teaches you just enough Python to read and run GenJAX code. You wonâ€™t become a software developer, but youâ€™ll be able to:\nUnderstand what the code is doing Modify values to experiment Run examples and see results Think of it like learning enough Italian to order food in a restaurant â€” you donâ€™t need fluency, just practical knowledge!\n1. Variables: Giving Names to Things In Python, we give names to values so we can use them later.\n1 2 probability_hamburger = 0.5 probability_tonkatsu = 0.5 What this means: â€œRemember these numbers and call them by these namesâ€\nConnection to probability: Just like we wrote $P(H) = 0.5$ in math, weâ€™re storing that value.\nTry it:\n1 2 3 4 x = 10 y = 20 result = x + y print(result) # Shows: 30 The # Symbol Anything after # is a comment â€” a note for humans, ignored by the computer.\n1 2 # This is a comment x = 5 # Comments can go after code too 2. Functions: Recipes for Actions A function is a named set of instructions. Think of it like a recipe.\n1 2 3 4 5 def greet_chibany(): print(\"Hello, Chibany!\") print(\"Time for tonkatsu!\") greet_chibany() # \"Calls\" the function (runs the recipe) Output:\nHello, Chibany! Time for tonkatsu! Functions with Inputs (Parameters) Functions can take inputs (called parameters):\n1 2 3 4 5 def greet_cat(name): print(f\"Hello, {name}!\") greet_cat(\"Chibany\") # Output: Hello, Chibany! greet_cat(\"Felix\") # Output: Hello, Felix! The f before the string lets you put variables inside {} inside the text.\nFunctions with Outputs (Return Values) Functions can return values:\n1 2 3 4 5 def add_numbers(a, b): result = a + b return result total = add_numbers(5, 3) # total is now 8 Connection to probability: Remember how $f(\\omega)$ is a function that takes an outcome and returns a number? Same idea!\n3. Lists: Collections of Things A list is like a shopping list â€” an ordered collection of items.\n1 meals = [\"HH\", \"HT\", \"TH\", \"TT\"] Connection to sets: This is like $\\Omega = \\{HH, HT, TH, TT\\}$ but with ordering!\nAccessing Items 1 2 3 4 meals = [\"HH\", \"HT\", \"TH\", \"TT\"] first_meal = meals[0] # \"HH\" (Python counts from 0!) second_meal = meals[1] # \"HT\" Python Counts from Zero! The first item is [0], the second is [1], etc.\nThis trips up everyone at first. Just remember: Python is a bit quirky!\nHow Many Items? 1 2 meals = [\"HH\", \"HT\", \"TH\", \"TT\"] count = len(meals) # 4 Connection: This is like $|\\Omega|$ (cardinality)!\n4. Loops: Doing Things Repeatedly A for loop repeats actions:\n1 2 for meal in [\"HH\", \"HT\", \"TH\", \"TT\"]: print(meal) Output:\nHH HT TH TT How to read it: â€œFor each meal in this list, print the mealâ€\nCounting Loops 1 2 for i in range(5): print(f\"Day {i}\") Output:\nDay 0 Day 1 Day 2 Day 3 Day 4 Connection: If we wanted to simulate 10,000 days, weâ€™d use range(10000)!\n5. Conditionals: Making Decisions An if statement lets code make choices:\n1 2 3 4 5 6 meal = \"TT\" if \"T\" in meal: print(\"Contains tonkatsu!\") else: print(\"No tonkatsu today :(\") How to read it: â€œIf T is in the meal, do this. Otherwise, do that.â€\nMultiple Conditions 1 2 3 4 5 6 7 8 tonkatsu_count = 2 if tonkatsu_count == 2: print(\"Two tonkatsus!\") elif tonkatsu_count == 1: print(\"One tonkatsu!\") else: print(\"No tonkatsu!\") Note:\n== means â€œequalsâ€ (comparison) = means â€œassignâ€ (giving a value) 6. Decorators: Adding Special Powers A decorator adds capabilities to a function. In GenJAX, we use @gen:\n1 2 3 @gen def my_function(): # ... code ... What @gen does: Tells GenJAX â€œthis is a generative function â€” please track all the random choices!â€\nYou donâ€™t need to fully understand decorators. Just know:\nThey go right before function definitions They modify how the function behaves In GenJAX, @gen is essential for probabilistic models 7. The @ Symbol in GenJAX (Addressing) In GenJAX, we use @ to name random choices:\n1 lunch = bernoulli(0.5) @ \"lunch\" How to read it: â€œGenerate a random Bernoulli value with probability 0.5, and call this choice â€™lunch'â€\nConnection to probability: This is like saying â€œlet $L$ be the random variable for lunchâ€\n8. Libraries and Imports Libraries are collections of pre-written code we can use:\n1 2 3 import jax import matplotlib.pyplot as plt from genjax import gen, bernoulli What this means:\nimport jax â€” Load the JAX library (for computation) import matplotlib.pyplot as plt â€” Load plotting tools, call them plt from genjax import gen, bernoulli â€” From GenJAX, load these specific tools You donâ€™t need to memorize these. Just run the import cells at the start of each notebook!\n9. Calling Methods with Dot Notation Sometimes we call functions â€œonâ€ an object:\n1 2 trace = model.simulate(key, args) choices = trace.get_choices() How to read it: â€œCall the simulate function that belongs to modelâ€\nThe . means â€œbelonging toâ€ or â€œpart ofâ€.\n10. Comments and Documentation Single-line Comments 1 2 # This is a comment x = 5 # This is also a comment Multi-line Comments (Docstrings) 1 2 3 4 5 6 def my_function(): \"\"\" This is a docstring. It explains what the function does. \"\"\" # ... code ... Why they matter: They help you understand what code does!\nQuick Reference: Reading GenJAX Code Hereâ€™s a typical GenJAX function broken down:\n1 2 3 4 5 6 7 8 9 10 11 12 @gen # Decorator: makes this a generative function def chibany_meals(): # Function name \"\"\"Generate one day of meals.\"\"\" # Docstring: what it does # Random choice: lunch lunch = bernoulli(0.5) @ \"lunch\" # @ names the choice # Random choice: dinner dinner = bernoulli(0.5) @ \"dinner\" # Another named choice # Return both meals as a pair return (lunch, dinner) # Return value To read GenJAX code:\nFind the @gen â€” itâ€™s a generative function Read the docstring â€” what does it do? Look for @ symbols â€” those are the random choices See what it returns â€” thatâ€™s the outcome Practice: Can You Read This? 1 2 3 4 5 6 7 8 9 10 11 @gen def coin_flips(n): \"\"\"Flip a fair coin n times.\"\"\" heads_count = 0 for i in range(n): flip = bernoulli(0.5) @ f\"flip_{i}\" if flip == 1: heads_count = heads_count + 1 return heads_count What does this do? Line by line:\n@gen â€” This is a generative function def coin_flips(n): â€” Takes a number n as input heads_count = 0 â€” Start counting at 0 for i in range(n): â€” Repeat n times flip = bernoulli(0.5) @ f\"flip_{i}\" â€” Flip a fair coin, name it â€œflip_0â€, â€œflip_1â€, etc. if flip == 1: â€” If itâ€™s heads (1) heads_count = heads_count + 1 â€” Add one to the count return heads_count â€” Return how many heads we got What it does: Flips a coin n times and counts the heads!\nConnection: This is like the binomial distribution from probability theory.\nWhat You Donâ€™t Need to Learn Donâ€™t worry about:\nâŒ Object-oriented programming âŒ Advanced data structures âŒ File I/O âŒ Error handling âŒ Most of Pythonâ€™s features! Focus on:\nâœ… Reading code to understand what it does âœ… Running code cells in notebooks âœ… Changing parameter values to experiment âœ… Understanding the connection to probability Tips for Success 1. You Donâ€™t Need to Memorize Keep this chapter open as a reference. When you see something in GenJAX code you donâ€™t recognize, come back here!\n2. Run Code to Understand It Donâ€™t just read â€” run the code! Seeing output makes everything clearer.\n3. Experiment! Try changing values:\nWhat happens if you change 0.5 to 0.8? What if you change the number of simulations? Break things and see what errors you get! 4. Ask â€œWhat is This Doing?â€ Not â€œHow does this work?â€ but â€œWhat is this trying to accomplish?â€\nReady for GenJAX! You now know enough Python to:\nâœ… Read GenJAX code âœ… Understand what generative functions do âœ… Run examples in Colab âœ… Modify values to experiment Next up: Letâ€™s write your first generative function!\nâ† Previous: Getting Started Next: Your First GenJAX Model â†’",
    "description": "You Donâ€™t Need to Become a Programmer! This chapter teaches you just enough Python to read and run GenJAX code. You wonâ€™t become a software developer, but youâ€™ll be able to:\nUnderstand what the code is doing Modify values to experiment Run examples and see results Think of it like learning enough Italian to order food in a restaurant â€” you donâ€™t need fluency, just practical knowledge!\n1. Variables: Giving Names to Things In Python, we give names to values so we can use them later.",
    "tags": [],
    "title": "Python Essentials for GenJAX",
    "uri": "/probintro/genjax/01_python_basics/index.html"
  },
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing TutorialÂ \u003eÂ A Narrative Introduction to Probability",
    "content": "One goal of this tutorial is to show you that probability is counting. When every possibility is equally likely, probability is defined as the relative number of possibilities in each set. When possibilities are not equally likely, it is only slightly more complicated. Rather than each possibility counting one towards the size of a set it is in, you count the possibility according to its relative weight.\nCounting The basic operation that we use to define probabilities is counting the number of elements in a set. If $A$ is a set, then $|A|$ is the cardinality or size of the set.\nMath Notation: Cardinality We write the size of a set using vertical bars:\n$|A|$ means â€œthe size of set $A$â€ or â€œhow many elements are in $A$â€ Example: $|\\{H, T\\}| = 2$ Think of it like: â€œHow many elements are between these bars?â€\nFor example, the set of Chibanyâ€™s lunch options is $\\{H,T\\}$. Counting the number of elements determines its size, which is $\\left|\\{H, T\\} \\right| = 2$. The set of Chibanyâ€™s meal offerings for a day, $\\Omega = \\{HH, HT, TH, TT \\}$. There are four possibilities, so its size $|\\Omega|$ is $4$.\nChibany is still hungryâ€¦ and desires Tonkatsu Chibany is still hungry and wondering what his meal possibilities are for the day. He wonders, what is the probability that students appease him today by giving him Tonkatsu?\nTo make this calculation, Chibany lists out the outcome space $\\Omega$ again. He then forms the event â€œTonkatsu offering todayâ€. He defines the set of possible outcomes with a Tonkatsu as $A = \\{HT, TH, TT\\}$ to encode the event. He highlights those in red. Chibany thinks â€œwowâ€¦ three of the four possible outcomes are red. Fortune must favor me today, right?â€\nblock-beta block columns 2 a[\"H(amburger) H(amburger)\"] b[\"H(amburger) T(onkastu)\"] c[\"T(onkastu) H(amburger)\"] d[\"T(onkastu) T(onkastu)\"] end style b stroke: #f33, stroke-width:4px style c stroke: #f33, stroke-width:4px style d stroke: #f33, stroke-width:4px Yes, Chibany, it does as it always should. Your chance of getting Tonkatsu is three out of four or 0.75. He calculated the probability exactly as he should!\nProbability as Counting The probability of an event $A$ is $\\frac{|A|}{|\\Omega|}$. It is written as $P(A)$. In the prior example, $|A| = | \\{HT, TH, TT\\} | = 3$ and $|\\Omega| = | \\{HH, HT, TH, TT\\}| = 4$ have three and four elements, respectively.\nThe Core Idea Probability = Counting\n$$P(A) = \\frac{|A|}{|\\Omega|} = \\frac{\\text{number of outcomes in event}}{\\text{total number of possible outcomes}}$$\nThatâ€™s it! Everything else builds from this foundation.\nVisualizing Probability as Counting Think of probability as shading parts of the outcome space:\nblock-beta block columns 2 a[\"HH\u003cbr/\u003eâŒ\"] b[\"HT\u003cbr/\u003eâœ“\"] c[\"TH\u003cbr/\u003eâœ“\"] d[\"TT\u003cbr/\u003eâœ“\"] end style b fill:#faa style c fill:#faa style d fill:#faa Red outcomes = Event $A$ (contains Tonkatsu) All outcomes = Outcome space $\\Omega$\n$$P(A) = \\frac{\\text{shaded outcomes}}{\\text{total outcomes}} = \\frac{3}{4} = 0.75$$\nWhen Outcomes Arenâ€™t Equally Likely Note that if the possible outcomes were not equally likely, we would sum their individual probabilities to calculate the cardinality. But everything works in the same way â€” the probability of the event is the total â€œsizeâ€ or â€œweightâ€ of the possible outcomes in the event as compared to the total size or weight of all possible outcomes. Weâ€™ll see an example of this later!\nAnother Example What is the probability that Chibany gets Tonkatsu for his first offering? Well the possible outcomes with Tonkatsu for lunch are $\\{TH, TT\\}$. There are four possible outcomes for his offerings $\\Omega = \\{HH,HT, TH, TT\\}$. So the probability he gets Tonkatsu for his first offering is $|\\{TH, TT\\}|/|\\{HH,HT, TH, TT\\}| = 2/4=1/2$. Chibany draws the following table to illustrate his counting:\nblock-beta block columns 2 a[\"H(amburger) H(amburger)\"] b[\"H(amburger) T(onkastu)\"] c[\"T(onkastu) H(amburger)\"] d[\"T(onkastu) T(onkastu)\"] end style c stroke: #f33, stroke-width:4px style d stroke: #f33, stroke-width:4px Random Variables Chibany wants to knowâ€¦ how much Tonkatsu? Chibany wants to know how much Tonkatsu he gets each day. To do so, he converts each possibility to a whole number: the number of Tonkatsu in that possibility. He calls this a function $f : \\Omega \\rightarrow \\{0, 1, 2, \\ldots\\}$, meaning it takes a possibility out of the outcome space and maps it (changes it into) a number.\nFunctions and Mappings A function $f : \\Omega \\rightarrow \\{0, 1, 2, \\ldots\\}$ is like a machine:\nInput: An outcome from $\\Omega$ Process: Apply the rule (count the tonkatsu!) Output: A number The arrow â€œ$\\rightarrow$â€ means â€œmaps toâ€ or â€œproducesâ€.\nHe notes: mapping every possibility to a whole number is like making each whole number an event! His Tonkatsu counter $f$ is defined as $f(HH) = 0$, $f(HT) = 1$, $f(TH)=1$, and $f(TT) = 2$. Chibany defined his first random variable.\nblock-beta block columns 2 a[\"HH: 0\"] space b[\"HT: 1\"] c[\"TH: 1\"] d[\"TT: 2\"] space end style b stroke: #44c, stroke-width:4px style c stroke: #44c, stroke-width:4px style d stroke: #f33, stroke-width:4px Why â€˜Randomâ€™ Variable? Itâ€™s called a random variable because:\nThe value depends on which outcome occurs (random) Itâ€™s a variable that takes different values for different outcomes But really, itâ€™s just a function on outcomes!\nCalculating Probabilities with Random Variables What is the probability of having two tonkatsus? We count the number of outcomes with two tonkatsus ($\\{TT\\}$ highlighted in red) and divide by the number of possible outcomes ($|\\Omega|=4$). So, it is 1 out of 4 or 1/4.\nWhat about the probability of having exactly one tonkatsu? We count the number of outcomes with exactly one tonkatsu ($\\{HT, TH\\}$ highlighted in blue) and divide by the number of possible outcomes ($|\\Omega|=4$). So it is 2/4 or 1/2.\nRandom Variables Create Events When we ask â€œWhatâ€™s $P(f = 1)$?â€, weâ€™re really asking:\nWhich outcomes give $f=1$? (Define the event) Count them! (Calculate the probability) Event: $\\{\\omega \\in \\Omega : f(\\omega) = 1\\} = \\{HT, TH\\}$ Probability: $P(f=1) = 2/4 = 1/2$\nWhat Weâ€™ve Learned In this chapter, we discovered:\nProbability is counting â€” $P(A) = |A|/|\\Omega|$ Cardinality â€” Using $|A|$ to denote the size of a set Random variables â€” Functions that map outcomes to numbers How random variables create events â€” Each value corresponds to a subset of $\\Omega$ Next, weâ€™ll explore what happens when we learn new information â€” conditional probability!\nâ† Previous: Chibany is Hungry Next: Conditional Probability â†’",
    "description": "One goal of this tutorial is to show you that probability is counting. When every possibility is equally likely, probability is defined as the relative number of possibilities in each set. When possibilities are not equally likely, it is only slightly more complicated. Rather than each possibility counting one towards the size of a set it is in, you count the possibility according to its relative weight.\nCounting The basic operation that we use to define probabilities is counting the number of elements in a set. If $A$ is a set, then $|A|$ is the cardinality or size of the set.",
    "tags": [],
    "title": "Probability and Counting",
    "uri": "/probintro/intro/03_prob_count/index.html"
  },
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing TutorialÂ \u003eÂ Probabilistic Programming with GenJAX",
    "content": "From Sets to Simulation Remember Chibanyâ€™s daily meals? We listed out the outcome space $\\Omega = \\{HH, HT, TH, TT\\}$ and counted possibilities.\nNow weâ€™ll teach a computer to generate those outcomes instead!\nThe Generative Process Each day:\nLunch arrives â€” randomly H or T (equal probability) Dinner arrives â€” randomly H or T (equal probability) Record the day â€” the pair of meals In GenJAX, we express this as a generative function.\nYour First Generative Function Hereâ€™s Chibanyâ€™s meals in GenJAX:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import jax from genjax import gen, bernoulli @gen def chibany_day(): \"\"\"Generate one day of Chibany's meals.\"\"\" # Lunch: flip a coin (0=Hamburger, 1=Tonkatsu) lunch_is_tonkatsu = bernoulli(0.5) @ \"lunch\" # Dinner: flip another coin dinner_is_tonkatsu = bernoulli(0.5) @ \"dinner\" # Return the pair return (lunch_is_tonkatsu, dinner_is_tonkatsu) Breaking It Down Line 1: @gen\nTells GenJAX: â€œThis is a generative functionâ€ GenJAX will track all random choices Line 2-3: Function definition\ndef chibany_day(): defines the function The docstring explains what it does Line 6: First random choice\n1 lunch_is_tonkatsu = bernoulli(0.5) @ \"lunch\" bernoulli(0.5) â€” Flip a fair coin (50% chance of 1, 50% chance of 0) @ \"lunch\" â€” Name this random choice â€œlunchâ€ Store the result in lunch_is_tonkatsu Line 9: Second random choice\n1 dinner_is_tonkatsu = bernoulli(0.5) @ \"dinner\" Another coin flip, named â€œdinnerâ€ Line 12: Return value\n1 return (lunch_is_tonkatsu, dinner_is_tonkatsu) Returns a tuple (pair) of the two values This is like one outcome from $\\Omega$! Running the Function Generating One Day 1 2 3 4 5 6 7 8 9 # Create a random key (JAX requirement for randomness) key = jax.random.key(42) # Generate one day trace = chibany_day.simulate(key, ()) # What happened? meals = trace.get_retval() print(f\"Today's meals: {meals}\") Output (example):\nToday's meals: (0, 1) This means: Hamburger for lunch (0), Tonkatsu for dinner (1) â€” or in our notation: $HT$!\nWhatâ€™s a â€˜keyâ€™? JAX uses random keys to control randomness. Think of it like a seed â€” the same key always gives the same â€œrandomâ€ results, which helps with reproducibility.\nDonâ€™t worry about the details! Just know:\nCreate a key with jax.random.key(some_number) Split it for multiple uses with jax.random.split(key, n) Accessing the Random Choices 1 2 3 4 5 # Get all the random choices made choices = trace.get_choices() print(f\"Lunch was tonkatsu: {choices['lunch']}\") print(f\"Dinner was tonkatsu: {choices['dinner']}\") Output (for the trace above):\nLunch was tonkatsu: 0 Dinner was tonkatsu: 1 Simulating Many Days Now letâ€™s generate 10,000 days!\n1 2 3 4 5 6 7 8 9 10 # Generate 10,000 random keys keys = jax.random.split(key, 10000) # Run the generative function for each key def run_one_day(k): trace = chibany_day.simulate(k, ()) return trace.get_retval() # Use JAX's vmap for parallel execution days = jax.vmap(run_one_day)(keys) Whatâ€™s vmap? vmap stands for â€œvectorized mapâ€ â€” it runs a function many times in parallel, which is very fast!\nThink of it like: â€œDo this 10,000 times, but do them all at once instead of one-by-oneâ€\nCounting Outcomes Now we have 10,000 days. Letâ€™s count how many have at least one tonkatsu:\n1 2 3 4 5 6 7 8 9 10 11 12 13 import jax.numpy as jnp # Check if either meal is tonkatsu (1) has_tonkatsu = jnp.logical_or(days[:, 0], days[:, 1]) # Count how many days have tonkatsu count_with_tonkatsu = jnp.sum(has_tonkatsu) # Calculate probability prob_tonkatsu = jnp.mean(has_tonkatsu) print(f\"Days with tonkatsu: {count_with_tonkatsu} out of 10000\") print(f\"P(at least one tonkatsu) â‰ˆ {prob_tonkatsu:.3f}\") Output (example):\nDays with tonkatsu: 7489 out of 10000 P(at least one tonkatsu) â‰ˆ 0.749 Remember from the probability tutorial: The exact answer is $3/4 = 0.75$!\nWith 10,000 simulations, we got very close: $0.749 \\approx 0.75$\nVisualizing the Results Letâ€™s make a bar chart showing all four outcomes:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import matplotlib.pyplot as plt # Count each outcome HH = jnp.sum((days[:, 0] == 0) \u0026 (days[:, 1] == 0)) HT = jnp.sum((days[:, 0] == 0) \u0026 (days[:, 1] == 1)) TH = jnp.sum((days[:, 0] == 1) \u0026 (days[:, 1] == 0)) TT = jnp.sum((days[:, 0] == 1) \u0026 (days[:, 1] == 1)) # Create bar chart outcomes = ['HH', 'HT', 'TH', 'TT'] counts = [HH, HT, TH, TT] plt.figure(figsize=(8, 5)) plt.bar(outcomes, counts, color=['#ff6b6b', '#4ecdc4', '#45b7d1', '#f7dc6f']) plt.xlabel('Outcome') plt.ylabel('Count (out of 10,000)') plt.title(\"Chibany's Meals: 10,000 Simulated Days\") plt.axhline(y=2500, color='gray', linestyle='--', label='Expected (2500 each)') plt.legend() plt.show() What youâ€™ll see: Four bars of roughly equal height (around 2500 each), matching our theoretical expectation of $1/4$ for each outcome!\nInteractive Exploration (In the Colab Notebook!) The companion notebook has interactive widgets so you can:\nSlider 1: Probability of Tonkatsu for Lunch Move the slider from 0.0 to 1.0 See how the distribution changes! Slider 2: Probability of Tonkatsu for Dinner Make dinner independent from lunch Or make tonkatsu more/less likely at different meals Slider 3: Number of Simulations Try 100, 1,000, 10,000, or even 100,000 simulations See how the estimate gets more accurate with more simulations The chart updates automatically as you move the sliders!\nTry This! In the Colab notebook:\nSet lunch probability to 0.8 (80% tonkatsu) Set dinner probability to 0.2 (20% tonkatsu) Run 10,000 simulations What do you notice about the distribution? Answer: Outcomes with tonkatsu for lunch (TH, TT) are much more common than those without (HH, HT)!\nConnection to Set-Based Probability Letâ€™s connect this back to what you learned:\nSet-Based Concept GenJAX Equivalent Outcome space $\\Omega$ Running simulate() many times One outcome $\\omega$ One call to simulate() Event $A \\subseteq \\Omega$ Filtering simulations $|A|$ (count elements) jnp.sum(condition) $P(A) = |A|/|\\Omega|$ jnp.mean(condition) Example:\nSet-based:\nEvent: â€œAt least one tonkatsuâ€ = $\\{HT, TH, TT\\}$ Probability: $|\\{HT, TH, TT\\}| / |\\{HH, HT, TH, TT\\}| = 3/4$ GenJAX:\n1 2 has_tonkatsu = (days[:, 0] == 1) | (days[:, 1] == 1) prob = jnp.mean(has_tonkatsu) # â‰ˆ 0.75 Itâ€™s the same concept! Just computed instead of counted by hand.\nUnderstanding Traces When you run chibany_day.simulate(key, ()), GenJAX creates a trace that records:\nArguments â€” What inputs were provided (none in this case) Random choices â€” All the random decisions made, with their names Return value â€” The final result 1 2 3 4 5 6 trace = chibany_day.simulate(key, ()) # Access different parts print(f\"Return value: {trace.get_retval()}\") print(f\"Choices: {trace.get_choices()}\") print(f\"Log probability: {trace.get_score()}\") Why Track Everything? Tracking all random choices is essential for inference â€” when we want to ask â€œgiven I observed this, whatâ€™s probable?â€\nWeâ€™ll see this in action in Chapter 4!\nExercises Try these in the Colab notebook:\nExercise 1: Different Probabilities Modify the code so:\nLunch is 70% likely to be tonkatsu Dinner is 30% likely to be tonkatsu Hint: Change the bernoulli(0.5) values!\nSolution 1 2 3 4 5 @gen def chibany_day_weighted(): lunch_is_tonkatsu = bernoulli(0.7) @ \"lunch\" dinner_is_tonkatsu = bernoulli(0.3) @ \"dinner\" return (lunch_is_tonkatsu, dinner_is_tonkatsu) Exercise 2: Counting Tonkatsu Write code to count how many tonkatsu Chibany gets across all simulated days (not just which days have tonkatsu, but the total count).\nHint: Add up days[:, 0] + days[:, 1]\nSolution 1 2 3 4 5 total_tonkatsu = jnp.sum(days[:, 0]) + jnp.sum(days[:, 1]) avg_per_day = total_tonkatsu / len(days) print(f\"Total tonkatsu: {total_tonkatsu}\") print(f\"Average per day: {avg_per_day:.2f}\") With equal probabilities (0.5 each), you should get close to 1.0 tonkatsu per day on average!\nExercise 3: Three Meals? Extend the model to include breakfast! Now Chibany gets three meals per day.\nSolution 1 2 3 4 5 6 @gen def chibany_three_meals(): breakfast_is_tonkatsu = bernoulli(0.5) @ \"breakfast\" lunch_is_tonkatsu = bernoulli(0.5) @ \"lunch\" dinner_is_tonkatsu = bernoulli(0.5) @ \"dinner\" return (breakfast_is_tonkatsu, lunch_is_tonkatsu, dinner_is_tonkatsu) Now the outcome space has $2^3 = 8$ possible outcomes!\nWhat Youâ€™ve Learned In this chapter, you:\nâœ… Wrote your first generative function âœ… Simulated thousands of random outcomes âœ… Calculated probabilities through counting âœ… Visualized distributions âœ… Understood the connection between sets and simulation âœ… Learned about traces and random choices\nThe key insight: Generative functions let computers do what you did by hand with sets â€” but now you can handle millions of possibilities!\nNext Steps Now that you can generate outcomes, the next question is:\nWhat if I observe something? How do I update my beliefs?\nThatâ€™s inference, and itâ€™s where GenJAX really shines!\nâ† Previous: Python Essentials Next: Understanding Traces â†’",
    "description": "From Sets to Simulation Remember Chibanyâ€™s daily meals? We listed out the outcome space $\\Omega = \\{HH, HT, TH, TT\\}$ and counted possibilities.\nNow weâ€™ll teach a computer to generate those outcomes instead!\nThe Generative Process Each day:\nLunch arrives â€” randomly H or T (equal probability) Dinner arrives â€” randomly H or T (equal probability) Record the day â€” the pair of meals In GenJAX, we express this as a generative function.",
    "tags": [],
    "title": "Your First GenJAX Model",
    "uri": "/probintro/genjax/02_first_model/index.html"
  },
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing TutorialÂ \u003eÂ A Narrative Introduction to Probability",
    "content": "Chibany wants a tonkatsu dinner A graduate of Chiba Tech, Tanaka-san, visits Chibany one day and tells Chibany that he knows that there will be at least one tonkatsu in tomorrowâ€™s offering. Chibany is excited. He wants to know how likely it is that the second meal is a Tonkatsu. He quizzes Tanaka-san. Tanaka-san says itâ€™s just as likely as before, so it should be 1/2. Chibany disagrees. Chibany says â€œI learned something because I know I will get at least one tonkatsuâ€. Also, Chibany is an optimist and deserves to have all the tonkatsu. Whoâ€™s right!? Letâ€™s check the chartâ€¦\nblock-beta block columns 1 a[\"HH\"] block:group1 b[\"HT\"] c[\"TH\"] d[\"TT\"] space end end style a fill:#999, text-decoration:line-through style group1 stroke: #33f, stroke-width: 6px style c stroke: #f33, stroke-width: 4px style d stroke: #f33, stroke-width: 4px In the case where there is at least one tonkatsu, the space of possible outcomes is $\\{HT, TH, TT\\}$, which is outlined in blue. The event of interest for Chibany is outlined in red. It turns out Chibany is right! There is a two in three chance that he gets a tonkatsu dinner. Thatâ€™s larger than one in two.\nChibany kindly reminds Tanaka-san that you never stop learning and to consider taking one of Joeâ€™s classes at Chiba Tech. Chibany hears great things about them!\nDefining conditional probability as set restriction What Chibany calculated is a conditional probability â€” the probability of an event (tonkatsu for dinner) conditioned on knowledge of another event (at least one tonkatsu). Conditioning on an event means that the possible outcomes in that event form the set of possibilities or outcome space. We then calculate probabilities as normal within that restricted outcome space. In our example, weâ€™re interested in the probability of the event $A= \\{TT\\}$ conditioned on the knowledge that thereâ€™s at least one tonkatsu, $ B = \\Omega_{\\geq 1 T}= \\{HT, TH, TT\\}$. Formally, this is written as $P(A \\mid B) = \\frac{|A|}{|B|}$, where everything to the left of the $\\mid$ is what weâ€™re interested in knowing the probability of and everything to the right of the $\\mid$ is what we know to be true.\nKey Insight: Conditional Probability Conditional probability = restricting the outcome space\nWhen you learn something new:\nCross out impossible outcomes (those not in $B$) Count only whatâ€™s left Calculate the probability in this restricted space The math notation $P(A \\mid B)$ just formalizes this intuition: $$P(A \\mid B) = \\frac{|A \\cap B|}{|B|}$$\nWhere $A \\cap B$ means â€œoutcomes in both $A$ and $B$â€ (the intersection).\nNote that this is a different, yet equivalent perspective to how conditional probability is traditionally taught.\nDependence and independence Tanaka-san explains to Chibany his reasoning: He did not think whether Chibany received a tonkatsu (T) for his first offering influenced whether he receives a tonkatsu (T) for his second offering.\nChibany is curious. Tanaka-sanâ€™s logic seems sound, but it sounds like a slightly different question. Chibany asks Tanaka-san to draw out the outcome space and events for this question to help clarify what is different. Tanaka-san states his question formally: What is the probability of getting a second tonkatsu ($\\{HT, TT\\}$) given the first offering was a tonkatsu ($\\{TH, TT\\}$) or $P(\\{HT, TT\\} \\mid \\{TH, TT\\})$\nblock-beta block columns 1 block:group1 a[\"HH\"] b[\"HT\"] end block:group2 c[\"TH\"] d[\"TT\"] end end style group1 fill:#999, text-decoration:line-through style a fill:#999, text-decoration:line-through style b fill:#999, text-decoration:line-through style group2 stroke: #33f, stroke-width: 6px style d stroke: #f33, stroke-width: 4px Thereâ€™s one outcome ($TT$) out of two possible outcomes ($\\{TH, TT\\}$). Thus the probability is $1/2$: $P(\\{HT, TT\\} \\mid \\{TH, TT\\}) = 1/2$.\nTanaka-san says this time the result is what he expected. He says â€œIf I just think about what the probability of the second meal is and make that my outcome space, then the probability of the second meal being tonkatsu should just be one-half.â€ Chibany asks Tanaka-san to draw out this outcome space and calculate the probability this way instead. Chibany notes that probability is much more fun when you ask your friends to help you do the hard parts!\nblock-beta block:group2 columns 2 c[\"H\"] d[\"T\"] end style group2 stroke: #33f, stroke-width: 6px style d stroke: #f33, stroke-width: 4px Look at that â€” Itâ€™s one half! Chibany prefers learning that there will be at least one tonkatsu because it makes it more likely that he will get a tonkatsu for his second offering.\nWe saw in one case that conditioning on an event (that there will be one tonkatsu) influenced the probability of another event (that the second offering will be tonkatsu). But in a different case, conditioning on a slightly different event (that the first meal will be a tonkatsu) did not influence the probability of another event (again, that the second offering will be a tonkatsu).\nWhen conditioning on one event $A$ influences the probability of another event $B$, those two events are called dependent. This is denoted as $A \\not\\perp B$. If they do not influence each other they are called independent, which is denoted as $A \\perp B$.\nIndependence: A Formal Definition Events $A$ and $B$ are independent if: $$P(A \\mid B) = P(A)$$\nIn words: â€œLearning $B$ happened doesnâ€™t change the probability of $A$â€\nEquivalently: $P(A, B) = P(A) \\times P(B)$ (weâ€™ll see why in the next section!)\nMarginal and joint probabilities Chibany is sad (marginalization) The student that normally gives Chibany his second offering is out sick. Now Chibany only gets one offering per day. Chibany lists out the new set of possibilities $\\Omega_1 = \\{H, T\\}$.\nblock-beta block columns 2 a[\"H(amburger)\"] b[\"T(onkatsu)\"] end style b stroke: #f33, stroke-width:4px He notes this is a much sadder set of possibilities. At least the probability of getting Tonkatsu isnâ€™t too low! Itâ€™s one of two possibilities.\nThankfully, on the next day, the student is healthy again and Chibany is back to getting two offerings each day. This changes the set of possibilities back to the original one $\\Omega_2 = \\{HH,HT, TH, TT \\}$. Chibany realizes he can calculate the probability of the first offering being Tonkatsu. Getting his second meal shouldnâ€™t influence the chance the first one is Tonkatsu, right? Letâ€™s check!\nblock-beta block columns 2 a[\"H(amburger) H(amburger)\"] b[\"H(amburger) T(onkastu)\"] c[\"T(onkastu) H(amburger)\"] d[\"T(onkastu) T(onkastu)\"] end style c stroke: #f33, stroke-width:4px style d stroke: #f33, stroke-width:4px In this case, he is interested in $P(\\{TH, TT \\}) = 2/4 = 1/2$. Phew!\nWhat happened here? In both cases, we are interested in the same event â€” the probability the first meal is a Tonkatsu. In the first case, we did not include the second meal. This is called using marginal probability. In the second case, we did include the second meal. This is called using joint probability. Technically it counts the number of outcomes in the intersection of the different events being considered jointly. This means the number of outcomes that are in all the events under consideration.\nThe Sum Rule: More on Marginalization and Marginal Probabilities Intuitively, the following two ways of calculating the probability a variable takes a value should give the same answer: (1) list the possible outcomes containing only that variable and count those where it has the specified value (marginal probability), and (2) enumerate the possible outcomes containing that variable and another variable and count all of those where the first variable has the value of interest (joint probability).\nFormally, if we have two random variables $A$ and $B$, the marginal probability of $A$ is $P(A)$:\n$$P(A) = \\sum_{b} P(A, B=b)$$\nSum Rule Notation If youâ€™re unfamiliar with the notation $\\sum_{b}$:\n$\\sum$ is a fancy way of saying â€œadd the following upâ€ The subscript $b$ tells you which values to add up over In this case: sum over all possible values $b$ that random variable $B$ could take Example: If $B \\in \\{H, T\\}$, then: $$\\sum_{b} P(A, B=b) = P(A, B=H) + P(A, B=T)$$\nIn the last example, $A$ was Chibanyâ€™s first meal and $B$ was Chibanyâ€™s second meal. We were interested in whether Chibanyâ€™s first meal was Tonkatsu or $P(A=T)$. The possible values for $B$ are Hamburger and Tonkatsu or $\\{H,T \\}$. What we showed was:\n$$P(A=T) = \\sum_{b} P(A=T,B=b) = P(A=T, B=H) + P(A=T, B=T) = 1/4 + 1/4 = 2/4 = 1/2$$\nThe other definition of conditional probability Using joint and marginal probabilities, we can define conditional probability in a different manner â€” as the ratio of the joint probability to the marginal probability of the conditioned information. Or:\n$$P(A \\mid B) = \\frac{P(A,B)}{P(B)}$$\nNote that the probability of $B$ must be greater than zero ($P(B) \u003e 0$). This makes sense to Chibany. How could he be given information that had zero chance of happening?\nChibany is no fan of this other way of calculating conditional probabilities, but he decides to practice using it. He goes back to his favorite example so far â€” the one where he had better than a one-half chance of getting two Tonkatsus. In that example, he learned he was going to get at least one Tonkatsu and was interested in finding the probability that there would be two Tonkatsus. So, $A$ is getting a tonkatsu dinner (second meal is tonkatsu) and $B$ is that there is at least one tonkatsu. So $A = \\{HT, TT\\}$ and $B=\\{HT, TH, TT\\}$. The intersection or common possibilities in $A$ and $B$ is $\\{HT,TT\\}$. Remember that there are four possible outcomes in the larger outcome space $\\Omega = \\{HH,HT,TH,TT\\}$. This means $P(A,B) = |\\{HT,TT\\}|/ | \\{HH,HT,TH,TT\\}| = 2/4$. $P(B) = |\\{HT,TH,TT\\}|/|\\{HH,HT,TH,TT\\}| = 3/4$. Putting these together we get:\n$$P(A \\mid B) = \\frac{P(A,B)}{P(B)} = \\frac{2/4}{3/4} = \\frac{2}{3}$$\nAlthough Chibany is happy to see the same result of it being more likely than not heâ€™ll have a second meal of Tonkatsu if he learns he gets at least one Tonkatsu, this felt a lot harder to him than the first way of doing things. It may have felt that way for you too (it does for me!). Thatâ€™s why Chibany wants everyone to know the set-based perspective to probability.\nTwo Perspectives, Same Answer Set-based perspective: $P(A \\mid B) = \\frac{|A \\cap B|}{|B|}$\nThink: â€œCount in the restricted spaceâ€ Formula-based perspective: $P(A \\mid B) = \\frac{P(A,B)}{P(B)}$\nThink: â€œRatio of joint to marginalâ€ Both give the same answer! Use whichever feels more intuitive.\nWeighted possibilities Chibany tells students that he likes Tonkatsu more Chibany is happy! He remembered that students love learning. He has important information for them: Chibany likes Tonkatsu more than Hamburgers.\nWhile wondering how to calculate probabilities taking this glorious news into account, Tanaka-san stops by. Tanaka-san lets Chibany know that the students coordinate to ensure that he gets at least one tonkatsu, but try not to make both offerings tonkatsu (that way he doesnâ€™t get tired of Tonkatsu). Tanaka-san shares the following chart the students use to guide their daily offerings:\nblock-beta block columns 2 a[\"HH: 4%\"] b[\"HT: 43%\"] c[\"TH: 43%\"] d[\"TT: 10%\"] style b stroke: #f33, stroke-width:4px style c stroke: #f33, stroke-width:4px style d stroke: #f33, stroke-width:4px end Chibany is confused at first, but he sticks with the rules he learned. He follows the same procedure as before, but adds the weighted versions of each outcome rather than each outcome counting 1 automatically.\nSo he adds up the outcomes containing Tonkatsu (outlined in red) and divides it by the total amount:\n$$P(\\textrm{Tonkatsu}) = \\frac{0.43+0.43+0.10}{0.04+0.43+0.43+0.10} = \\frac{0.96}{1}=0.96$$\nHe gets a lot more Tonkatsu â€” Tonkatsu 96% of the time. Joyous times!\nWeighted Counting When outcomes arenâ€™t equally likely:\nEach outcome has a weight (its probability) Sum the weights in the event (instead of counting 1, 1, 1, â€¦) Divide by sum of all weights The logic is identical â€” just weighted counting instead of simple counting!\nPractice question Can you determine whether the first and second meals are dependent? How would you do that?\nanswer If $A$ and $B$ are random variables encoding Chibanyâ€™s first meal and second meals, we would want to see whether $P(A=a)$ is different from $P(A =a \\mid B=b)$ for any possible $a$ or $b$. Letâ€™s consider whether the probability the first meal is Tonkatsu is influenced by the second meal being Tonkatsu.\nFirst letâ€™s calculate $P(A=T)$. To do this, weâ€™ll use the sum rule: $$P(A=T) = \\sum_b{P(A=T, B= b)} = P(A=T, B=H) + P(A=T, B=T) = 0.43+0.10 = 0.53$$\nIs this different from $P(A = T \\mid B=T)$? How do we calculate this in the weighted case? The same as before except the $|\\Omega|$ is the amount of weight for the conditioned event $B=T$. So: $$P(A=T \\mid B=T) = \\frac{0.10}{0.43+0.10} = \\frac{0.10}{0.53} \\approx 0.19$$\nSince $0.53 \\neq 0.19$, the events are dependent! Learning about the second meal changes the probability of the first meal.\nWhat Weâ€™ve Learned This chapter covered some of the most important concepts in probability:\nConditional probability as restriction â€” Learning changes whatâ€™s possible Independence vs. dependence â€” Whether events influence each other Marginal vs. joint probability â€” Different ways to think about the same event The sum rule â€” Marginalizing out variables Weighted counting â€” Handling unequal probabilities These tools will be essential for Bayesâ€™ rule in the next chapter!\nâ† Previous: Probability and Counting Next: Bayesâ€™ Theorem â†’",
    "description": "Chibany wants a tonkatsu dinner A graduate of Chiba Tech, Tanaka-san, visits Chibany one day and tells Chibany that he knows that there will be at least one tonkatsu in tomorrowâ€™s offering. Chibany is excited. He wants to know how likely it is that the second meal is a Tonkatsu. He quizzes Tanaka-san. Tanaka-san says itâ€™s just as likely as before, so it should be 1/2. Chibany disagrees. Chibany says â€œI learned something because I know I will get at least one tonkatsuâ€. Also, Chibany is an optimist and deserves to have all the tonkatsu. Whoâ€™s right!? Letâ€™s check the chartâ€¦",
    "tags": [],
    "title": "Conditional probability as changing the possible outcomes",
    "uri": "/probintro/intro/04_conditional/index.html"
  },
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing TutorialÂ \u003eÂ Probabilistic Programming with GenJAX",
    "content": "What Gets Recorded When Code Runs? When you run a regular Python function, it does its work and returns a value. Then itâ€™s done â€” no record of what happened internally.\nGenJAX is different. When you run a generative function, GenJAX creates a trace â€” a complete record of:\nWhat random choices were made What values they took What the function returned How probable this execution was Think of it like a lab notebook that automatically records every detail of an experiment!\nWhy Traces Matter Short answer: Traces enable inference â€” answering â€œwhat if I observed this?â€\nExample scenario:\nYou run chibany_day() and it returns (0, 1) â€” Hamburger for lunch, Tonkatsu for dinner The trace records: â€œI chose 0 for lunch, 1 for dinnerâ€ Later, you can ask: â€œGiven that dinner was Tonkatsu, whatâ€™s the probability lunch was also Tonkatsu?â€ Traces let us reason backwards from observations to causes!\nWeâ€™ll explore this fully in Chapter 4. For now, letâ€™s understand what traces contain.\nAnatomy of a Trace Recall our generative function:\n1 2 3 4 5 @gen def chibany_day(): lunch_is_tonkatsu = bernoulli(0.5) @ \"lunch\" dinner_is_tonkatsu = bernoulli(0.5) @ \"dinner\" return (lunch_is_tonkatsu, dinner_is_tonkatsu) When we run it:\n1 2 key = jax.random.key(42) trace = chibany_day.simulate(key, ()) GenJAX creates a trace object containing three key components:\n1. The Return Value What the function returned:\n1 2 meals = trace.get_retval() print(meals) # Output: (0, 1) This is the final result â€” the observable outcome.\n2. The Random Choices All the random decisions made, with their names:\n1 2 3 choices = trace.get_choices() print(choices) # Output: {'lunch': 0, 'dinner': 1} This is the choice map â€” a dictionary mapping addresses (names) to values.\nWhy Names Matter In bernoulli(0.5) @ \"lunch\", the @ \"lunch\" part gives this random choice a name (or address).\nGenJAX uses these names to:\nTrack which choice is which Let you specify observations (more in Chapter 4!) Enable inference algorithms Think of it like labeling test tubes in a chemistry lab. You need to know which is which!\n3. The Log Probability (Score) How probable was this execution?\n1 2 score = trace.get_score() print(score) # Output: -1.3862943611198906 This is the log probability of this particular execution.\nMath Notation: Log Probability For our example:\nLunch = 0 has probability 0.5 Dinner = 1 has probability 0.5 Joint probability: $P(\\text{lunch}=0, \\text{dinner}=1) = 0.5 \\times 0.5 = 0.25$ Log probability: $\\log(0.25) = -1.386â€¦$\nWhy use logs?\nPrevents numerical underflow (very small probabilities) Turns multiplication into addition (easier math!) Standard in probabilistic programming You donâ€™t need to work with log probabilities directly â€” GenJAX handles this for you. Just know they measure â€œhow likely was this outcome.â€\nThe Complete Trace Diagram Letâ€™s visualize whatâ€™s in a trace:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ TRACE OBJECT â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚ â”‚ 1. Arguments: () â”‚ â”‚ (what was passed to the function) â”‚ â”‚ â”‚ â”‚ 2. Random Choices (Choice Map): â”‚ â”‚ {'lunch': 0, 'dinner': 1} â”‚ â”‚ (all random decisions made) â”‚ â”‚ â”‚ â”‚ 3. Return Value: â”‚ â”‚ (0, 1) â”‚ â”‚ (what the function returned) â”‚ â”‚ â”‚ â”‚ 4. Log Probability (Score): â”‚ â”‚ -1.386 â”‚ â”‚ (how probable was this trace) â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Every time you call simulate(), you get a new trace with (potentially) different random choices.\nAccessing Trace Components Hereâ€™s a complete example showing all three ways to access trace information:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import jax from genjax import gen, bernoulli @gen def chibany_day(): lunch_is_tonkatsu = bernoulli(0.5) @ \"lunch\" dinner_is_tonkatsu = bernoulli(0.5) @ \"dinner\" return (lunch_is_tonkatsu, dinner_is_tonkatsu) # Generate one trace key = jax.random.key(42) trace = chibany_day.simulate(key, ()) # Access different parts print(\"=== TRACE CONTENTS ===\") print(f\"Return value: {trace.get_retval()}\") print(f\"Random choices: {trace.get_choices()}\") print(f\"Log probability: {trace.get_score()}\") # Decode to outcome notation outcome_map = {(0, 0): \"HH\", (0, 1): \"HT\", (1, 0): \"TH\", (1, 1): \"TT\"} outcome = outcome_map[tuple(trace.get_retval())] print(f\"Outcome: {outcome}\") Output (example):\n=== TRACE CONTENTS === Return value: (0, 1) Random choices: {'lunch': 0, 'dinner': 1} Log probability: -1.3862943611198906 Outcome: HT Multiple Traces, Multiple Histories Each trace represents one possible execution of the generative function.\nRun it 5 times, get 5 different traces:\n1 2 3 4 5 6 7 8 9 10 11 key = jax.random.key(42) for i in range(5): # Split key for each run (JAX requirement) key, subkey = jax.random.split(key) trace = chibany_day.simulate(subkey, ()) outcome = outcome_map[tuple(trace.get_retval())] choices = trace.get_choices() print(f\"Day {i+1}: {outcome} â€” lunch={choices['lunch']}, dinner={choices['dinner']}\") Output (example):\nDay 1: HT â€” lunch=0, dinner=1 Day 2: TH â€” lunch=1, dinner=0 Day 3: HH â€” lunch=0, dinner=0 Day 4: TT â€” lunch=1, dinner=1 Day 5: HT â€” lunch=0, dinner=1 Each trace is a different history â€” a different way the random process could have unfolded.\nJAX Random Keys Notice we use jax.random.split(key) to create new keys for each run?\nWhy? JAX uses explicit random keys for reproducibility. The same key always gives the same result.\nPattern:\n1 2 key, subkey = jax.random.split(key) # Create new key trace = model.simulate(subkey, ...) # Use the subkey This ensures different random outcomes each time while maintaining reproducibility.\nTraces vs Return Values Important distinction:\nsimulate() returns get_retval() returns Trace object The actual value Contains choices, score, return value Just the return value Used for inference Used for the result Example:\n1 2 3 4 5 6 7 8 9 # This is a trace object trace = chibany_day.simulate(key, ()) # This is the return value (a tuple) meals = trace.get_retval() # These are different! print(type(trace)) # \u003cclass 'genjax.generative_functions.static.trace.StaticTrace'\u003e print(type(meals)) # \u003cclass 'tuple'\u003e When to use which:\nNeed just the outcome? Use trace.get_retval() Need to inspect random choices? Use trace.get_choices() Doing inference? Use the full trace object Connection to Probability Theory Letâ€™s connect traces back to set-based probability:\nProbability Concept Trace Equivalent Outcome $\\omega \\in \\Omega$ One trace (one execution) Outcome space $\\Omega$ All possible traces $P(\\omega)$ exp(trace.get_score()) Random variable $X(\\omega)$ A choice in the choice map Joint distribution Distribution over traces Key insight: A trace IS an outcome! The trace represents one complete way the random process could unfold.\nExample:\nSet-based: $\\omega = HT$ (one outcome from $\\Omega = {HH, HT, TH, TT}$) Trace-based: A trace where choices = {'lunch': 0, 'dinner': 1} Theyâ€™re the same thing! Just different representations.\nWhy This Matters for Inference Consider this question:\nâ€œGiven that Chibany got Tonkatsu for dinner, whatâ€™s the probability he also got Tonkatsu for lunch?â€\nSet-based approach:\nDefine event $D$ = â€œdinner is Tonkatsuâ€ = ${HT, TT}$ Define event $L$ = â€œlunch is Tonkatsuâ€ = ${TH, TT}$ Calculate $P(L \\mid D) = \\frac{|L \\cap D|}{|D|} = \\frac{1}{2}$ Trace-based approach:\nGenerate many traces Filter traces where choices['dinner'] == 1 Among those, count how many have choices['lunch'] == 1 Calculate the ratio The trace structure makes this filtering possible! Because GenJAX records all the random choices, we can look inside and check what happened.\nWeâ€™ll implement this in Chapter 4!\nPractical Example: Inspecting Traces Letâ€™s generate 10 traces and inspect them:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import jax import jax.numpy as jnp from genjax import gen, bernoulli @gen def chibany_day(): lunch_is_tonkatsu = bernoulli(0.5) @ \"lunch\" dinner_is_tonkatsu = bernoulli(0.5) @ \"dinner\" return (lunch_is_tonkatsu, dinner_is_tonkatsu) # Generate 10 traces key = jax.random.key(42) outcome_map = {(0, 0): \"HH\", (0, 1): \"HT\", (1, 0): \"TH\", (1, 1): \"TT\"} print(\"Day | Outcome | Lunch | Dinner | Log Prob\") print(\"----|---------|-------|--------|----------\") for i in range(10): key, subkey = jax.random.split(key) trace = chibany_day.simulate(subkey, ()) outcome = outcome_map[tuple(trace.get_retval())] choices = trace.get_choices() score = trace.get_score() print(f\" {i+1:2d} | {outcome} | {choices['lunch']} | {choices['dinner']} | {score:.2f}\") Output (example):\nDay | Outcome | Lunch | Dinner | Log Prob ----|---------|-------|--------|---------- 1 | HT | 0 | 1 | -1.39 2 | TH | 1 | 0 | -1.39 3 | HH | 0 | 0 | -1.39 4 | TT | 1 | 1 | -1.39 5 | HT | 0 | 1 | -1.39 6 | HH | 0 | 0 | -1.39 7 | TT | 1 | 1 | -1.39 8 | HT | 0 | 1 | -1.39 9 | TH | 1 | 0 | -1.39 10 | HH | 0 | 0 | -1.39 Notice: All log probabilities are the same (-1.39 â‰ˆ log(0.25)) because all outcomes are equally probable!\nExercises Exercise 1: Trace Exploration Run this code and answer the questions:\n1 2 3 4 5 6 key = jax.random.key(123) trace = chibany_day.simulate(key, ()) print(f\"Return value: {trace.get_retval()}\") print(f\"Choices: {trace.get_choices()}\") print(f\"Score: {trace.get_score()}\") Questions:\nWhat outcome did you get? (HH, HT, TH, or TT) Whatâ€™s in the choice map? Is the log probability the same as previous examples? Solution Answers:\nThe outcome depends on the random seed (123) The choice map contains {'lunch': 0 or 1, 'dinner': 0 or 1} Yes! All outcomes have equal probability (0.25), so log probability is always -1.386â€¦ Key insight: Different random keys â†’ different traces, but same probabilities (for this symmetric example)\nExercise 2: Unequal Probabilities Modify chibany_day to have unequal probabilities:\n1 2 3 4 5 @gen def chibany_day_biased(): lunch_is_tonkatsu = bernoulli(0.8) @ \"lunch\" # 80% Tonkatsu dinner_is_tonkatsu = bernoulli(0.2) @ \"dinner\" # 20% Tonkatsu return (lunch_is_tonkatsu, dinner_is_tonkatsu) Generate 5 traces and compare their log probabilities.\nQuestion: Are all log probabilities the same? Why or why not?\nSolution 1 2 3 4 5 6 7 8 9 10 key = jax.random.key(42) for i in range(5): key, subkey = jax.random.split(key) trace = chibany_day_biased.simulate(subkey, ()) outcome = outcome_map[tuple(trace.get_retval())] score = trace.get_score() print(f\"Day {i+1}: {outcome} â€” Log prob: {score:.3f}\") Answer: No! Log probabilities differ because outcomes have different probabilities:\nTT: $P = 0.8 \\times 0.2 = 0.16$, $\\log(0.16) = -1.83$ TH: $P = 0.8 \\times 0.8 = 0.64$, $\\log(0.64) = -0.45$ HT: $P = 0.2 \\times 0.2 = 0.04$, $\\log(0.04) = -3.22$ HH: $P = 0.2 \\times 0.8 = 0.16$, $\\log(0.16) = -1.83$ TH is most likely (highest probability = least negative log probability)!\nExercise 3: Conditional Counting Generate 1000 traces from chibany_day() and answer:\nâ€œAmong days when dinner is Tonkatsu, what fraction also have Tonkatsu for lunch?â€\nHint: Filter traces where choices['dinner'] == 1, then count how many have choices['lunch'] == 1.\nSolution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import jax import jax.numpy as jnp key = jax.random.key(42) keys = jax.random.split(key, 1000) # Generate all traces def run_one_day(k): trace = chibany_day.simulate(k, ()) return trace.get_retval() days = jax.vmap(run_one_day)(keys) # Filter: dinner is Tonkatsu (dinner == 1) dinner_is_tonkatsu = days[:, 1] == 1 # Among those, count lunch is Tonkatsu both_tonkatsu = (days[:, 0] == 1) \u0026 (days[:, 1] == 1) # Calculate conditional probability n_dinner_tonkatsu = jnp.sum(dinner_is_tonkatsu) n_both = jnp.sum(both_tonkatsu) prob_lunch_given_dinner = n_both / n_dinner_tonkatsu print(f\"Days with dinner = Tonkatsu: {n_dinner_tonkatsu}\") print(f\"Days with both = Tonkatsu: {n_both}\") print(f\"P(lunch=T | dinner=T) â‰ˆ {prob_lunch_given_dinner:.3f}\") Expected result: â‰ˆ 0.5 (50%)\nWhy? Lunch and dinner are independent! Knowing dinner doesnâ€™t change lunch probability.\nThis is conditional probability through filtering! (More in Chapter 4)\nWhat Youâ€™ve Learned In this chapter, you learned:\nâœ… What traces are â€” complete records of random execution âœ… Three key components â€” return value, choice map, log probability âœ… Why names matter â€” @ \"address\" enables tracking and inference âœ… How to access trace parts â€” get_retval(), get_choices(), get_score() âœ… Traces as outcomes â€” connection to probability theory âœ… Preview of inference â€” filtering traces to answer conditional questions\nThe key insight: Traces arenâ€™t just records â€” theyâ€™re the bridge between generative code and probabilistic reasoning!\nNext Steps Now that you understand traces, youâ€™re ready for the most powerful feature of GenJAX:\nChapter 4: Conditioning and Observations â€” How to ask â€œwhat if I observed this?â€ and update beliefs based on evidence!\nThis is where GenJAX really shines compared to regular simulation.\nâ† Previous: Your First GenJAX Model Next: Conditioning and Observations â†’",
    "description": "What Gets Recorded When Code Runs? When you run a regular Python function, it does its work and returns a value. Then itâ€™s done â€” no record of what happened internally.\nGenJAX is different. When you run a generative function, GenJAX creates a trace â€” a complete record of:\nWhat random choices were made What values they took What the function returned How probable this execution was Think of it like a lab notebook that automatically records every detail of an experiment!",
    "tags": [],
    "title": "Understanding Traces",
    "uri": "/probintro/genjax/03_traces/index.html"
  },
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing TutorialÂ \u003eÂ A Narrative Introduction to Probability",
    "content": "What is Bayesâ€™ Theorem? Imagine you have a belief about the world (a hypothesis), and then you observe something new (data). Bayesâ€™ Theorem tells you how to update your belief based on what you observed.\nExample: You believe most taxis are green. Then you see a taxi that looks blue in the fog. How should you update your belief about which color it really was?\nThe Formula Bayesâ€™ Theorem (Bayesâ€™ rule) provides a way to update our beliefs in one random variable given information about a different random variable. Letâ€™s say we have certain hypotheses about how the world works, which we denote as random variable $H$. Further, we have senses that provide us information. Letâ€™s encode the information that we might get from our senses as $D$ (maybe an image from our eyes) and we currently observe $d$ (maybe a picture of tonkatsu).\nBayes Theorem tells us to update our beliefs in hypothesis $h$ being the way the world works after learning $D=d$ in the following manner:\n$$P(H=h \\mid D = d) = \\frac{P(D=d\\mid H=h) P(H=h)}{P(D=d)}$$\nwhere:\n$P(H=h \\mid D=d)$ is called the posterior â€” our updated belief after seeing the data $P(D=d \\mid H=h)$ is called the likelihood â€” the probability of observing $d$ given $h$ is the true hypothesis for how the world works $P(H=h)$ is called the prior â€” how likely it is that $h$ is the way the world works before seeing any data $P(D=d)$ is called the evidence or marginal likelihood â€” the total probability of observing $d$ across all hypotheses Understanding the Terms Prior = What you believed before seeing data Likelihood = How well the data fits each hypothesis Evidence = How surprising is this data overall? Posterior = What you should believe after seeing data The key insight: Strong evidence (high likelihood) can overcome weak priors, but extraordinary claims still require extraordinary evidence!\nWe have all the information to prove this! Feel free to skip to the next subsection if you donâ€™t care about proofs.\nProving Bayesâ€™ rule Using the other definition of conditional probability, we know that $P(H \\mid D) = \\frac{P(H,D)}{P(D)}$. If we multiply both sides of the equation by $P(D)$, we get $P(H,D) = P(H \\mid D) P(D)$. We can do the same thing but for the opposite way of conditioning (the joint probability can be written in either order and it is the same as it is the common elements of two sets which is the same no matter which order you consider the two sets), so $P(D \\mid H) = \\frac{P(H,D)}{P(H)}$. We can solve for $P(H,D)$ in a similar manner: multiply both sides of the equation by $P(H)$ and we get $P(H,D) = P(D \\mid H) P(H)$. Putting these together, we can prove Bayesâ€™ rule:\n$$P(H \\mid D) P(D) = P(H,D) = P(D \\mid H) P(H)$$ $$\\Rightarrow P(H \\mid D) = \\frac{P(H,D)}{P(D)} = \\frac{P(D \\mid H) P(H)}{P(D)}$$\nTip Donâ€™t worry if this felt abstract. The taxicab problem below will make it concrete!\nThe Taxicab Problem TODO: Add picture with chibany seeing a hit and run with a taxi with fog/smoke\nIn Chibanyâ€™s hometown, there are two taxi companies: the Green and the Blue . All Green companyâ€™s taxis are painted green and all the Blue companyâ€™s taxis are painted blue .\n85% of the townâ€™s taxis work for the Green company. So 15% of the townâ€™s taxis work for the Blue company.\nLate one foggy evening, Chibany saw a cab perform a hit-and-run (hit another car and leave without providing any information). Chibany saw a Blue taxi!\nChibany is an outstanding citizen and so he goes to the police with this information. The police know it was foggy and dark, so itâ€™s possible Chibany might not have seen the taxiâ€™s color correctly. They test Chibany several times and find that Chibany reports the correct taxi color 80% of the time!\nTaking all of this information into account, how likely do you think it is that the cab involved in the hit-and-run was a Blue taxi ?\nanswer The correct answer is 41%, but most people think it is closer to 60-80%! This is known as the Taxicab Problem (Kahneman and Tversky, 1972; Bar-Hillel, 1980).\nBase Rate Neglect Most people focus on Chibanyâ€™s 80% accuracy and ignore the base rate (85% green taxis). This is a classic cognitive bias called base-rate neglect.\nThe key insight: Even with pretty good accuracy (80%), if something is rare (15% blue taxis), evidence for it isnâ€™t as strong as it seems!\nA note: Kahneman and Tversky (and others) use this example (and others) to argue that people are not Bayesian at all! There are a number of replies through the years and it is an ongoing debate. Joe loves discussing it. If interested, please reach out and he would be more than happy to discuss it more.\nTaxicab Solution 1: The Set-Based Perspective One way to solve this is to use the outcome space perspective! Let us assume there are 100 taxis in Chibanyâ€™s hometown. That means the set of possibilities $\\Omega$ has 85 individual Green taxis and 15 individual Blue taxis .\nblock-beta block columns 10 g1[\"fa:fa-taxi\"] g2[\"fa:fa-taxi\"] g3[\"fa:fa-taxi\"] g4[\"fa:fa-taxi\"] g5[\"fa:fa-taxi\"] g6[\"fa:fa-taxi\"] g7[\"fa:fa-taxi\"] g8[\"fa:fa-taxi\"] g9[\"fa:fa-taxi\"] g10[\"fa:fa-taxi\"] g11[\"fa:fa-taxi\"] g12[\"fa:fa-taxi\"] g13[\"fa:fa-taxi\"] g14[\"fa:fa-taxi\"] g15[\"fa:fa-taxi\"] g16[\"fa:fa-taxi\"] g17[\"fa:fa-taxi\"] g18[\"fa:fa-taxi\"] g19[\"fa:fa-taxi\"] g20[\"fa:fa-taxi\"] g21[\"fa:fa-taxi\"] g22[\"fa:fa-taxi\"] g23[\"fa:fa-taxi\"] g24[\"fa:fa-taxi\"] g25[\"fa:fa-taxi\"] g26[\"fa:fa-taxi\"] g27[\"fa:fa-taxi\"] g28[\"fa:fa-taxi\"] g29[\"fa:fa-taxi\"] g30[\"fa:fa-taxi\"] g31[\"fa:fa-taxi\"] g32[\"fa:fa-taxi\"] g33[\"fa:fa-taxi\"] g34[\"fa:fa-taxi\"] g35[\"fa:fa-taxi\"] g36[\"fa:fa-taxi\"] g37[\"fa:fa-taxi\"] g38[\"fa:fa-taxi\"] g39[\"fa:fa-taxi\"] g40[\"fa:fa-taxi\"] g41[\"fa:fa-taxi\"] g42[\"fa:fa-taxi\"] g43[\"fa:fa-taxi\"] g44[\"fa:fa-taxi\"] g45[\"fa:fa-taxi\"] g46[\"fa:fa-taxi\"] g47[\"fa:fa-taxi\"] g48[\"fa:fa-taxi\"] g49[\"fa:fa-taxi\"] g50[\"fa:fa-taxi\"] g51[\"fa:fa-taxi\"] g52[\"fa:fa-taxi\"] g53[\"fa:fa-taxi\"] g54[\"fa:fa-taxi\"] g55[\"fa:fa-taxi\"] g56[\"fa:fa-taxi\"] g57[\"fa:fa-taxi\"] g58[\"fa:fa-taxi\"] g59[\"fa:fa-taxi\"] g60[\"fa:fa-taxi\"] g61[\"fa:fa-taxi\"] g62[\"fa:fa-taxi\"] g63[\"fa:fa-taxi\"] g64[\"fa:fa-taxi\"] g65[\"fa:fa-taxi\"] g66[\"fa:fa-taxi\"] g67[\"fa:fa-taxi\"] g68[\"fa:fa-taxi\"] g69[\"fa:fa-taxi\"] g70[\"fa:fa-taxi\"] g71[\"fa:fa-taxi\"] g72[\"fa:fa-taxi\"] g73[\"fa:fa-taxi\"] g74[\"fa:fa-taxi\"] g75[\"fa:fa-taxi\"] g76[\"fa:fa-taxi\"] g77[\"fa:fa-taxi\"] g78[\"fa:fa-taxi\"] g79[\"fa:fa-taxi\"] g80[\"fa:fa-taxi\"] g81[\"fa:fa-taxi\"] g82[\"fa:fa-taxi\"] g83[\"fa:fa-taxi\"] g84[\"fa:fa-taxi\"] g85[\"fa:fa-taxi\"] b11[\"fa:fa-taxi\"] b12[\"fa:fa-taxi\"] b13[\"fa:fa-taxi\"] b14[\"fa:fa-taxi\"] b15[\"fa:fa-taxi\"] b1[\"fa:fa-taxi\"] b2[\"fa:fa-taxi\"] b3[\"fa:fa-taxi\"] b4[\"fa:fa-taxi\"] b5[\"fa:fa-taxi\"] b6[\"fa:fa-taxi\"] b7[\"fa:fa-taxi\"] b8[\"fa:fa-taxi\"] b9[\"fa:fa-taxi\"] b10[\"fa:fa-taxi\"] classDef blueTaxi color: #06f, min-width:22px, font-size:18px classDef greenTaxi color: #0d2, min-width:22px, font-size:18px class b1,b2,b3,b4,b5,b6,b7,b8,b9,b10,b11,b12,b13,b14,b15 blueTaxi class g1,g2,g3,g4,g5,g6,g7,g8,g9,g10,g11,g12,g13,g14,g15,g16,g17,g18,g19,g20,g21,g22,g23,g24,g25,g26,g27,g28,g29,g30,g31,g32,g33,g34,g35,g36,g37,g38,g39,g40,g41,g42,g43,g44,g45,g46,g47,g48,g49,g50,g51,g52,g53,g54,g55,g56,g57,g58,g59,g60,g61,g62,g63,g64,g65,g66,g67,g68,g69,g70,g71,g72,g73,g74,g75,g76,g77,g78,g79,g80,g81,g82,g83,g84,g85 greenTaxi end Now we can make the outcome space include the taxi color and whether Chibany identifies the taxi as Blue in foggy nighttime conditions. As Chibany correctly identifies 80% of the Blue taxis as Blue, ($15 \\times 0.80=12$), this means 12 of the Blue taxis are identified as Blue and ($15 \\times 0.2 = 3$) 3 are incorrectly identified as Green. As Chibany incorrectly identifies 20% of the Green taxis as Blue, this means ($85 \\times 0.2 = 17$) 17 of the Green taxis are identified as Blue and ($85 \\times 0.8=68$) 68 are correctly identified as Green.\nblock-beta block columns 10 g1[\"fa:fa-taxi\"] g2[\"fa:fa-taxi\"] g3[\"fa:fa-taxi\"] g4[\"fa:fa-taxi\"] g5[\"fa:fa-taxi\"] g6[\"fa:fa-taxi\"] g7[\"fa:fa-taxi\"] g8[\"fa:fa-taxi\"] g9[\"fa:fa-taxi\"] g10[\"fa:fa-taxi\"] g11[\"fa:fa-taxi\"] g12[\"fa:fa-taxi\"] g13[\"fa:fa-taxi\"] g14[\"fa:fa-taxi\"] g15[\"fa:fa-taxi\"] g16[\"fa:fa-taxi\"] g17[\"fa:fa-taxi\"] g18[\"fa:fa-taxi\"] g19[\"fa:fa-taxi\"] g20[\"fa:fa-taxi\"] g21[\"fa:fa-taxi\"] g22[\"fa:fa-taxi\"] g23[\"fa:fa-taxi\"] g24[\"fa:fa-taxi\"] g25[\"fa:fa-taxi\"] g26[\"fa:fa-taxi\"] g27[\"fa:fa-taxi\"] g28[\"fa:fa-taxi\"] g29[\"fa:fa-taxi\"] g30[\"fa:fa-taxi\"] g31[\"fa:fa-taxi\"] g32[\"fa:fa-taxi\"] g33[\"fa:fa-taxi\"] g34[\"fa:fa-taxi\"] g35[\"fa:fa-taxi\"] g36[\"fa:fa-taxi\"] g37[\"fa:fa-taxi\"] g38[\"fa:fa-taxi\"] g39[\"fa:fa-taxi\"] g40[\"fa:fa-taxi\"] g41[\"fa:fa-taxi\"] g42[\"fa:fa-taxi\"] g43[\"fa:fa-taxi\"] g44[\"fa:fa-taxi\"] g45[\"fa:fa-taxi\"] g46[\"fa:fa-taxi\"] g47[\"fa:fa-taxi\"] g48[\"fa:fa-taxi\"] g49[\"fa:fa-taxi\"] g50[\"fa:fa-taxi\"] g51[\"fa:fa-taxi\"] g52[\"fa:fa-taxi\"] g53[\"fa:fa-taxi\"] g54[\"fa:fa-taxi\"] g55[\"fa:fa-taxi\"] g56[\"fa:fa-taxi\"] g57[\"fa:fa-taxi\"] g58[\"fa:fa-taxi\"] g59[\"fa:fa-taxi\"] g60[\"fa:fa-taxi\"] g61[\"fa:fa-taxi\"] g62[\"fa:fa-taxi\"] g63[\"fa:fa-taxi\"] g64[\"fa:fa-taxi\"] g65[\"fa:fa-taxi\"] g66[\"fa:fa-taxi\"] g67[\"fa:fa-taxi\"] g68[\"fa:fa-taxi\"] g69[\"fa:fa-taxi\"] g70[\"fa:fa-taxi\"] g71[\"fa:fa-taxi\"] g72[\"fa:fa-taxi\"] g73[\"fa:fa-taxi\"] g74[\"fa:fa-taxi\"] g75[\"fa:fa-taxi\"] g76[\"fa:fa-taxi\"] g77[\"fa:fa-taxi\"] g78[\"fa:fa-taxi\"] g79[\"fa:fa-taxi\"] g80[\"fa:fa-taxi\"] g81[\"fa:fa-taxi\"] g82[\"fa:fa-taxi\"] g83[\"fa:fa-taxi\"] g84[\"fa:fa-taxi\"] g85[\"fa:fa-taxi\"] b11[\"fa:fa-taxi\"] b12[\"fa:fa-taxi\"] b13[\"fa:fa-taxi\"] b14[\"fa:fa-taxi\"] b15[\"fa:fa-taxi\"] b1[\"fa:fa-taxi\"] b2[\"fa:fa-taxi\"] b3[\"fa:fa-taxi\"] b4[\"fa:fa-taxi\"] b5[\"fa:fa-taxi\"] b6[\"fa:fa-taxi\"] b7[\"fa:fa-taxi\"] b8[\"fa:fa-taxi\"] b9[\"fa:fa-taxi\"] b10[\"fa:fa-taxi\"] classDef blueTaxi color: #06f, min-width:22px, font-size:18px, stroke: #f33, stroke-width:2px classDef blueGrayTaxi color: #028, min-width:22px, font-size:18px classDef greenTaxi color: #0d2, min-width:22px, font-size:18px, stroke: #f33, stroke-width:2px classDef greenGrayTaxi color: #051, min-width:22px, font-size:18px class b1,b2,b3,b4,b5,b6,b7,b11,b12,b13,b14,b15 blueTaxi class b8,b9,b10 blueGrayTaxi class g1,g2,g3,g4,g5,g6,g7,g8,g9,g10,g11,g12,g13,g14,g15,g16,g17 greenTaxi class g18,g19,g20,g21,g22,g23,g24,g25,g26,g27,g28,g29,g30,g31,g32,g33,g34,g35,g36,g37,g38,g39,g40,g41,g42,g43,g44,g45,g46,g47,g48,g49,g50,g51,g52,g53,g54,g55,g56,g57,g58,g59,g60,g61,g62,g63,g64,g65,g66,g67,g68,g69,g70,g71,g72,g73,g74,g75,g76,g77,g78,g79,g80,g81,g82,g83,g84,g85 greenGrayTaxi end The brightly colored taxis that are outlined in red are those that Chibany reports as Blue in the difficult viewing conditions. We can already see there are more Green taxis than Blue , so it is still more probable that the taxi involved in the hit-and-run was Green. We can get the exact probability that it was a Blue taxi by the same counting rule as before. There are 12 Blue taxis and 17 Green taxis identified as blue. So, the probability that it was a blue taxi given Chibany reports it as Blue is $12/(12+17)=12/29 \\approx 0.41$.\nThe Power of Visualization The diagram makes the answer obvious! Even though Chibany is 80% accurate:\n12 truly blue taxis are reported as blue 17 actually green taxis are reported as blue There are more false positives than true positives because green taxis are so common!\nTaxicab Solution 2: Using Bayesâ€™ Formula We can also solve this without counting in a sample space by following the rules of probability theory as described before. This is powerful when counting becomes impractical (imagine 1 million taxis!).\nLet $X$ be the actual color of the taxi involved in the hit-and-run and $W$ be the color reported by Chibany (â€œwhat they witnessâ€). Based on the percentage of Blue and Green taxis in the city, we know that $P(X=G) = 0.85$ and $P(X=B)=0.15$. We also know that Chibany is accurate 80% of the time. So, $P(W = B \\mid X = B) = 0.8$ and $P(W=G \\mid X=G)=0.8$. This also means Chibany is inaccurate 20% of the time: $P(W = B \\mid X=G)=0.2$ and $P(W=G \\mid X=B)=0.2$.\nChibany said the taxi is Blue and given this, how likely is it that the taxi is Blue? So, weâ€™re interested in $P(X=B \\mid W=B)$. We can solve this using Bayesâ€™ rule and the sum rule.\n$$P(X=B \\mid W=B) = \\frac{P(W =B \\mid X=B) P(X=B)}{P(W=B)}$$\n$$P(X=B \\mid W=B) = \\frac{P(W =B \\mid X=B) P(X=B)}{\\sum_c{P(W=B,X=c)}}$$\n$$P(X=B \\mid W=B) = \\frac{P(W =B \\mid X=B) P(X=B)}{\\sum_c{P(W=B \\mid X=c)P(X=c)}}$$\n$$P(X=B \\mid W=B) = \\frac{P(W =B \\mid X=B) P(X=B)}{P(W=B \\mid X=B)P(X=B) + P(W=B \\mid X=G)P(X=G)}$$\n$$P(X=B \\mid W=B) = \\frac{0.8 \\times 0.15 }{0.8 \\times 0.15 + 0.2 \\times 0.85} = \\frac{0.12}{0.12+0.17} = \\frac{0.12}{0.29} \\approx 0.41$$\nBreaking Down Bayesâ€™ Rule Letâ€™s identify each component:\nNumerator (likelihood Ã— prior):\nLikelihood: $P(W=B \\mid X=B) = 0.8$ â€” â€œIf itâ€™s blue, Iâ€™ll probably say blueâ€ Prior: $P(X=B) = 0.15$ â€” â€œBlue taxis are rareâ€ Product: $0.8 \\times 0.15 = 0.12$ Denominator (total evidence):\nBlue AND reported blue: $0.8 \\times 0.15 = 0.12$ Green BUT reported blue: $0.2 \\times 0.85 = 0.17$ Total: $0.12 + 0.17 = 0.29$ Posterior: $\\frac{0.12}{0.29} \\approx 0.41$ â€” Only 41% chance itâ€™s actually blue!\nWhy Learn the Set-Based Perspective to Probability Theory? If we can solve probability problems via symbol manipulation, why learn the set-based perspective to probability theory?\nHere are some reasons:\nScales to computation: As variables become more complex, explicitly solving problems becomes infeasible. Thinking through how to count is a strong starting point for a generative process perspective, which discusses how outcomes are produced according to computer programs with random choices. These define probabilistic models! Probabilistic computing frameworks are programming languages for specifying probabilistic models and built to calculate different probabilities according to this model in an efficient manner. We will build to exploring how to do this over the next few tutorials.\nClarity on joint vs. conditional: Many probability novices find the distinction between joint and conditional probabilities confusing and unintuitive. From the set-based perspective, their difference is clear. Joint probabilities count outcomes where multiple events occur simultaneously. Conditional probabilities change the outcome space to be whatever is consistent with the conditioned information and then count in that new space.\nForces representation thinking: It requires you to think about how events and outcomes are represented. This can be obscured at times when thinking about probabilities from the rule-based perspective.\nFormal equivalence: The set-based and formula-based approaches are formally equivalent â€” they always give the same answer.\nItâ€™s more intuitive: For many people (including this tutorialâ€™s author!), visualizing and counting feels more natural than manipulating symbols.\nConnects combinatorics and probability: It makes the deep connection between counting and probability explicit.\nIt makes Chibany happy: And thatâ€™s what really matters!\nTransfer additional practice questions Example with rare disease and not too diagnostic test.\nExample with organic fruit and made at a local place\nWhat Weâ€™ve Learned In this chapter, we tackled one of the most important tools in probability:\nBayesâ€™ Theorem â€” How to update beliefs with new evidence The taxicab problem â€” Why base rates matter Two solution methods â€” Visualization vs. formula Base-rate neglect â€” A common reasoning error Why set-based thinking helps â€” Making abstract concepts concrete You now have all the core tools of probability theory! The next chapter summarizes key definitions, and then youâ€™ll be ready for advanced topics.\nâ† Previous: Conditional Probability Next: Glossary â†’",
    "description": "What is Bayesâ€™ Theorem? Imagine you have a belief about the world (a hypothesis), and then you observe something new (data). Bayesâ€™ Theorem tells you how to update your belief based on what you observed.\nExample: You believe most taxis are green. Then you see a taxi that looks blue in the fog. How should you update your belief about which color it really was?\nThe Formula Bayesâ€™ Theorem (Bayesâ€™ rule) provides a way to update our beliefs in one random variable given information about a different random variable. Letâ€™s say we have certain hypotheses about how the world works, which we denote as random variable $H$. Further, we have senses that provide us information. Letâ€™s encode the information that we might get from our senses as $D$ (maybe an image from our eyes) and we currently observe $d$ (maybe a picture of tonkatsu).",
    "tags": [],
    "title": "Bayes' Theorem: Updating Beliefs",
    "uri": "/probintro/intro/05_bayes/index.html"
  },
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing TutorialÂ \u003eÂ Probabilistic Programming with GenJAX",
    "content": "From Simulation to Inference So far, weâ€™ve used GenJAX to generate outcomes â€” simulating what could happen.\nNow weâ€™ll learn to infer â€” reasoning backwards from observations to causes.\nThis is the heart of probabilistic programming!\nRecall: Conditional Probability From the probability tutorial, remember conditional probability:\nâ€œGiven that I observed $B$, whatâ€™s the probability of $A$?â€\nWritten: $P(A \\mid B)$\nMeaning: Restrict the outcome space to only outcomes in $B$, then calculate the probability of $A$ within that restricted space.\nFormula: $P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{|A \\cap B|}{|B|}$\nExample from Set-Based Probability Chibanyâ€™s meals: $\\Omega = \\{HH, HT, TH, TT\\}$\nQuestion: â€œGiven that dinner is Tonkatsu, whatâ€™s the probability lunch was also Tonkatsu?â€\nSet-based solution:\nObserve $D$ = â€œdinner is Tonkatsuâ€ = $\\{HT, TT\\}$ Want $L$ = â€œlunch is Tonkatsuâ€ = $\\{TH, TT\\}$ Intersection: $L \\cap D = \\{TT\\}$ Conditional probability: $P(L \\mid D) = \\frac{|\\{TT\\}|}{|\\{HT, TT\\}|} = \\frac{1}{2}$ Key insight: We restricted the outcome space from $\\{HH, HT, TH, TT\\}$ to just $\\{HT, TT\\}$ (outcomes where dinner = Tonkatsu).\nConditional Probability in GenJAX In GenJAX, we do the same thing â€” but with code instead of sets!\nThree approaches:\nApproach 1: Filtering Simulations (Rejection Sampling) Generate many traces, keep only those matching the observation.\nPseudocode:\n1. Generate many traces 2. Keep only traces where observation is true 3. Among those, count how many satisfy the query 4. Calculate the ratio This is Monte Carlo conditional probability â€” exactly what we did by hand with sets!\nApproach 2: Conditioning with generate GenJAX has built-in support for specifying observations. We provide a choice map with the observed values, and GenJAX generates traces consistent with those observations.\nApproach 3: Full Inference (Importance Sampling, MCMC) More advanced methods that weâ€™ll explore in Chapter 5. These are more efficient when observations are rare.\nThis chapter focuses on Approach 1 and 2 â€” the most intuitive methods.\nApproach 1: Filtering Simulations Letâ€™s answer: â€œGiven dinner is Tonkatsu, whatâ€™s P(lunch is Tonkatsu)?â€\nStep 1: Generate Many Traces 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import jax import jax.numpy as jnp from genjax import gen, bernoulli @gen def chibany_day(): lunch_is_tonkatsu = bernoulli(0.5) @ \"lunch\" dinner_is_tonkatsu = bernoulli(0.5) @ \"dinner\" return (lunch_is_tonkatsu, dinner_is_tonkatsu) # Generate 10,000 days key = jax.random.key(42) keys = jax.random.split(key, 10000) def run_one_day(k): trace = chibany_day.simulate(k, ()) return trace.get_retval() days = jax.vmap(run_one_day)(keys) Step 2: Filter to Observation Observation: Dinner is Tonkatsu (dinner = 1)\n1 2 3 4 5 6 # Filter: keep only days where dinner is Tonkatsu dinner_is_tonkatsu = days[:, 1] == 1 # Count how many days match n_matching = jnp.sum(dinner_is_tonkatsu) print(f\"Days where dinner is Tonkatsu: {n_matching} / {len(days)}\") Output (example):\nDays where dinner is Tonkatsu: 4982 / 10000 This is about 50% â€” makes sense because dinner has 50% probability!\nStep 3: Query Among Filtered Traces Among days where dinner is Tonkatsu, how many also have lunch as Tonkatsu?\n1 2 3 4 5 6 7 # Both meals are Tonkatsu both_tonkatsu = (days[:, 0] == 1) \u0026 (days[:, 1] == 1) # Count n_both = jnp.sum(both_tonkatsu) print(f\"Days with both Tonkatsu: {n_both} / {n_matching}\") Output (example):\nDays with both Tonkatsu: 2491 / 4982 Step 4: Calculate Conditional Probability 1 2 prob_lunch_given_dinner = n_both / n_matching print(f\"P(lunch=T | dinner=T) â‰ˆ {prob_lunch_given_dinner:.3f}\") Output:\nP(lunch=T | dinner=T) â‰ˆ 0.500 Perfect! This matches the theoretical answer (0.5) because lunch and dinner are independent.\nComplete Example: Filtering Hereâ€™s the complete code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import jax import jax.numpy as jnp from genjax import gen, bernoulli @gen def chibany_day(): lunch_is_tonkatsu = bernoulli(0.5) @ \"lunch\" dinner_is_tonkatsu = bernoulli(0.5) @ \"dinner\" return (lunch_is_tonkatsu, dinner_is_tonkatsu) # Generate simulations key = jax.random.key(42) keys = jax.random.split(key, 10000) def run_one_day(k): trace = chibany_day.simulate(k, ()) return trace.get_retval() days = jax.vmap(run_one_day)(keys) # Observation: dinner is Tonkatsu observation_satisfied = days[:, 1] == 1 # Query: lunch is also Tonkatsu query_satisfied = days[:, 0] == 1 # Both observation AND query both_satisfied = observation_satisfied \u0026 query_satisfied # Calculate conditional probability n_observation = jnp.sum(observation_satisfied) n_both = jnp.sum(both_satisfied) prob_conditional = n_both / n_observation print(f\"=== Conditional Probability via Filtering ===\") print(f\"Observation (dinner=T): {n_observation} traces\") print(f\"Both (lunch=T AND dinner=T): {n_both} traces\") print(f\"P(lunch=T | dinner=T) â‰ˆ {prob_conditional:.3f}\") The Pattern Conditional probability via filtering:\nGenerate many traces Filter to observations (keep only matching traces) Count queries among filtered traces Divide to get conditional probability This is rejection sampling â€” the simplest form of inference!\nApproach 2: Conditioning with Choice Maps GenJAX also lets you specify observations when generating traces.\nCreating a Choice Map A choice map is a dictionary specifying values for named random choices:\n1 2 3 4 5 6 from genjax import ChoiceMap # Specify that dinner must be Tonkatsu (1) observations = ChoiceMap({ \"dinner\": 1 }) Generating with Observations Use the generate function instead of simulate:\n1 2 3 4 5 6 7 8 key = jax.random.key(42) # Generate a trace consistent with observations trace, weight = chibany_day.generate(key, (), observations) print(f\"Lunch: {trace.get_choices()['lunch']}\") print(f\"Dinner: {trace.get_choices()['dinner']}\") print(f\"Weight: {weight}\") Output (example):\nLunch: 1 Dinner: 1 # Always 1 because we observed it! Weight: -0.6931471805599453 Whatâ€™s the weight? Itâ€™s the log probability of the observation. Here, $P(\\text{dinner}=1) = 0.5$, so $\\log(0.5) = -0.693â€¦$\ngenerate() vs simulate() simulate(key, args):\nGenerates a trace with all choices random No observations specified Returns just the trace generate(key, args, observations):\nGenerates a trace consistent with observations Specified choices take given values Unspecified choices are random Returns (trace, weight) where weight = log probability of observations When to use which:\nForward simulation (no observations): Use simulate() Conditional sampling (some observations): Use generate() Generating Multiple Conditional Traces Letâ€™s generate 1000 traces where dinner is Tonkatsu:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from genjax import ChoiceMap # Observation: dinner = Tonkatsu observations = ChoiceMap({\"dinner\": 1}) # Generate many conditional traces key = jax.random.key(42) keys = jax.random.split(key, 1000) def run_conditional(k): trace, weight = chibany_day.generate(k, (), observations) return trace.get_retval() conditional_days = jax.vmap(run_conditional)(keys) # Count lunch outcomes lunch_tonkatsu = jnp.sum(conditional_days[:, 0] == 1) print(f\"Among {len(conditional_days)} days where dinner=Tonkatsu:\") print(f\" Lunch is Tonkatsu: {lunch_tonkatsu} ({lunch_tonkatsu/len(conditional_days):.1%})\") print(f\" Lunch is Hamburger: {len(conditional_days) - lunch_tonkatsu} ({(len(conditional_days) - lunch_tonkatsu)/len(conditional_days):.1%})\") Output:\nAmong 1000 days where dinner=Tonkatsu: Lunch is Tonkatsu: 501 (50.1%) Lunch is Hamburger: 499 (49.9%) Perfect! Confirms $P(\\text{lunch}=T \\mid \\text{dinner}=T) = 0.5$\nConnection to the Probability Tutorial Letâ€™s revisit the exact example from Chapter 4 of the probability tutorial!\nScenario: Chibany observes that the student bringing his lunch said â€œHe says itâ€™s from a place starting with T.â€\nIf itâ€™s Tonkatsu, theyâ€™d definitely say â€œTâ€ (P = 1.0) If itâ€™s Hamburger, they might still say â€œTâ€ for â€œThe Burger Placeâ€ (P = 0.3) Question: Whatâ€™s the probability lunch is actually Tonkatsu?\nThe Generative Model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @gen def lunch_with_clue(): \"\"\"Model lunch with a clue about the first letter.\"\"\" # Prior: 50% chance of each meal is_tonkatsu = bernoulli(0.5) @ \"is_tonkatsu\" # Clue depends on the actual meal if is_tonkatsu: # If Tonkatsu, definitely says \"T\" says_t = bernoulli(1.0) @ \"says_t\" else: # If Hamburger, only 30% chance of saying \"T\" says_t = bernoulli(0.3) @ \"says_t\" return is_tonkatsu Prior (Before Hearing the Clue) 1 2 3 4 5 6 7 8 9 10 11 12 # Generate without observations key = jax.random.key(42) keys = jax.random.split(key, 10000) def run_prior(k): trace = lunch_with_clue.simulate(k, ()) return trace.get_retval() prior_samples = jax.vmap(run_prior)(keys) prob_tonkatsu_prior = jnp.mean(prior_samples) print(f\"Prior: P(Tonkatsu) = {prob_tonkatsu_prior:.3f}\") Output:\nPrior: P(Tonkatsu) = 0.500 Makes sense! Before hearing the clue, itâ€™s 50-50.\nPosterior (After Hearing â€œTâ€) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from genjax import ChoiceMap # Observation: heard \"T\" observations = ChoiceMap({\"says_t\": 1}) # Generate conditional on observation def run_posterior(k): trace, weight = lunch_with_clue.generate(k, (), observations) return trace.get_retval() keys = jax.random.split(key, 10000) posterior_samples = jax.vmap(run_posterior)(keys) prob_tonkatsu_posterior = jnp.mean(posterior_samples) print(f\"Posterior: P(Tonkatsu | heard 'T') = {prob_tonkatsu_posterior:.3f}\") Output:\nPosterior: P(Tonkatsu | heard 'T') = 0.769 Perfect! This matches the theoretical answer from Bayesâ€™ theorem:\n$$P(T \\mid \\text{heard â€œTâ€}) = \\frac{P(\\text{heard â€œTâ€} \\mid T) \\cdot P(T)}{P(\\text{heard â€œTâ€})} = \\frac{1.0 \\times 0.5}{0.65} \\approx 0.769$$\nThe probability increased from 50% to 77% after hearing the clue!\nVisualizing Prior vs Posterior 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import matplotlib.pyplot as plt # Data categories = ['Hamburger', 'Tonkatsu'] prior_probs = [1 - prob_tonkatsu_prior, prob_tonkatsu_prior] posterior_probs = [1 - prob_tonkatsu_posterior, prob_tonkatsu_posterior] # Plot fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5)) # Prior ax1.bar(categories, prior_probs, color=['#ff6b6b', '#4ecdc4']) ax1.set_ylabel('Probability') ax1.set_title('Prior: Before Hearing Clue') ax1.set_ylim(0, 1) ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5) for i, prob in enumerate(prior_probs): ax1.text(i, prob + 0.05, f'{prob:.1%}', ha='center', fontweight='bold') # Posterior ax2.bar(categories, posterior_probs, color=['#ff6b6b', '#4ecdc4']) ax2.set_ylabel('Probability') ax2.set_title('Posterior: After Hearing \"T\"') ax2.set_ylim(0, 1) ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5) for i, prob in enumerate(posterior_probs): ax2.text(i, prob + 0.05, f'{prob:.1%}', ha='center', fontweight='bold') plt.tight_layout() plt.show() The visualization shows: Evidence shifts our belief! We started at 50-50, but hearing â€œTâ€ pushed us to 77% Tonkatsu.\nKey Concepts Prior Distribution What we believe before seeing data.\nGenerated by simulate() (no observations) Represents our initial uncertainty Posterior Distribution What we believe after seeing data.\nGenerated by generate() with observations Represents updated beliefs incorporating evidence Bayesâ€™ Theorem in Action GenJAX automatically handles the math:\n$$P(\\text{hypothesis} \\mid \\text{data}) = \\frac{P(\\text{data} \\mid \\text{hypothesis}) \\cdot P(\\text{hypothesis})}{P(\\text{data})}$$\nYou just:\nDefine the generative model (encodes $P(\\text{data} \\mid \\text{hypothesis})$ and $P(\\text{hypothesis})$) Specify observations (the data) Generate conditional traces (GenJAX computes the posterior) No manual Bayesâ€™ rule calculation needed!\nThe Power of Generative Models When you write a generative function, youâ€™re specifying:\nPrior: The distribution of random choices before observations Likelihood: How observations depend on hidden variables Joint distribution: The complete probabilistic model GenJAX handles the inference automatically!\nExercises Exercise 1: Independent Variables Verify that lunch and dinner are independent:\nTask: Show that $P(\\text{lunch}=T \\mid \\text{dinner}=T) = P(\\text{lunch}=T)$\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Generate unconditional samples (prior) key = jax.random.key(42) keys = jax.random.split(key, 10000) def run_prior(k): trace = chibany_day.simulate(k, ()) return trace.get_retval() days = jax.vmap(run_prior)(keys) # Prior: P(lunch=T) prob_lunch_prior = jnp.mean(days[:, 0] == 1) # Conditional: P(lunch=T | dinner=T) dinner_t = days[:, 1] == 1 prob_lunch_given_dinner = jnp.sum((days[:, 0] == 1) \u0026 dinner_t) / jnp.sum(dinner_t) print(f\"P(lunch=T) = {prob_lunch_prior:.3f}\") print(f\"P(lunch=T | dinner=T) = {prob_lunch_given_dinner:.3f}\") print(f\"Independent: {abs(prob_lunch_prior - prob_lunch_given_dinner) \u003c 0.05}\") Expected Output P(lunch=T) = 0.500 P(lunch=T | dinner=T) = 0.500 Independent: True Conclusion: Knowing dinner doesnâ€™t change lunch probability â†’ independent!\nExercise 2: Dependent Variables Create a model where lunch and dinner are not independent:\nScenario: If lunch is Tonkatsu, Chibany wants variety for dinner (only 20% chance of Tonkatsu again). If lunch is Hamburger, he craves Tonkatsu for dinner (80% chance).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @gen def chibany_day_dependent(): \"\"\"Meals where dinner depends on lunch.\"\"\" # Lunch is random (50-50) lunch_is_tonkatsu = bernoulli(0.5) @ \"lunch\" # Dinner depends on lunch! if lunch_is_tonkatsu: # Had Tonkatsu for lunch â†’ wants variety dinner_is_tonkatsu = bernoulli(0.2) @ \"dinner\" else: # Had Hamburger for lunch â†’ craves Tonkatsu dinner_is_tonkatsu = bernoulli(0.8) @ \"dinner\" return (lunch_is_tonkatsu, dinner_is_tonkatsu) Task: Calculate $P(\\text{lunch}=T \\mid \\text{dinner}=T)$ and compare to $P(\\text{lunch}=T)$.\nSolution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Prior: P(lunch=T) keys = jax.random.split(key, 10000) def run_prior(k): trace = chibany_day_dependent.simulate(k, ()) return trace.get_retval() days = jax.vmap(run_prior)(keys) prob_lunch_prior = jnp.mean(days[:, 0] == 1) # Conditional: P(lunch=T | dinner=T) dinner_t = days[:, 1] == 1 prob_lunch_given_dinner = jnp.sum((days[:, 0] == 1) \u0026 dinner_t) / jnp.sum(dinner_t) print(f\"P(lunch=T) = {prob_lunch_prior:.3f}\") print(f\"P(lunch=T | dinner=T) = {prob_lunch_given_dinner:.3f}\") print(f\"Independent: {abs(prob_lunch_prior - prob_lunch_given_dinner) \u003c 0.05}\") Expected output:\nP(lunch=T) = 0.500 P(lunch=T | dinner=T) = 0.200 Independent: False Explanation:\nUnconditionally, lunch is 50-50 But if we know dinner=T, itâ€™s more likely lunch was H (because Tâ†’T is only 20%)! So $P(\\text{lunch}=T \\mid \\text{dinner}=T) = 0.2 \\neq 0.5 = P(\\text{lunch}=T)$ Theyâ€™re dependent! Knowing dinner tells us about lunch.\nExercise 3: Multiple Observations Extend the lunch clue model to multiple observations:\nScenario:\nStudent says â€œIt starts with Tâ€ You smell the food and it smells fried (90% if Tonkatsu, 50% if Hamburger) Task: Calculate $P(\\text{Tonkatsu} \\mid \\text{says T AND smells fried})$\nSolution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @gen def lunch_with_multiple_clues(): \"\"\"Model with two pieces of evidence.\"\"\" is_tonkatsu = bernoulli(0.5) @ \"is_tonkatsu\" # Clue 1: What they say if is_tonkatsu: says_t = bernoulli(1.0) @ \"says_t\" else: says_t = bernoulli(0.3) @ \"says_t\" # Clue 2: Smell if is_tonkatsu: smells_fried = bernoulli(0.9) @ \"smells_fried\" else: smells_fried = bernoulli(0.5) @ \"smells_fried\" return is_tonkatsu # Observations: both clues from genjax import ChoiceMap observations = ChoiceMap({ \"says_t\": 1, \"smells_fried\": 1 }) # Generate posterior samples key = jax.random.key(42) keys = jax.random.split(key, 10000) def run_posterior(k): trace, weight = lunch_with_multiple_clues.generate(k, (), observations) return trace.get_retval() posterior = jax.vmap(run_posterior)(keys) prob_tonkatsu = jnp.mean(posterior) print(f\"P(Tonkatsu | says T AND smells fried) = {prob_tonkatsu:.3f}\") Expected: Higher than 0.769 (from single clue) because we have more evidence!\nWhat Youâ€™ve Learned In this chapter, you learned:\nâœ… Conditional probability â€” restriction to observations âœ… Filtering approach â€” rejection sampling for inference âœ… generate() function â€” conditioning with choice maps âœ… Prior vs Posterior â€” beliefs before and after data âœ… Bayesâ€™ theorem in action â€” automatic Bayesian update âœ… Dependent vs Independent â€” how observations provide information\nThe key insight: Probabilistic programming lets you ask questions instead of just generate samples!\nNext Steps You now know how to:\nGenerate samples (simulation) Condition on observations (inference) Calculate conditional probabilities Next up: Chapter 5 applies these ideas to a real problem â€” the taxicab scenario from the probability tutorial!\nYouâ€™ll see how Bayesâ€™ theorem solves practical inference problems, and why base rates matter.\nâ† Previous: Understanding Traces Next: Inference in Action â†’",
    "description": "From Simulation to Inference So far, weâ€™ve used GenJAX to generate outcomes â€” simulating what could happen.\nNow weâ€™ll learn to infer â€” reasoning backwards from observations to causes.\nThis is the heart of probabilistic programming!\nRecall: Conditional Probability From the probability tutorial, remember conditional probability:\nâ€œGiven that I observed $B$, whatâ€™s the probability of $A$?â€\nWritten: $P(A \\mid B)$",
    "tags": [],
    "title": "Conditioning and Observations",
    "uri": "/probintro/genjax/04_conditioning/index.html"
  },
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing TutorialÂ \u003eÂ A Narrative Introduction to Probability",
    "content": "This glossary provides definitions for key terms used throughout the tutorial. Click on any term to expand its definition.\nCore Concepts set set A set is a collection of elements or members. Sets are defined by the elements they do or do not contain. The elements are listed with commas between them and â€œ$\\{$â€ denotes the start of a set and â€œ$\\}$â€ the end of a set. Note that the elements of a set are unique.\nExample: $\\{H, T\\}$ is a set containing two elements: H and T.\noutcome space outcome space The outcome space (denoted $\\Omega$, the Greek letter omega) is the set of all possible outcomes for a random process. It forms the foundation for calculating probabilities.\nExample: For Chibanyâ€™s two daily meals, $\\Omega = \\{HH, HT, TH, TT\\}$.\nevent event An event is a set that contains none, some, or all of the possible outcomes. In other words, an event is any subset of the outcome space $\\Omega$.\nExample: â€œAt least one tonkatsuâ€ is the event $\\{HT, TH, TT\\} \\subseteq \\Omega$.\ncardinality cardinality The cardinality or size of a set is the number of elements it contains. If $A = \\{H, T\\}$, then the cardinality of $A$ is $|A|=2$.\nNotation: $|A|$ means â€œthe size of set $A$â€\nProbability Concepts probability probability The probability of an event $A$ relative to an outcome space $\\Omega$ is the ratio of their sizes: $P(A) = \\frac{|A|}{|\\Omega|}$.\nWhen outcomes are weighted (not equally likely), we sum the weights instead of counting.\nInterpretation: â€œWhat fraction of possible outcomes are in event $A$?â€\nconditional probability conditional probability The conditional probability is the probability of an event conditioned on knowledge of another event. Conditioning on an event means that the possible outcomes in that event form the set of possibilities or outcome space. We then calculate probabilities as normal within that restricted outcome space.\nFormally, this is written as $P(A \\mid B) = \\frac{|A \\cap B|}{|B|}$, where everything to the left of the $\\mid$ is what weâ€™re interested in knowing the probability of and everything to the right of the $\\mid$ is what we know to be true.\nAlternative formula: $P(A \\mid B) = \\frac{P(A,B)}{P(B)}$ (assuming $P(B) \u003e 0$)\nthe other definition of conditional probability the other definition of conditional probability Using joint and marginal probabilities, conditional probability can be defined as the ratio of the joint probability to the marginal probability of the conditioned information:\n$$P(A \\mid B) = \\frac{P(A,B)}{P(B)}$$\nThis is equivalent to the set-based definition but uses probability formulas instead of counting.\nmarginal probability marginal probability A marginal probability is the probability of a random variable that has been calculated by summing over the possible values of one or more other random variables.\nFormula: $P(A) = \\sum_{b} P(A, B=b)$\nIntuition: â€œWhatâ€™s the probability of $A$ regardless of what $B$ is?â€\njoint probability joint probability The joint probability is the probability that multiple events all occur. This corresponds to the intersection of the events (outcomes that are in all the events).\nNotation: $P(A, B)$ or $P(A \\cap B)$\nIntuition: â€œWhatâ€™s the probability that both $A$ and $B$ happen?â€\nRelationships Between Events dependence dependence When knowing the outcome of one random variable or event influences the probability of another, those variables or events are called dependent. This is denoted as $A \\not\\perp B$.\nWhen they do not influence each other, they are called independent. This is denoted as $A \\perp B$.\nFormal definition of independence: $P(A \\mid B) = P(A)$, or equivalently, $P(A, B) = P(A) \\times P(B)$\nRandom Variables random variable random variable A random variable is a function that maps from the set of possible outcomes to some set or space. The output or range of the function could be the set of outcomes again, a whole number based on the outcome (e.g., counting the number of Tonkatsu), or something more complex (e.g., the worldâ€™s friendship matrix, an 8-billion by 8-billion binary matrix where $N_{1,100}=1$ if person 1 is friends with person 100).\nTechnically the output must be measurable. You shouldnâ€™t worry about that distinction unless your random variableâ€™s output gets really, really big (like continuous). Weâ€™ll talk more about probabilities over continuous random variables later.\nKey insight: Itâ€™s called â€œrandomâ€ because its value depends on which outcome occurs, but itâ€™s really just a function!\nAdvanced Concepts Bayes theorem Bayes Theorem Bayes Theorem (or Bayesâ€™ rule) is a formula for reversing the order that variables are conditioned â€” how to go from $P(A \\mid B)$ to $P(B \\mid A)$.\nFormula: $P(H \\mid D) = \\frac{P(D \\mid H) P(H)}{P(D)}$\nComponents:\n$P(H \\mid D)$ = posterior (updated belief after seeing data) $P(D \\mid H)$ = likelihood (how well data fits hypothesis) $P(H)$ = prior (belief before seeing data) $P(D)$ = evidence (total probability of data) Application: Updating beliefs with new information\ngenerative process generative process A generative process defines the probabilities for possible outcomes according to an algorithm with random choices. Think of it as a recipe for producing outcomes.\nExample: â€œFlip two coins: first for lunch (H or T), second for dinner (H or T). Record the pair.â€\nThis connects to probabilistic programming, where we write code that generates outcomes.\nprobabilistic computing probabilistic computing Probabilistic computing refers to programming languages and systems for specifying probabilistic models and performing inference (calculating different probabilities according to the model) in an efficient manner.\nExamples: GenJAX, PyMC, Stan, Turing.jl\nKey idea: Instead of listing all outcomes by hand, write code that generates them, and let the computer do the counting!\nAdditional Terms Monte Carlo simulation Monte Carlo simulation A computational method for approximating probabilities by generating many random samples and counting outcomes. Named after the Monte Carlo casino.\nProcess:\nGenerate many random outcomes (e.g., 10,000 simulated days) Count how many satisfy your event Calculate the ratio When useful: When outcome spaces are too large to enumerate by hand\ntrace trace In probabilistic programming, a trace records all random choices made during one execution of a generative function, along with their addresses (names) and the return value.\nThink of it as: A complete record of â€œwhat happenedâ€ during one run of a probabilistic program\nUsed in: GenJAX and other probabilistic programming systems\ngenerative function generative function In GenJAX and similar systems, a generative function is a Python function decorated with @gen that can make addressed random choices. It represents a probability distribution over its return values.\nExample:\n1 2 3 4 @gen def coin_flip(): result = bernoulli(0.5) @ \"flip\" return result choice map choice map A dictionary-like structure in GenJAX that maps addresses (names) to the values of random choices. Used for:\nRecording what random choices were made (from traces) Specifying observations for inference Constraining random choices Think of it as: A way to name and track all the random decisions\nâ† Previous: Bayesâ€™ Theorem Next: Acknowledgements â†’",
    "description": "This glossary provides definitions for key terms used throughout the tutorial. Click on any term to expand its definition.\nCore Concepts set set A set is a collection of elements or members. Sets are defined by the elements they do or do not contain. The elements are listed with commas between them and â€œ$\\{$â€ denotes the start of a set and â€œ$\\}$â€ the end of a set. Note that the elements of a set are unique.",
    "tags": [],
    "title": "Glossary",
    "uri": "/probintro/intro/06_glossary/index.html"
  },
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing TutorialÂ \u003eÂ Probabilistic Programming with GenJAX",
    "content": "Solving Real Problems with Probabilistic Code Remember the taxicab problem from the probability tutorial?\nScenario: Chibany witnesses a hit-and-run at night. He says the taxi was blue. But:\n85% of taxis are green, 15% are blue Chibany identifies colors correctly 80% of the time Question: Whatâ€™s the probability it was actually a blue taxi?\nIn the probability tutorial, we solved this with sets and Bayesâ€™ theorem. Now weâ€™ll solve it with GenJAX!\nThe Taxicab Problem: Quick Recap The Setup Base rates:\n$P(\\text{Blue}) = 0.15$ (15% of taxis are blue) $P(\\text{Green}) = 0.85$ (85% of taxis are green) Chibanyâ€™s accuracy:\n$P(\\text{says Blue} \\mid \\text{Blue}) = 0.80$ (correct 80% of the time) $P(\\text{says Green} \\mid \\text{Green}) = 0.80$ (correct 80% of the time) Therefore, $P(\\text{says Blue} \\mid \\text{Green}) = 0.20$ (mistakes 20% of the time) Observation: Chibany says â€œBlueâ€\nQuestion: $P(\\text{Blue} \\mid \\text{says Blue}) = $ ?\nThe Generative Model Letâ€™s express this as a GenJAX generative function:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import jax import jax.numpy as jnp from genjax import gen, bernoulli @gen def taxicab_model(base_rate_blue=0.15, accuracy=0.80): \"\"\"Generate the taxi color and what Chibany says. Args: base_rate_blue: Probability a taxi is blue (default 0.15) accuracy: Probability Chibany identifies correctly (default 0.80) Returns: True if taxi is blue, False if green \"\"\" # True taxi color (blue = 1, green = 0) is_blue = bernoulli(base_rate_blue) @ \"is_blue\" # What Chibany says depends on the true color if is_blue: # If blue, says \"blue\" with probability = accuracy says_blue = bernoulli(accuracy) @ \"says_blue\" else: # If green, says \"blue\" with probability = 1 - accuracy (mistake) says_blue = bernoulli(1 - accuracy) @ \"says_blue\" return is_blue What this encodes:\nPrior: Taxis are blue 15% of the time (base rate) Likelihood: How observation (â€œsays blueâ€) depends on true color Complete model: Joint distribution over true color and observation Approach 1: Filtering (Rejection Sampling) Letâ€™s solve it by generating many scenarios and filtering to the observation.\nStep 1: Generate Many Scenarios 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Generate 100,000 scenarios key = jax.random.key(42) keys = jax.random.split(key, 100000) def run_scenario(k): trace = taxicab_model.simulate(k, (0.15, 0.80)) return { 'is_blue': trace.get_choices()['is_blue'], 'says_blue': trace.get_choices()['says_blue'] } # Vectorized version def run_scenario_vec(k): trace = taxicab_model.simulate(k, (0.15, 0.80)) choices = trace.get_choices() return jnp.array([choices['is_blue'], choices['says_blue']]) scenarios = jax.vmap(run_scenario_vec)(keys) is_blue = scenarios[:, 0] says_blue = scenarios[:, 1] Step 2: Filter to Observation Observation: Chibany says â€œblueâ€\n1 2 3 4 5 # Keep only scenarios where Chibany says \"blue\" observation_satisfied = says_blue == 1 n_says_blue = jnp.sum(observation_satisfied) print(f\"Scenarios where Chibany says blue: {n_says_blue} / {len(scenarios)}\") Output (example):\nScenarios where Chibany says blue: 29017 / 100000 Why ~29%?\n$P(\\text{says Blue}) = P(\\text{Blue}) \\cdot P(\\text{says Blue} \\mid \\text{Blue}) + P(\\text{Green}) \\cdot P(\\text{says Blue} \\mid \\text{Green})$ $= 0.15 \\times 0.80 + 0.85 \\times 0.20 = 0.12 + 0.17 = 0.29$ Step 3: Count True Positives Among scenarios where he says â€œblueâ€, how many are actually blue?\n1 2 3 4 5 # Both says blue AND is blue both_blue = observation_satisfied \u0026 (is_blue == 1) n_actually_blue = jnp.sum(both_blue) print(f\"Scenarios where taxi IS blue: {n_actually_blue} / {n_says_blue}\") Output (example):\nScenarios where taxi IS blue: 12038 / 29017 Step 4: Calculate Posterior 1 2 prob_blue_given_says_blue = n_actually_blue / n_says_blue print(f\"\\nP(Blue | says Blue) â‰ˆ {prob_blue_given_says_blue:.3f}\") Output:\nP(Blue | says Blue) â‰ˆ 0.415 Only 41.5%! Even though Chibany is 80% accurate, thereâ€™s less than 50% chance the taxi was actually blue!\nThe Base Rate Strikes Again! Why so low?\nEven though Chibany is 80% accurate, most taxis are green (85%). So even with his 20% error rate on green taxis, there are more green taxis misidentified as blue than there are actual blue taxis!\nThe numbers:\nBlue taxis correctly identified: $0.15 \\times 0.80 = 0.12$ (12%) Green taxis incorrectly identified: $0.85 \\times 0.20 = 0.17$ (17%) More false positives than true positives!\nThis is why the posterior is only 41.5% â‰ˆ 12/(12+17).\nApproach 2: Using generate() with Observations Now letâ€™s use GenJAXâ€™s built-in conditioning:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from genjax import ChoiceMap # Observation: Chibany says \"blue\" observation = ChoiceMap({\"says_blue\": 1}) # Generate 10,000 traces conditional on observation key = jax.random.key(42) keys = jax.random.split(key, 10000) def run_conditional(k): trace, weight = taxicab_model.generate(k, (0.15, 0.80), observation) return trace.get_retval() # Returns is_blue posterior_samples = jax.vmap(run_conditional)(keys) # Calculate posterior probability prob_blue_posterior = jnp.mean(posterior_samples) print(f\"P(Blue | says Blue) â‰ˆ {prob_blue_posterior:.3f}\") Output:\nP(Blue | says Blue) â‰ˆ 0.414 Same answer! Both methods work â€” generate() is just more convenient.\nTheoretical Answer (Bayesâ€™ Theorem) Letâ€™s verify against the exact Bayesâ€™ theorem calculation:\n$$P(\\text{Blue} \\mid \\text{says Blue}) = \\frac{P(\\text{says Blue} \\mid \\text{Blue}) \\cdot P(\\text{Blue})}{P(\\text{says Blue})}$$\nCalculate each term:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Prior P_blue = 0.15 P_green = 0.85 # Likelihood P_says_blue_given_blue = 0.80 P_says_blue_given_green = 0.20 # Evidence (total probability of saying blue) P_says_blue = (P_blue * P_says_blue_given_blue + P_green * P_says_blue_given_green) # Posterior (Bayes' theorem) P_blue_given_says_blue = (P_says_blue_given_blue * P_blue) / P_says_blue print(f\"=== Bayes' Theorem Calculation ===\") print(f\"P(Blue) = {P_blue}\") print(f\"P(says Blue | Blue) = {P_says_blue_given_blue}\") print(f\"P(says Blue | Green) = {P_says_blue_given_green}\") print(f\"P(says Blue) = {P_says_blue}\") print(f\"\\nP(Blue | says Blue) = {P_blue_given_says_blue:.3f}\") Output:\n=== Bayes' Theorem Calculation === P(Blue) = 0.15 P(says Blue | Blue) = 0.8 P(says Blue | Green) = 0.2 P(says Blue) = 0.29 P(Blue | says Blue) = 0.414 Perfect match! GenJAX simulation â‰ˆ 0.415, Bayesâ€™ theorem exact = 0.414\nVisualizing Prior vs Posterior Letâ€™s visualize how our beliefs change:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import matplotlib.pyplot as plt # Prior: before observation prior_blue = 0.15 prior_green = 0.85 # Posterior: after observation posterior_blue = prob_blue_posterior # From simulation posterior_green = 1 - posterior_blue # Plot fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5)) categories = ['Green', 'Blue'] colors = ['#4ecdc4', '#6c5ce7'] # Prior ax1.bar(categories, [prior_green, prior_blue], color=colors) ax1.set_ylabel('Probability', fontsize=12) ax1.set_title('Prior: Before Chibany Speaks', fontsize=14, fontweight='bold') ax1.set_ylim(0, 1) ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='50%') for i, prob in enumerate([prior_green, prior_blue]): ax1.text(i, prob + 0.05, f'{prob:.1%}', ha='center', fontsize=14, fontweight='bold') # Posterior ax2.bar(categories, [posterior_green, posterior_blue], color=colors) ax2.set_ylabel('Probability', fontsize=12) ax2.set_title('Posterior: After Chibany Says \"Blue\"', fontsize=14, fontweight='bold') ax2.set_ylim(0, 1) ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='50%') for i, prob in enumerate([posterior_green, posterior_blue]): ax2.text(i, prob + 0.05, f'{prob:.1%}', ha='center', fontsize=14, fontweight='bold') plt.tight_layout() plt.show() print(f\"\\nğŸ“Š Belief Update:\") print(f\" Before: P(Blue) = {prior_blue:.1%}\") print(f\" After: P(Blue | says Blue) = {posterior_blue:.1%}\") print(f\" Change: +{(posterior_blue - prior_blue):.1%}\") Key insight: Evidence increased our belief in blue from 15% to 41%, but still not even 50% because the base rate is so strong!\nWhat If the Base Rate Were Different? Letâ€™s explore how changing the base rate affects the answer.\nScenario 1: Equal taxis (50% blue, 50% green)\n1 2 3 4 5 6 7 8 9 10 11 12 # Generate with 50-50 base rate observation = ChoiceMap({\"says_blue\": 1}) def run_equal_base(k): trace, weight = taxicab_model.generate(k, (0.50, 0.80), observation) return trace.get_retval() keys = jax.random.split(key, 10000) posterior_equal = jax.vmap(run_equal_base)(keys) prob_equal = jnp.mean(posterior_equal) print(f\"If 50% blue: P(Blue | says Blue) = {prob_equal:.3f}\") Output:\nIf 50% blue: P(Blue | says Blue) = 0.800 Now itâ€™s 80%! When base rates are equal, accuracy dominates.\nScenario 2: Mostly blue (85% blue, 15% green)\n1 2 3 4 5 6 7 8 def run_mostly_blue(k): trace, weight = taxicab_model.generate(k, (0.85, 0.80), observation) return trace.get_retval() posterior_mostly_blue = jax.vmap(run_mostly_blue)(keys) prob_mostly_blue = jnp.mean(posterior_mostly_blue) print(f\"If 85% blue: P(Blue | says Blue) = {prob_mostly_blue:.3f}\") Output:\nIf 85% blue: P(Blue | says Blue) = 0.971 Now itâ€™s 97%! When most taxis are blue, seeing â€œblueâ€ is strong evidence.\nVisualizing the Effect of Base Rates 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # Test different base rates base_rates = jnp.linspace(0.01, 0.99, 50) posteriors = [] for rate in base_rates: def run_with_rate(k): trace, weight = taxicab_model.generate(k, (float(rate), 0.80), observation) return trace.get_retval() keys = jax.random.split(key, 1000) post = jax.vmap(run_with_rate)(keys) posteriors.append(jnp.mean(post)) # Plot plt.figure(figsize=(10, 6)) plt.plot(base_rates, posteriors, linewidth=2, color='#6c5ce7') plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='50% threshold') plt.axvline(x=0.15, color='red', linestyle='--', alpha=0.7, label='Original problem (15%)') plt.scatter([0.15], [0.414], color='red', s=100, zorder=5) plt.xlabel('Base Rate: P(Blue)', fontsize=12) plt.ylabel('Posterior: P(Blue | says Blue)', fontsize=12) plt.title('How Base Rates Affect Inference\\n(Chibany 80% accurate)', fontsize=14, fontweight='bold') plt.grid(alpha=0.3) plt.legend(fontsize=10) plt.tight_layout() plt.show() The graph shows: Even with high accuracy (80%), the posterior depends heavily on the base rate!\nThe Lesson Base rates matter enormously in real-world inference!\nMedical tests, fraud detection, witness testimony â€” all require considering:\nHow accurate is the test/witness? (likelihood) How common is the condition/crime? (prior/base rate) Ignoring base rates leads to wrong conclusions.\nThis is called base rate neglect â€” a common cognitive bias.\nComplete Code Example Hereâ€™s everything together:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import jax import jax.numpy as jnp from genjax import gen, bernoulli, ChoiceMap import matplotlib.pyplot as plt @gen def taxicab_model(base_rate_blue=0.15, accuracy=0.80): \"\"\"Taxicab problem generative model.\"\"\" is_blue = bernoulli(base_rate_blue) @ \"is_blue\" if is_blue: says_blue = bernoulli(accuracy) @ \"says_blue\" else: says_blue = bernoulli(1 - accuracy) @ \"says_blue\" return is_blue # Observation: Chibany says \"blue\" observation = ChoiceMap({\"says_blue\": 1}) # Generate posterior samples key = jax.random.key(42) keys = jax.random.split(key, 10000) def run_inference(k): trace, weight = taxicab_model.generate(k, (0.15, 0.80), observation) return trace.get_retval() posterior_samples = jax.vmap(run_inference)(keys) prob_blue = jnp.mean(posterior_samples) print(f\"=== TAXICAB INFERENCE ===\") print(f\"Base rate: 15% blue\") print(f\"Accuracy: 80%\") print(f\"Observation: Says 'blue'\") print(f\"\\nP(Blue | says Blue) â‰ˆ {prob_blue:.3f}\") Exercises Exercise 1: Higher Accuracy What if Chibany were 95% accurate instead of 80%?\nTask: Modify the code to use accuracy=0.95 and calculate the posterior.\nSolution 1 2 3 4 5 6 7 8 9 def run_high_accuracy(k): trace, weight = taxicab_model.generate(k, (0.15, 0.95), observation) return trace.get_retval() keys = jax.random.split(key, 10000) posterior_high_acc = jax.vmap(run_high_accuracy)(keys) prob_high_acc = jnp.mean(posterior_high_acc) print(f\"With 95% accuracy: P(Blue | says Blue) = {prob_high_acc:.3f}\") Expected: â‰ˆ 0.75 (75%)\nMuch higher! Accuracy matters, but even at 95%, base rates still pull it below 100%.\nTheoretical: $$P = \\frac{0.95 \\times 0.15}{0.95 \\times 0.15 + 0.05 \\times 0.85} = \\frac{0.1425}{0.1850} \\approx 0.770$$\nExercise 2: Opposite Observation What if Chibany said â€œgreenâ€ instead of â€œblueâ€?\nTask: Calculate $P(\\text{Blue} \\mid \\text{says Green})$\nSolution 1 2 3 4 5 6 7 8 9 10 11 12 # Observation: says \"green\" observation_green = ChoiceMap({\"says_blue\": 0}) def run_says_green(k): trace, weight = taxicab_model.generate(k, (0.15, 0.80), observation_green) return trace.get_retval() keys = jax.random.split(key, 10000) posterior_green = jax.vmap(run_says_green)(keys) prob_blue_given_green = jnp.mean(posterior_green) print(f\"P(Blue | says Green) = {prob_blue_given_green:.3f}\") Expected: â‰ˆ 0.041 (4.1%)\nVery low! If Chibany (80% accurate) says â€œgreenâ€, itâ€™s very likely green.\nTheoretical: $$P = \\frac{0.20 \\times 0.15}{0.20 \\times 0.15 + 0.80 \\times 0.85} = \\frac{0.03}{0.71} \\approx 0.042$$\nExercise 3: Two Witnesses What if two independent witnesses both say â€œblueâ€?\nTask: Extend the model to include two witnesses, both 80% accurate. Calculate the posterior.\nSolution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 @gen def taxicab_two_witnesses(base_rate_blue=0.15, accuracy=0.80): \"\"\"Two independent witnesses.\"\"\" is_blue = bernoulli(base_rate_blue) @ \"is_blue\" # Witness 1 if is_blue: witness1 = bernoulli(accuracy) @ \"witness1\" else: witness1 = bernoulli(1 - accuracy) @ \"witness1\" # Witness 2 (independent) if is_blue: witness2 = bernoulli(accuracy) @ \"witness2\" else: witness2 = bernoulli(1 - accuracy) @ \"witness2\" return is_blue # Both say \"blue\" observation_two = ChoiceMap({\"witness1\": 1, \"witness2\": 1}) def run_two_witnesses(k): trace, weight = taxicab_two_witnesses.generate(k, (0.15, 0.80), observation_two) return trace.get_retval() keys = jax.random.split(key, 10000) posterior_two = jax.vmap(run_two_witnesses)(keys) prob_two = jnp.mean(posterior_two) print(f\"P(Blue | both say Blue) = {prob_two:.3f}\") Expected: â‰ˆ 0.73 (73%)\nMuch higher! Two independent pieces of evidence are much stronger.\nTheoretical: $$P(\\text{both say Blue} \\mid \\text{Blue}) = 0.80^2 = 0.64$$ $$P(\\text{both say Blue} \\mid \\text{Green}) = 0.20^2 = 0.04$$ $$P = \\frac{0.64 \\times 0.15}{0.64 \\times 0.15 + 0.04 \\times 0.85} = \\frac{0.096}{0.130} \\approx 0.738$$\nTwo witnesses push us above 50% despite the low base rate!\nWhat Youâ€™ve Learned In this chapter, you:\nâœ… Implemented a real inference problem â€” the taxicab scenario âœ… Used filtering and generate() â€” two approaches to conditioning âœ… Saw Bayesâ€™ theorem in action â€” automatic Bayesian update âœ… Understood base rate effects â€” why priors matter enormously âœ… Explored parameter sensitivity â€” how accuracy and base rates interact âœ… Calculated with code, not formulas â€” GenJAX does the math\nThe key insight: Probabilistic programming lets you encode assumptions (generative model) and ask questions (conditioning) without manual Bayesâ€™ rule calculations!\nWhy This Matters Real-world applications:\nMedical diagnosis: Test accuracy + disease prevalence â†’ probability of disease Fraud detection: Transaction patterns + fraud base rate â†’ probability of fraud Spam filtering: Email features + spam base rate â†’ probability of spam Criminal justice: Witness accuracy + crime base rate â†’ probability of guilt All follow the same pattern:\nDefine generative model (how data arises) Observe data Infer hidden causes GenJAX makes this systematic and scalable.\nNext Steps You now know:\nHow to build generative models How to perform inference with observations How to interpret posterior probabilities Why base rates matter Final chapter: Chapter 6 shows you how to build your own models from scratch!\nâ† Previous: Conditioning and Observations Next: Building Your Own Models â†’",
    "description": "Solving Real Problems with Probabilistic Code Remember the taxicab problem from the probability tutorial?\nScenario: Chibany witnesses a hit-and-run at night. He says the taxi was blue. But:\n85% of taxis are green, 15% are blue Chibany identifies colors correctly 80% of the time Question: Whatâ€™s the probability it was actually a blue taxi?\nIn the probability tutorial, we solved this with sets and Bayesâ€™ theorem. Now weâ€™ll solve it with GenJAX!",
    "tags": [],
    "title": "Inference in Action: The Taxicab Problem",
    "uri": "/probintro/genjax/05_inference/index.html"
  },
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing TutorialÂ \u003eÂ A Narrative Introduction to Probability",
    "content": "Written by Joe Austerweil. Thank you to Kyana Burhite, Hongtao Hao, and the many students who took Human and Machine Learning over the years who have provided invaluable feedback on an early draft of this tutorial. Further thanks to the Japan Probabilistic Computing Consortium Association (JPCCA) for funding my ability to polish and publish this tutorial series.\nPlease reach out to Joe via email if you have any constructive feedback (anything from X could be more clear or this is a great resource I will share with my class).\nprev: glossary",
    "description": "Written by Joe Austerweil. Thank you to Kyana Burhite, Hongtao Hao, and the many students who took Human and Machine Learning over the years who have provided invaluable feedback on an early draft of this tutorial. Further thanks to the Japan Probabilistic Computing Consortium Association (JPCCA) for funding my ability to polish and publish this tutorial series.\nPlease reach out to Joe via email if you have any constructive feedback (anything from X could be more clear or this is a great resource I will share with my class).",
    "tags": [],
    "title": "Acknowledgements",
    "uri": "/probintro/intro/07_ack/index.html"
  },
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing TutorialÂ \u003eÂ Probabilistic Programming with GenJAX",
    "content": "From Following Recipes to Creating Your Own Youâ€™ve learned to use GenJAX through examples. Now itâ€™s time to build your own probabilistic models!\nThis chapter shows you how to think about building generative models â€” turning real-world problems into code.\nThe Model-Building Process Step 1: Understand the Problem Before writing any code, answer:\nWhat am I trying to predict or understand? (The question) What do I observe? (The data/evidence) Whatâ€™s hidden? (The unknown variables) How are they related? (The causal structure) Example: Spam detection\nQuestion: Is this email spam? Observations: Email content, sender, time Hidden: True spam status Relationship: Spam emails have certain word patterns Step 2: Sketch the Generative Story Write out the process that generates the data:\nâ€œFirst, nature choosesâ€¦, then based on that, it generatesâ€¦, which producesâ€¦â€\nExample: Coin flips\nFirst, the coin has a (hidden) bias parameter Based on that bias, each flip is heads or tails We observe a sequence of flips This narrative becomes your code!\nStep 3: Choose Distributions For each random choice, pick a distribution:\nType of Variable Common Distributions Binary (yes/no) bernoulli(p) Categorical (A/B/C) categorical(probs) Count (0, 1, 2, â€¦) poisson(rate) Continuous normal(mean, std), uniform(low, high) Start simple! Use bernoulli for most binary choices.\nStep 4: Write the Code Pattern:\n1 2 3 4 5 6 7 8 9 10 11 12 13 @gen def my_model(parameters): # Hidden variables (causes) hidden = distribution(...) @ \"hidden\" # Observed variables (effects) # Usually depend on hidden variables if hidden: observed = distribution_A(...) @ \"observed\" else: observed = distribution_B(...) @ \"observed\" return hidden # Or whatever you want to predict Key points:\nUse @gen decorator Name all random choices with @ \"name\" Return what you want to infer Use if statements to model dependencies Step 5: Test and Validate Generate samples â€” does the output look reasonable? Check extreme cases â€” what if parameters are 0 or 1? Verify inference â€” do posterior results make intuitive sense? Common Patterns Pattern 1: Independent Observations Scenario: Multiple independent measurements\nExample: Coin flips\n1 2 3 4 5 6 7 8 9 10 11 @gen def coin_flips(n_flips, bias=0.5): \"\"\"Generate n independent coin flips.\"\"\" results = [] for i in range(n_flips): # Each flip is independent flip = bernoulli(bias) @ f\"flip_{i}\" results.append(flip) return jnp.array(results) Usage:\n1 2 3 4 key = jax.random.key(42) trace = coin_flips.simulate(key, (10, 0.7)) flips = trace.get_retval() print(f\"Flips: {flips}\") Pattern 2: Hierarchical Structure Scenario: Parameters have their own distributions\nExample: Learning a coinâ€™s bias from flips\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @gen def coin_with_unknown_bias(n_flips): \"\"\"Coin with unknown bias â€” infer it from flips.\"\"\" # Hidden: the coin's true bias (uniform between 0 and 1) bias = uniform(0.0, 1.0) @ \"bias\" # Observations: flip outcomes flips = [] for i in range(n_flips): flip = bernoulli(bias) @ f\"flip_{i}\" flips.append(flip) return bias # Want to infer this! Inference:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from genjax import ChoiceMap # Observe 7 heads out of 10 flips observations = ChoiceMap({ \"flip_0\": 1, \"flip_1\": 1, \"flip_2\": 0, \"flip_3\": 1, \"flip_4\": 1, \"flip_5\": 0, \"flip_6\": 1, \"flip_7\": 1, \"flip_8\": 0, \"flip_9\": 1 }) # Infer bias keys = jax.random.split(key, 1000) def infer_bias(k): trace, weight = coin_with_unknown_bias.generate(k, (10,), observations) return trace.get_retval() posterior_bias = jax.vmap(infer_bias)(keys) mean_bias = jnp.mean(posterior_bias) print(f\"Estimated bias: {mean_bias:.2f}\") # Should be around 0.70 (7 heads / 10 flips) Pattern 3: Conditional Dependencies Scenario: Observations depend on hidden state\nExample: Weather affects mood\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @gen def mood_model(): \"\"\"Weather affects Chibany's mood.\"\"\" # Hidden: today's weather is_sunny = bernoulli(0.7) @ \"is_sunny\" # 70% sunny days # Observable: Chibany's mood depends on weather if is_sunny: # Sunny â†’ happy 90% of the time is_happy = bernoulli(0.9) @ \"is_happy\" else: # Rainy â†’ happy only 30% of the time is_happy = bernoulli(0.3) @ \"is_happy\" return is_sunny Question: â€œChibany is happy. Whatâ€™s the probability itâ€™s sunny?â€\n1 2 3 4 5 6 7 8 9 10 11 observation = ChoiceMap({\"is_happy\": 1}) def infer_weather(k): trace, weight = mood_model.generate(k, (), observation) return trace.get_retval() keys = jax.random.split(key, 10000) posterior_sunny = jax.vmap(infer_weather)(keys) prob_sunny = jnp.mean(posterior_sunny) print(f\"P(Sunny | Happy) â‰ˆ {prob_sunny:.3f}\") Theoretical Answer Using Bayesâ€™ theorem:\n$$P(\\text{Sunny} \\mid \\text{Happy}) = \\frac{P(\\text{Happy} \\mid \\text{Sunny}) \\cdot P(\\text{Sunny})}{P(\\text{Happy})}$$\n$P(\\text{Sunny}) = 0.7$ $P(\\text{Happy} \\mid \\text{Sunny}) = 0.9$ $P(\\text{Happy} \\mid \\text{Rainy}) = 0.3$ $P(\\text{Happy}) = 0.7 \\times 0.9 + 0.3 \\times 0.3 = 0.63 + 0.09 = 0.72$ $$P = \\frac{0.9 \\times 0.7}{0.72} = \\frac{0.63}{0.72} \\approx 0.875$$\nExpected: â‰ˆ 87.5%\nPattern 4: Sequences and Time Series Scenario: Events unfold over time\nExample: Chibanyâ€™s weekly meals\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @gen def weekly_meals(days=7): \"\"\"Model a week of meals with memory.\"\"\" meals = [] # First day is random prev_meal = bernoulli(0.5) @ \"day_0\" meals.append(prev_meal) # Each subsequent day depends on previous day for day in range(1, days): if prev_meal == 1: # Had tonkatsu yesterday # Want variety â†’ lower probability current_meal = bernoulli(0.3) @ f\"day_{day}\" else: # Had hamburger yesterday # Craving tonkatsu â†’ higher probability current_meal = bernoulli(0.8) @ f\"day_{day}\" meals.append(current_meal) prev_meal = current_meal return jnp.array(meals) This models dependence through time!\nPattern 5: Mixture Models Scenario: Data comes from multiple sources\nExample: Two types of days (weekday vs weekend)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @gen def mixed_days(): \"\"\"Different behavior on weekends vs weekdays.\"\"\" # Hidden: is it a weekend? is_weekend = bernoulli(2/7) @ \"is_weekend\" # 2 out of 7 days if is_weekend: # Weekend: high chance of tonkatsu (relaxed) tonkatsu_prob = 0.9 else: # Weekday: lower chance (busy) tonkatsu_prob = 0.3 lunch = bernoulli(tonkatsu_prob) @ \"lunch\" return is_weekend Infer: â€œGiven Chibany had tonkatsu, is it a weekend?â€\nBuilding a Complete Model: Medical Diagnosis Letâ€™s build a realistic example from scratch.\nScenario: Diagnosing a disease based on symptoms\nSetup:\nDisease prevalence: 1% (rare) Symptom 1 (fever): 90% if diseased, 10% if healthy Symptom 2 (cough): 80% if diseased, 20% if healthy Question: Patient has fever and cough. Probability of disease?\nStep 1: Understand the Problem Question: Does patient have disease? Observations: Fever and cough Hidden: True disease status Relationships: Symptoms more likely if diseased Step 2: Generative Story First, patient either has disease (1%) or not (99%) If diseased, fever is very likely (90%) If diseased, cough is very likely (80%) If healthy, both symptoms are rare (10%, 20%) Step 3: Write the Model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @gen def disease_model(prevalence=0.01, fever_if_disease=0.9, cough_if_disease=0.8, fever_if_healthy=0.1, cough_if_healthy=0.2): \"\"\"Medical diagnosis model.\"\"\" # Hidden: disease status has_disease = bernoulli(prevalence) @ \"has_disease\" # Symptoms depend on disease if has_disease: fever = bernoulli(fever_if_disease) @ \"fever\" cough = bernoulli(cough_if_disease) @ \"cough\" else: fever = bernoulli(fever_if_healthy) @ \"fever\" cough = bernoulli(cough_if_healthy) @ \"cough\" return has_disease Step 4: Run Inference 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Patient has both symptoms observation = ChoiceMap({\"fever\": 1, \"cough\": 1}) def infer_disease(k): trace, weight = disease_model.generate(k, (), observation) return trace.get_retval() keys = jax.random.split(key, 10000) posterior = jax.vmap(infer_disease)(keys) prob_disease = jnp.mean(posterior) print(f\"=== MEDICAL DIAGNOSIS ===\") print(f\"Prevalence: 1%\") print(f\"Symptoms: Fever + Cough\") print(f\"P(Disease | Symptoms) â‰ˆ {prob_disease:.3f}\") Expected output: â‰ˆ 0.265 (26.5%)\nInterpretation: Even with both symptoms, only 26.5% chance of disease because itâ€™s so rare!\nBase Rate Neglect in Medicine! This is why false positives are a problem in medical testing.\nEven accurate tests produce many false positives for rare diseases because:\nTrue positives: $0.01 \\times 0.9 \\times 0.8 = 0.0072$ (0.72%) False positives: $0.99 \\times 0.1 \\times 0.2 = 0.0198$ (1.98%) More false positives than true positives!\nThis is why doctors donâ€™t diagnose based on symptoms alone â€” they need confirmatory tests or consider patient history (updating the prior).\nBest Practices âœ… DO Name everything clearly\n1 2 3 4 5 # Good is_diseased = bernoulli(0.01) @ \"is_diseased\" # Bad x = bernoulli(0.01) @ \"x\" Use meaningful parameters\n1 2 3 4 5 6 7 8 9 # Good @gen def model(disease_prevalence=0.01, test_accuracy=0.95): ... # Bad @gen def model(p1=0.01, p2=0.95): ... Document your model\n1 2 3 4 5 6 7 8 9 10 @gen def weather_mood(sunny_prior=0.7): \"\"\"Model how weather affects mood. Args: sunny_prior: Base rate of sunny days (default 0.7) Returns: is_sunny: Whether it's sunny today \"\"\" Start simple, add complexity\nBuild the simplest model first Verify it works Add features incrementally Test edge cases\nWhat if parameters are 0? 1? What if all observations are the same? Does the posterior make intuitive sense? âŒ DONâ€™T Donâ€™t forget to name random choices\n1 2 3 4 5 # Bad â€” can't condition on this! x = bernoulli(0.5) # Good x = bernoulli(0.5) @ \"x\" Donâ€™t use the same name twice\n1 2 3 4 5 6 7 # Bad â€” name collision! flip1 = bernoulli(0.5) @ \"flip\" flip2 = bernoulli(0.5) @ \"flip\" # ERROR! # Good â€” unique names flip1 = bernoulli(0.5) @ \"flip_1\" flip2 = bernoulli(0.5) @ \"flip_2\" Donâ€™t overthink distributions\nbernoulli covers most binary cases normal for continuous categorical for multiple choices You donâ€™t need exotic distributions to start! Donâ€™t skip validation\nAlways generate samples first Check if outputs look reasonable Verify extreme parameter values Exercises Exercise 1: Email Spam Filter Build a simple spam filter model.\nScenario:\n30% of emails are spam Spam emails contain â€œFREEâ€ 80% of the time Legitimate emails contain â€œFREEâ€ 10% of the time Task: Calculate $P(\\text{Spam} \\mid \\text{contains â€œFREEâ€})$\nSolution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 @gen def spam_filter(spam_rate=0.30): \"\"\"Simple spam filter based on keyword.\"\"\" # Hidden: is it spam? is_spam = bernoulli(spam_rate) @ \"is_spam\" # Observation: contains \"FREE\"? if is_spam: contains_free = bernoulli(0.80) @ \"contains_free\" else: contains_free = bernoulli(0.10) @ \"contains_free\" return is_spam # Email contains \"FREE\" observation = ChoiceMap({\"contains_free\": 1}) def infer_spam(k): trace, weight = spam_filter.generate(k, (), observation) return trace.get_retval() keys = jax.random.split(key, 10000) posterior = jax.vmap(infer_spam)(keys) prob_spam = jnp.mean(posterior) print(f\"P(Spam | contains 'FREE') â‰ˆ {prob_spam:.3f}\") Expected: â‰ˆ 0.774 (77.4%)\nTheoretical: $$P = \\frac{0.80 \\times 0.30}{0.80 \\times 0.30 + 0.10 \\times 0.70} = \\frac{0.24}{0.31} \\approx 0.774$$\nExercise 2: Learning from Multiple Observations Extend the coin flip model to infer bias from multiple observations.\nTask: Given a sequence of 20 flips (e.g., [1,1,0,1,1,1,0,1,1,1,1,0,1,1,0,1,1,1,1,1]), infer the coinâ€™s bias.\nSolution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @gen def coin_model(n_flips): \"\"\"Infer coin bias from observed flips.\"\"\" # Hidden: coin's true bias bias = uniform(0.0, 1.0) @ \"bias\" # Observations: flips for i in range(n_flips): flip = bernoulli(bias) @ f\"flip_{i}\" return bias # Observed flips: 16 heads out of 20 observed_flips = [1,1,0,1,1,1,0,1,1,1,1,0,1,1,0,1,1,1,1,1] observations = ChoiceMap({f\"flip_{i}\": observed_flips[i] for i in range(20)}) def infer_bias(k): trace, weight = coin_model.generate(k, (20,), observations) return trace.get_retval() keys = jax.random.split(key, 1000) posterior_bias = jax.vmap(infer_bias)(keys) mean_bias = jnp.mean(posterior_bias) std_bias = jnp.std(posterior_bias) print(f\"Estimated bias: {mean_bias:.2f} Â± {std_bias:.2f}\") # Should be around 0.80 (16/20) Expected: Mean â‰ˆ 0.80, with some uncertainty\nPlot the posterior:\n1 2 3 4 5 6 7 8 9 import matplotlib.pyplot as plt plt.hist(posterior_bias, bins=50, density=True, alpha=0.7, color='#4ecdc4') plt.axvline(mean_bias, color='red', linestyle='--', label=f'Mean = {mean_bias:.2f}') plt.xlabel('Coin Bias') plt.ylabel('Posterior Density') plt.title('Posterior Distribution of Coin Bias\\n(16 heads in 20 flips)') plt.legend() plt.show() Exercise 3: Multi-Symptom Diagnosis Extend the disease model to include 3 symptoms: fever, cough, fatigue.\nParameters:\nDisease: 2% prevalence If diseased: fever 90%, cough 80%, fatigue 95% If healthy: fever 10%, cough 20%, fatigue 30% Task: Calculate posterior for:\nFever only Fever + cough All three symptoms Solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @gen def disease_three_symptoms(prevalence=0.02): \"\"\"Disease model with three symptoms.\"\"\" has_disease = bernoulli(prevalence) @ \"has_disease\" if has_disease: fever = bernoulli(0.90) @ \"fever\" cough = bernoulli(0.80) @ \"cough\" fatigue = bernoulli(0.95) @ \"fatigue\" else: fever = bernoulli(0.10) @ \"fever\" cough = bernoulli(0.20) @ \"cough\" fatigue = bernoulli(0.30) @ \"fatigue\" return has_disease # Scenario 1: Fever only obs1 = ChoiceMap({\"fever\": 1}) # Scenario 2: Fever + cough obs2 = ChoiceMap({\"fever\": 1, \"cough\": 1}) # Scenario 3: All three obs3 = ChoiceMap({\"fever\": 1, \"cough\": 1, \"fatigue\": 1}) for i, obs in enumerate([obs1, obs2, obs3], 1): def infer(k): trace, weight = disease_three_symptoms.generate(k, (), obs) return trace.get_retval() keys = jax.random.split(key, 10000) posterior = jax.vmap(infer)(keys) prob = jnp.mean(posterior) print(f\"Scenario {i}: P(Disease) â‰ˆ {prob:.3f}\") Expected output:\nScenario 1: P(Disease) â‰ˆ 0.155 # Fever only Scenario 2: P(Disease) â‰ˆ 0.419 # Fever + cough Scenario 3: P(Disease) â‰ˆ 0.774 # All three symptoms Insight: More evidence â†’ higher posterior!\nWhat Youâ€™ve Learned In this chapter, you learned:\nâœ… The model-building process â€” from problem to code âœ… Common patterns â€” independent, hierarchical, conditional, sequential, mixture âœ… Best practices â€” naming, documentation, testing âœ… Complete examples â€” medical diagnosis, spam filtering, coin flipping âœ… How to think generatively â€” â€œwhat generates the data?â€\nThe key insight: Building models is about encoding your assumptions about how the world works, then letting GenJAX do the inference!\nNext Steps Youâ€™re Ready to Build! You now have all the tools to:\nBuild generative models for your problems Perform Bayesian inference automatically Understand uncertainty in your predictions Where to go from here:\n1. Explore More Distributions GenJAX supports many distributions beyond bernoulli:\nnormal(mean, std) â€” Continuous values (heights, weights, temperatures) categorical(probs) â€” Multiple discrete choices (A, B, C, D) poisson(rate) â€” Count data (number of events) gamma, beta, exponential â€” Specialized continuous distributions See the GenJAX documentation for complete reference.\n2. Learn Advanced Inference This tutorial covered:\nFiltering/rejection sampling Conditioning with generate() Next level:\nImportance sampling (more efficient for rare events) Markov Chain Monte Carlo (MCMC) for complex models Variational inference (approximate but fast) Check out: GenJAX advanced tutorials\n3. Real-World Applications Apply what you learned to:\nScience: Modeling experiments, analyzing data Medicine: Diagnosis, treatment optimization Engineering: Fault detection, quality control Social science: Understanding human behavior AI/ML: Building better models with uncertainty The Journey You started with: Sets, counting, basic probability\nNow you can: Build probabilistic programs, perform Bayesian inference, reason under uncertainty\nThatâ€™s a huge accomplishment!\nFinal Thoughts Probabilistic programming is a superpower:\nExpress uncertainty â€” the world is uncertain, our models should reflect that Automate inference â€” computers do the hard math Combine knowledge and data â€” use both domain expertise (priors) and observations (data) Make better decisions â€” understand risks and probabilities Keep building, keep learning, keep questioning!\nResources GenJAX Documentation:\nOfficial docs: gen.dev More examples: GenJAX GitHub Probability Theory:\nThis tutorialâ€™s probability chapters! â€œProbabilistic Programming \u0026 Bayesian Methods for Hackersâ€ (free online) â€œThink Bayesâ€ by Allen Downey (beginner-friendly) Community:\nGenJAX Discourse: Ask questions, share models Probabilistic Programming Slack: Connect with practitioners You Did It! Congratulations on completing the GenJAX tutorial!\nYouâ€™ve gone from â€œwhat is GenJAX?â€ to building complete probabilistic models. Thatâ€™s no small feat!\nNow go build something amazing. ğŸš€\nâ† Previous: Inference in Action Back to Introduction â†’",
    "description": "From Following Recipes to Creating Your Own Youâ€™ve learned to use GenJAX through examples. Now itâ€™s time to build your own probabilistic models!\nThis chapter shows you how to think about building generative models â€” turning real-world problems into code.\nThe Model-Building Process Step 1: Understand the Problem Before writing any code, answer:\nWhat am I trying to predict or understand? (The question) What do I observe? (The data/evidence) Whatâ€™s hidden? (The unknown variables) How are they related? (The causal structure) Example: Spam detection",
    "tags": [],
    "title": "Building Your Own Models",
    "uri": "/probintro/genjax/06_building_models/index.html"
  },
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing Tutorial",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/probintro/categories/index.html"
  },
  {
    "breadcrumb": "Probability \u0026 Probabilistic Computing Tutorial",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/probintro/tags/index.html"
  }
]
