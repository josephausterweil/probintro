+++
title = "What You'll Learn"
weight = 1
+++

This tutorial will give you a solid foundation in probability theory through a set-based approach. By the end, you'll be able to reason clearly about uncertainty and be prepared for advanced topics in probabilistic computing and Bayesian inference.

## Learning Goals

### 1. Think About Probability Using Sets

Learn to see probability as **counting** — a concrete, visual approach that makes abstract concepts intuitive.

**You'll be able to:**
- Define outcome spaces (What could happen?)
- Identify events as subsets (What am I interested in?)
- Calculate probabilities by counting (What's the ratio?)

### 2. Understand Core Concepts Through Narrative

Follow Chibany's story to master the fundamental building blocks of probability theory.

**Key concepts:**
- **Events and outcome spaces** — The foundation of probabilistic reasoning
- **Conditional probability** — How learning changes what's possible
- **Independence** — When things don't affect each other
- **Bayes' rule** — Updating beliefs with new information
- **Random variables** — Functions on outcomes
- **Joint and marginal probabilities** — Relationships between variables

### 3. Avoid Common Misconceptions

Understand where intuition misleads us and develop tools to think more clearly.

**You'll tackle:**
- The [Taxicab Problem](https://en.wikipedia.org/wiki/Representativeness_heuristic#The_taxicab_problem) — How base rates matter
- Base-rate neglect — Why rare events stay rare even with evidence
- The difference between $P(A \mid B)$ and $P(B \mid A)$ — A source of endless confusion

### 4. Build Foundation for Probabilistic Programming

Develop the mental models that make probabilistic computing approachable.

**You'll understand:**
- How outcome spaces become generative processes
- Why conditioning is fundamental to inference
- How counting scales to computation
- The connection between sets and code

### 5. Develop Confidence for Further Learning

After this tutorial, you'll be ready to explore more advanced topics with a solid conceptual foundation.

**Next steps will include:**
- Probabilistic programming with GenJAX
- Continuous probability distributions
- Bayesian inference and conjugate models
- Mixture models and hierarchical models
- Machine learning and generative AI

---

## How the Tutorial Works

Each chapter builds on previous concepts using Chibany's daily meal situation:
1. Start with simple counting (two meals, two options)
2. Introduce probability as ratios
3. Explore what happens when you learn something new
4. Apply Bayes' rule to real problems
5. Connect to computational approaches

**The progression is gradual** — from concrete examples to general principles, from small outcome spaces to the logic that scales to complex models.

---

## What You Need

- **No math prerequisites** — We introduce everything from scratch
- **Curiosity** — Willingness to think through examples carefully
- **Patience** — Some concepts feel strange at first but become natural with practice

---

**Ready to meet Chibany and start learning?**

[← Previous: Introduction](./_index.md) | [Next: Chibany is Hungry →](./02_hungry.md)
